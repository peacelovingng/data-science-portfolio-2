{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Kaggle competition - Titanic__\n",
    "\n",
    "1. [Import](#Import)\n",
    "    1. [Tools](#Tools)\n",
    "    1. [Data](#Data)    \n",
    "1. [Initial EDA](#Initial-EDA)\n",
    "    1. [Categorical feature EDA](#Categorical-feature-EDA)\n",
    "        1. [Univariate & feature vs. target](#Univariate-&-feature-vs.-target)\n",
    "    1. [Continuous feature EDA](#Continuous-feature-EDA)\n",
    "        1. [Univariate & feature vs. target](#Univariate-&-feature-vs.-target2)\n",
    "        1. [Correlation](#Correlation)\n",
    "        1. [Pair plot](#Pair-plot)\n",
    "    1. [Faceting](#Faceting)\n",
    "    1. [Target variable evaluation](#Target-variable-evaluation)    \n",
    "1. [Data preparation](#Data-preparation)\n",
    "    1. [Outliers (preliminary)](#Outliers-preliminary)\n",
    "        1. [Evaluate](#Evaluate)\n",
    "        1. [Remove](#remove)\n",
    "    1. [Missing data](#Missing-data)\n",
    "        1. [Evaluate](#Evaluate1)\n",
    "        1. [Impute](#Impute)\n",
    "    1. [Engineering](#Engineering)\n",
    "        1. [Evaluate](#Evaluate3)\n",
    "        1. [Engineer](#Engineer)\n",
    "    1. [Encoding](#Encoding)\n",
    "        1. [Evaluate](#Evaluate2)\n",
    "        1. [Encode](#Encode)\n",
    "    1. [Transformation](#Transformation)\n",
    "        1. [Evaluate](#Evaluate4)\n",
    "        1. [Transform](#Transform)\n",
    "    1. [Outliers (final)](#Outliers-final)\n",
    "        1. [Evaluate](#Evaluate5)\n",
    "        1. [Remove](#remove1)\n",
    "1. [Data evaluation](#Data-evaluation)\n",
    "    1. [Feature importance](#Feature-importance)    \n",
    "    1. [Rationality](#Rationality)\n",
    "    1. [Value override](#Value-override)\n",
    "    1. [Continuous feature EDA](#Continuous-feature-EDA3)\n",
    "    1. [Correlation](#Correlation3)\n",
    "1. [Modeling](#Modeling)\n",
    "    1. [Data preparation](#Data-preparation)\n",
    "    1. [Bayesian hyper-parameter optimization](#Bayesian-hyper-parameter-optimization)\n",
    "        1. [Model loss by iteration](#Model-loss-by-iteration)\n",
    "        1. [Parameter selection by iteration](#Parameter-selection-by-iteration)\n",
    "    1. [Model performance evaluation](#Model-performance-evaluation)\n",
    "        1. [Classification report](#Classification-report)\n",
    "        1. [Confusion matrix](#Confusion-matrix)\n",
    "        1. [ROC curve](#ROC-curve)\n",
    "    1. [Model explanability](#Model-explanability)\n",
    "        1. [Permutation importance](#Permutation-importance)\n",
    "        1. [Partial plots](#Partial-plots)\n",
    "        1. [SHAP values](#SHAP-values)\n",
    "    1. [Stacking](#Stacking)\n",
    "        1. [Primary models](#Primary-models)\n",
    "        1. [Meta model](#Meta-model)                \n",
    "1. [Submission](#Submission)\n",
    "    1. [Standard](#Standard)\n",
    "    1. [Stack](#Stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Import'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Tools'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T23:13:25.574506Z",
     "start_time": "2019-07-21T23:13:23.558320Z"
    }
   },
   "outputs": [],
   "source": [
    "# standard libary and settings\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "import itertools\n",
    "import csv\n",
    "import ast\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "global ITERATION\n",
    "import time\n",
    "from functools import reduce\n",
    "\n",
    "rundate = time.strftime(\"%Y%m%d\")\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "# data extensions and settings\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(threshold=np.inf, suppress=True)\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.options.display.float_format = \"{:,.6f}\".format\n",
    "\n",
    "# modeling extensions\n",
    "import sklearn.base as base\n",
    "import sklearn.cluster as cluster\n",
    "import sklearn.datasets as datasets\n",
    "import sklearn.decomposition as decomposition\n",
    "import sklearn.discriminant_analysis as discriminant_analysis\n",
    "import sklearn.ensemble as ensemble\n",
    "import sklearn.feature_extraction as feature_extraction\n",
    "import sklearn.feature_selection as feature_selection\n",
    "import sklearn.gaussian_process as gaussian_process\n",
    "import sklearn.linear_model as linear_model\n",
    "import sklearn.kernel_ridge as kernel_ridge\n",
    "import sklearn.metrics as metrics\n",
    "import sklearn.model_selection as model_selection\n",
    "import sklearn.naive_bayes as naive_bayes\n",
    "import sklearn.neighbors as neighbors\n",
    "import sklearn.pipeline as pipeline\n",
    "import sklearn.preprocessing as preprocessing\n",
    "import sklearn.svm as svm\n",
    "import sklearn.tree as tree\n",
    "import sklearn.utils as utils\n",
    "\n",
    "import eif\n",
    "\n",
    "from scipy import stats, special\n",
    "import xgboost\n",
    "import lightgbm\n",
    "import catboost\n",
    "\n",
    "from hyperopt import hp, tpe, Trials, fmin, STATUS_OK\n",
    "from hyperopt.pyll.stochastic import sample\n",
    "\n",
    "# visualization extensions and settings\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "try:\n",
    "    import mlmachine as mlm\n",
    "    from prettierplot.plotter import PrettierPlot\n",
    "    import prettierplot.style as style\n",
    "except ModuleNotFoundError:\n",
    "    sys.path.append(\"../../../mlmachine\") if \"../../../../mlmachine\" not in sys.path else None\n",
    "    sys.path.append(\"../../../prettierplot\") if \"../../../../prettierplot\" not in sys.path else None\n",
    "    \n",
    "    import mlmachine as mlm\n",
    "    from prettierplot.plotter import PrettierPlot\n",
    "    import prettierplot.style as style\n",
    "else:\n",
    "    print('This notebook relies on the libraries mlmachine and prettierplot. Please run:')\n",
    "    print('\\tpip install mlmachine')\n",
    "    print('\\tpip install prettierplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T23:13:25.593388Z",
     "start_time": "2019-07-21T23:13:25.576827Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load data and print dimensions\n",
    "data = pd.read_csv(\"../../data/projectEmployeeAttrition/IbmEmployeeAttrition.csv\")\n",
    "\n",
    "print(\"Training data dimensions: {}\".format(data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T23:13:25.628915Z",
     "start_time": "2019-07-21T23:13:25.595984Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# display info and first 5 rows\n",
    "data.info()\n",
    "display(data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T23:13:25.644625Z",
     "start_time": "2019-07-21T23:13:25.638213Z"
    }
   },
   "outputs": [],
   "source": [
    "# review counts of different column types\n",
    "data.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into train and validation datasets\n",
    "dfTrain, dfValid = mlm.trainTestCompile(data=data, targetCol='Attrition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T23:13:25.657448Z",
     "start_time": "2019-07-21T23:13:25.646578Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load training data into mlmachine\n",
    "train = mlm.Machine(\n",
    "    data=dfTrain,\n",
    "    target=[\"Attrition\"],\n",
    "    removeFeatures=[\"EmployeeNumber\",\"EmployeeCount\",\"StandardHours\",\"PerformanceRating\",\"RelationshipSatisfaction\",\n",
    "                   \"StockOptionLevel\",\"TrainingTimesLastYear\",\"WorkLifeBalance\"],\n",
    "    overrideCat=[\"Education\",\"EnvironmentSatisfaction\",\"JobInvolvement\",\"JobLevel\",\"JobSatisfaction\",\n",
    "                    \"MaritalStatus\"],\n",
    "    targetType=\"categorical\",\n",
    ")\n",
    "print(train.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T23:13:25.657448Z",
     "start_time": "2019-07-21T23:13:25.646578Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load training data into mlmachine\n",
    "valid = mlm.Machine(\n",
    "    data=dfValid,\n",
    "    target=[\"Attrition\"],\n",
    "    removeFeatures=[\"EmployeeNumber\",\"EmployeeCount\",\"StandardHours\",\"PerformanceRating\",\"RelationshipSatisfaction\",\n",
    "                   \"StockOptionLevel\",\"TrainingTimesLastYear\",\"WorkLifeBalance\"],\n",
    "    overrideCat=[\"Education\",\"EnvironmentSatisfaction\",\"JobInvolvement\",\"JobLevel\",\"JobSatisfaction\",\n",
    "                    \"MaritalStatus\"],\n",
    "    targetType=\"categorical\",\n",
    ")\n",
    "print(valid.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Initial-EDA'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical feature EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Categorical-feature-EDA'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate & feature vs. target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Univariate-&-feature-vs.-target'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T23:13:31.521453Z",
     "start_time": "2019-07-21T23:13:25.659269Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# categorical features\n",
    "train.edaCatTargetCatFeat(skipCols=[\"\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous feature EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Continuous-feature-EDA'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate & feature vs. target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Univariate-&-feature-vs.-target2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T23:13:42.734971Z",
     "start_time": "2019-07-21T23:13:31.524097Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# continuous features\n",
    "train.edaCatTargetNumFeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Correlation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Correlation (all samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T23:13:43.387378Z",
     "start_time": "2019-07-21T23:13:42.736786Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# correlation heat map\n",
    "p = PrettierPlot()\n",
    "ax = p.makeCanvas()\n",
    "p.prettyCorrHeatmap(df=train.data, annot=False, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Correlation (top vs. target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T23:13:43.860056Z",
     "start_time": "2019-07-21T23:13:43.388963Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# correlation heat map with most highly correlated features relative to the target\n",
    "p = PrettierPlot(plotOrientation='tall')\n",
    "ax = p.makeCanvas()\n",
    "p.prettyCorrHeatmapTarget(\n",
    "    df=train.data, target=train.target, thresh=0.02, annot=True, ax=ax\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Remarks - There are three pairs of highly correlated features:\n",
    "    - 'GarageArea' and 'GarageCars'\n",
    "    - 'TotRmsAbvGrd' and 'GrLivArea'\n",
    "    - '1stFlrSF' and 'TotalBsmtSF\n",
    "This makes sense, given what each feature represents and how each pair items relate to each other. We likely only need one feature from each pair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pair plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Pair-plot'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T23:14:52.815407Z",
     "start_time": "2019-07-21T23:14:12.586722Z"
    }
   },
   "outputs": [],
   "source": [
    "# pair plot\n",
    "p = PrettierPlot(chartProp=12)\n",
    "p.prettyPairPlot(df=train.data, cols=train.featureByDtype_['continuous'], diag_kind=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T19:21:40.906692Z",
     "start_time": "2019-07-21T19:21:12.258740Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pair plot\n",
    "p = PrettierPlot(chartProp=12)\n",
    "p.prettyPairPlot(\n",
    "    df=train.data.dropna(),\n",
    "    diag_kind=\"kde\",\n",
    "    target=train.target,\n",
    "    cols=train.featureByDtype_['continuous'][:10],\n",
    "    legendLabels=[\"Stays\", \"Leaves\"],\n",
    "    bbox=(2.0, 0.0),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faceting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Faceting'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T23:22:22.027114Z",
     "start_time": "2019-07-21T23:22:21.746572Z"
    }
   },
   "outputs": [],
   "source": [
    "# facet MaritalStatus vs. Gender\n",
    "p = PrettierPlot(chartProp=12)\n",
    "ax = p.makeCanvas(title=\"Attrition, MaritalStatus vs. Gender\", yShift=0.7)\n",
    "p.prettyFacetTwoCatBar(\n",
    "    df=train.edaData(train.data, train.target),\n",
    "    x=\"MaritalStatus\",\n",
    "    y=train.target.name,\n",
    "    split=\"Gender\",\n",
    "    yUnits=\"fff\",\n",
    "    bbox = (1.2, 0.8),\n",
    "    ax=ax,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T23:18:22.808276Z",
     "start_time": "2019-07-21T23:18:22.530914Z"
    }
   },
   "outputs": [],
   "source": [
    "# facet MaritalStatus vs. Gender\n",
    "p = PrettierPlot(chartProp=12)\n",
    "ax = p.makeCanvas(title=\"Attrition, BusinessTravel vs. Gender\", yShift=0.7)\n",
    "p.prettyFacetTwoCatBar(\n",
    "    df=train.edaData(train.data, train.target),\n",
    "    x=\"BusinessTravel\",\n",
    "    y=train.target.name,\n",
    "    split=\"Gender\",\n",
    "    yUnits=\"fff\",\n",
    "    bbox = (1.2, 0.8),\n",
    "    ax=ax,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T23:23:05.927754Z",
     "start_time": "2019-07-21T23:23:05.650988Z"
    }
   },
   "outputs": [],
   "source": [
    "# facet MaritalStatus vs. Gender\n",
    "p = PrettierPlot(chartProp=12)\n",
    "ax = p.makeCanvas(title=\"Attrition, JobSatisfaction vs. Gender\", yShift=0.7)\n",
    "p.prettyFacetTwoCatBar(\n",
    "    df=train.edaData(train.data, train.target),\n",
    "    x=\"JobSatisfaction\",\n",
    "    y=train.target.name,\n",
    "    split=\"Gender\",\n",
    "    yUnits=\"fff\",\n",
    "    bbox = (1.2, 0.8),\n",
    "    ax=ax,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T23:21:33.897889Z",
     "start_time": "2019-07-21T23:21:33.572666Z"
    }
   },
   "outputs": [],
   "source": [
    "# facet MaritalStatus vs. Gender\n",
    "p = PrettierPlot(chartProp=12)\n",
    "ax = p.makeCanvas(title=\"Attrition, JobSatisfaction by Education\", yShift=0.7)\n",
    "p.prettyFacetTwoCatBar(\n",
    "    df=train.edaData(train.data, train.target),\n",
    "    x=\"JobSatisfaction\",\n",
    "    y=train.target.name,\n",
    "    split=\"Education\",\n",
    "    yUnits=\"fff\",\n",
    "    bbox = (1.3, 0.8),\n",
    "    ax=ax,\n",
    "    legendLabels = ['Below College','College','Bachelor','Master','Doctor']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Points plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T23:28:45.257831Z",
     "start_time": "2019-07-21T23:28:44.541808Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "p = PrettierPlot()\n",
    "p.prettyFacetTwoCatPoint(\n",
    "    df=train.edaData(train.data, train.target),\n",
    "    x=\"Education\",\n",
    "    y=train.target.name,\n",
    "    split=\"Gender\",\n",
    "    catRow=\"JobSatisfaction\",\n",
    "    height=5,\n",
    "    bbox=(1.3, 1.2),\n",
    "#     legendLabels=[\"1st class\", \"2nd class\", \"3rd class\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T23:34:48.458611Z",
     "start_time": "2019-07-21T23:34:47.765105Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "p = PrettierPlot()\n",
    "p.prettyFacetTwoCatPoint(\n",
    "    df=train.edaData(train.data, train.target),\n",
    "    x=\"BusinessTravel\",\n",
    "    y=train.target.name,\n",
    "    split=\"Gender\",\n",
    "    catRow=\"MaritalStatus\",\n",
    "    aspect = 1.5,\n",
    "    height=5,\n",
    "    bbox=(1.3, 1.2),\n",
    "#     legendLabels=[\"1st class\", \"2nd class\", \"3rd class\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T23:15:22.395604Z",
     "start_time": "2019-07-21T23:15:22.391535Z"
    }
   },
   "outputs": [],
   "source": [
    "train.featureByDtype_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T21:36:14.536172Z",
     "start_time": "2019-07-20T21:36:12.946151Z"
    }
   },
   "outputs": [],
   "source": [
    "# #\n",
    "# p = PrettierPlot()\n",
    "# p.prettyFacetCatNumHist(\n",
    "#     df=train.edaData(train.data, train.target),\n",
    "#     split=train.target.name,\n",
    "#     legendLabels=[\"Died\", \"Lived\"],\n",
    "#     catRow=\"Sex\",\n",
    "#     catCol=\"Embarked\",\n",
    "#     numCol=\"Age\",\n",
    "#     bbox=(1.9, 1.0),\n",
    "#     height=4,\n",
    "#     aspect=1,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T21:36:16.059971Z",
     "start_time": "2019-07-20T21:36:14.541057Z"
    }
   },
   "outputs": [],
   "source": [
    "# #\n",
    "# p = PrettierPlot(chartProp=15)\n",
    "# p.prettyFacetCatNumScatter(\n",
    "#     df=train.edaData(train.data, train.target),\n",
    "#     split=train.target.name,\n",
    "#     legendLabels=[\"Died\", \"Lived\"],\n",
    "#     catRow=\"Sex\",\n",
    "#     catCol=\"Embarked\",\n",
    "#     xNum=\"Fare\",\n",
    "#     yNum=\"Age\",\n",
    "#     bbox=(1.9, 1.0),\n",
    "#     height=4,\n",
    "#     aspect=1,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target variable evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Target-variable-evaluation'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T23:44:24.169068Z",
     "start_time": "2019-07-21T23:44:24.163312Z"
    }
   },
   "outputs": [],
   "source": [
    "# null score\n",
    "pd.Series(train.target).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Data-preparation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers (preliminary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Outliers-preliminary'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Evaluate'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T21:36:16.092537Z",
     "start_time": "2019-07-20T21:36:16.081028Z"
    }
   },
   "outputs": [],
   "source": [
    "# identify columns that have zero missing values\n",
    "nonNull = train.data.columns[train.data.isnull().sum() == 0].values.tolist()\n",
    "\n",
    "# identify intersection between non-null columns and continuous columns\n",
    "nonNullNumCol = list(set(nonNull).intersection(train.featureByDtype_[\"continuous\"]))\n",
    "print(nonNullNumCol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T21:36:16.119200Z",
     "start_time": "2019-07-20T21:36:16.097048Z"
    }
   },
   "outputs": [],
   "source": [
    "# identify outliers using IQR\n",
    "trainPipe = pipeline.Pipeline([\n",
    "    (\"outlier\",train.OutlierIQR(\n",
    "                outlierCount=5,\n",
    "                iqrStep=1.5,\n",
    "                features=nonNullNumCol,\n",
    "                dropOutliers=False,))\n",
    "    ])\n",
    "train.data = trainPipe.transform(train.data)\n",
    "\n",
    "# capture outliers\n",
    "iqrOutliers = np.array(sorted(trainPipe.named_steps[\"outlier\"].outliers_))\n",
    "print(iqrOutliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T21:36:16.380935Z",
     "start_time": "2019-07-20T21:36:16.121645Z"
    }
   },
   "outputs": [],
   "source": [
    "# identify outliers using Isolation Forest\n",
    "clf = ensemble.IsolationForest(\n",
    "    behaviour=\"new\", max_samples=train.data.shape[0], random_state=0, contamination=0.02\n",
    ")\n",
    "clf.fit(train.data[nonNullNumCol])\n",
    "preds = clf.predict(train.data[nonNullNumCol])\n",
    "\n",
    "# evaluate index values\n",
    "mask = np.isin(preds, -1)\n",
    "ifOutliers = np.array(train.data[mask].index)\n",
    "print(ifOutliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T21:36:20.318917Z",
     "start_time": "2019-07-20T21:36:16.383982Z"
    }
   },
   "outputs": [],
   "source": [
    "# identify outliers using extended isolation forest\n",
    "trainPipe = pipeline.Pipeline([\n",
    "    (\"outlier\",train.ExtendedIsoForest(\n",
    "                cols=nonNullNumCol,\n",
    "                nTrees=100,\n",
    "                sampleSize=256,\n",
    "                ExtensionLevel=1,\n",
    "                anomaliesRatio=0.02,\n",
    "                dropOutliers=False,))\n",
    "    ])\n",
    "train.data = trainPipe.transform(train.data)\n",
    "\n",
    "# capture outliers\n",
    "eifOutliers = np.array(sorted(trainPipe.named_steps[\"outlier\"].outliers_))\n",
    "print(eifOutliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T21:36:20.328036Z",
     "start_time": "2019-07-20T21:36:20.321588Z"
    }
   },
   "outputs": [],
   "source": [
    "# identify outliers that are identified in multiple algorithms\n",
    "outliers = reduce(np.intersect1d, (iqrOutliers, ifOutliers, eifOutliers))\n",
    "# outliers = reduce(np.intersect1d, (ifOutliers, eifOutliers))\n",
    "print(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# review outlier identification summary\n",
    "outlierSummary = train.outlierSummary(iqrOutliers=iqrOutliers,\n",
    "                             ifOutliers=ifOutliers,\n",
    "                             eifOutliers=eifOutliers\n",
    "                            )\n",
    "outlierSummary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validation outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'remove'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T21:36:20.380105Z",
     "start_time": "2019-07-20T21:36:20.331220Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # remove outlers from predictors and response\n",
    "# outliers = np.array([59,121])\n",
    "# train.data = train.data.drop(outliers)\n",
    "# train.target = train.target.drop(index=outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Missing-data'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Evaluate1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-21T18:54:28.549354Z",
     "start_time": "2019-07-21T18:54:28.540270Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate missing data\n",
    "train.edaMissingSummary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validation missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T21:36:23.944186Z",
     "start_time": "2019-07-20T21:36:23.660271Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate missing data\n",
    "valid.edaMissingSummary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Training vs. validation missingness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T21:36:26.878909Z",
     "start_time": "2019-07-20T21:36:26.865280Z"
    }
   },
   "outputs": [],
   "source": [
    "# compare feature with missing data\n",
    "train.missingColCompare(train.data, valid.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Impute'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Impute training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T21:36:27.120424Z",
     "start_time": "2019-07-20T21:36:26.882212Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Impute validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T21:36:27.353587Z",
     "start_time": "2019-07-20T21:36:27.123813Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Engineering'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Evaluate3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engineer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Engineer'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Engineer training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T21:36:27.557352Z",
     "start_time": "2019-07-20T21:36:27.356655Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T21:36:27.570789Z",
     "start_time": "2019-07-20T21:36:27.560854Z"
    }
   },
   "outputs": [],
   "source": [
    "# print new columns\n",
    "for col in train.data.columns:\n",
    "    if (\n",
    "        col not in train.featureByDtype_[\"categorical\"]\n",
    "        and col not in train.featureByDtype_[\"continuous\"]\n",
    "    ):\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T21:36:27.583604Z",
     "start_time": "2019-07-20T21:36:27.578679Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T21:36:33.093998Z",
     "start_time": "2019-07-20T21:36:27.591686Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate additional features\n",
    "train.edaCatTargetCatFeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Engineer validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T21:36:33.468506Z",
     "start_time": "2019-07-20T21:36:33.099766Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T21:36:33.477915Z",
     "start_time": "2019-07-20T21:36:33.470687Z"
    }
   },
   "outputs": [],
   "source": [
    "# print new columns\n",
    "for col in valid.data.columns:\n",
    "    if (\n",
    "        col not in valid.featureByDtype_[\"categorical\"]\n",
    "        and col not in valid.featureByDtype_[\"continuous\"]\n",
    "    ):\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T21:36:33.503375Z",
     "start_time": "2019-07-20T21:36:33.490279Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Encoding'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Evaluate2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training feature evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T21:36:33.605818Z",
     "start_time": "2019-07-20T21:36:33.509166Z"
    }
   },
   "outputs": [],
   "source": [
    "# counts of unique values in training data string columns\n",
    "train.data[train.featureByDtype_[\"categorical\"]].apply(pd.Series.nunique, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T21:36:33.626221Z",
     "start_time": "2019-07-20T21:36:33.608146Z"
    }
   },
   "outputs": [],
   "source": [
    "# print unique values in each categorical columns\n",
    "for col in train.data[train.featureByDtype_[\"categorical\"]]:\n",
    "    try:\n",
    "        print(col, np.unique(train.data[col]))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validation feature evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T21:36:33.676910Z",
     "start_time": "2019-07-20T21:36:33.628767Z"
    }
   },
   "outputs": [],
   "source": [
    "# counts of unique values in validation data string columns\n",
    "valid.data[valid.featureByDtype_[\"categorical\"]].apply(pd.Series.nunique, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T21:36:33.769650Z",
     "start_time": "2019-07-20T21:36:33.681395Z"
    }
   },
   "outputs": [],
   "source": [
    "# print unique values in each categorical columns\n",
    "for col in valid.data[valid.featureByDtype_[\"categorical\"]]:\n",
    "    if col not in [\"\"]:\n",
    "        print(col, np.unique(valid.data[col]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training vs. validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T21:36:33.789094Z",
     "start_time": "2019-07-20T21:36:33.771856Z"
    }
   },
   "outputs": [],
   "source": [
    "# identify values that are present in the training data but not the validation data, and vice versa\n",
    "for col in train.featureByDtype_[\"categorical\"]:\n",
    "    if col not in [\"\"]:\n",
    "        trainValues = train.data[col].unique()\n",
    "        validValues = valid.data[col].unique()\n",
    "\n",
    "        trainDiff = set(trainValues) - set(validValues)\n",
    "        validDiff = set(validValues) - set(trainValues)\n",
    "\n",
    "        if len(trainDiff) > 0 or len(validDiff) > 0:\n",
    "            print(\"\\n\\n*** \" + col)\n",
    "            print(\"Value present in training data, not in validation data\")\n",
    "            print(trainDiff)\n",
    "            print(\"Value present in validation data, not in training data\")\n",
    "            print(validDiff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Encode'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Encode training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T21:36:33.832581Z",
     "start_time": "2019-07-20T21:36:33.791814Z"
    }
   },
   "outputs": [],
   "source": [
    "# ordinal column encoding instructions\n",
    "ordCatCols = {\"Pclass\": {1: 1, 2: 2, 3: 3}}\n",
    "\n",
    "# nominal columns\n",
    "nomCatCols = [\"Embarked\", \"Sex\", \"CabinQuarter\", \"Title\"]\n",
    "\n",
    "# apply encodings to training data\n",
    "trainPipe = pipeline.Pipeline(\n",
    "    [\n",
    "        (\"encodeOrdinal\", train.CustomOrdinalEncoder(encodings=ordCatCols)),\n",
    "        (\"dummyNominal\", train.Dummies(cols=nomCatCols, dropFirst=True)),\n",
    "    ]\n",
    ")\n",
    "train.data = trainPipe.transform(train.data)\n",
    "train.data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Encode validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T21:36:33.884411Z",
     "start_time": "2019-07-20T21:36:33.836230Z"
    }
   },
   "outputs": [],
   "source": [
    "# apply encodings to validation data\n",
    "validPipe = pipeline.Pipeline(\n",
    "    [\n",
    "        (\"encodeOrdinal\", valid.CustomOrdinalEncoder(encodings=ordCatCols)),\n",
    "        (\"dummyNominal\", valid.Dummies(cols=nomCatCols, dropFirst=False)),\n",
    "        (\"levels\", valid.MissingDummies(trainCols=train.data.columns)),\n",
    "    ]\n",
    ")\n",
    "valid.data = validPipe.transform(valid.data)\n",
    "valid.data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Transformation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Evaluate4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training feature transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T21:36:33.927522Z",
     "start_time": "2019-07-20T21:36:33.888125Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate skew of continuous features - training data\n",
    "train.skewSummary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validation feature transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T21:36:33.972762Z",
     "start_time": "2019-07-20T21:36:33.931818Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate skew of continuous features - validation data\n",
    "valid.skewSummary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Transform'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transform training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T21:36:34.108579Z",
     "start_time": "2019-07-20T21:36:33.975912Z"
    }
   },
   "outputs": [],
   "source": [
    "# skew correct in training dataset, which also learns te best lambda value for each columns\n",
    "trainPipe = pipeline.Pipeline([\n",
    "        (\"skew\",train.SkewTransform(cols=train.featureByDtype_[\"continuous\"], skewMin=0.75, pctZeroMax=1.0, verbose = True))\n",
    "    ])\n",
    "train.data = trainPipe.transform(train.data)\n",
    "train.skewSummary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transform validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T21:36:34.207328Z",
     "start_time": "2019-07-20T21:36:34.112559Z"
    }
   },
   "outputs": [],
   "source": [
    "# skew correction in validation dataset using lambdas learned on training data\n",
    "validPipe = pipeline.Pipeline([\n",
    "        (\"skew\",valid.SkewTransform(train=False, trainValue=trainPipe.named_steps[\"skew\"].trainValue_))\n",
    "    ])\n",
    "valid.data = validPipe.transform(valid.data)\n",
    "valid.skewSummary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers (final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Outliers-final'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Evaluate5'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T21:36:34.291114Z",
     "start_time": "2019-07-20T21:36:34.213474Z"
    }
   },
   "outputs": [],
   "source": [
    "# identify outliers using IQR\n",
    "trainPipe = pipeline.Pipeline([\n",
    "        (\"outlier\",train.OutlierIQR(\n",
    "                outlierCount=5,\n",
    "                iqrStep=1.5,\n",
    "                features=train.data.columns,\n",
    "                dropOutliers=False,\n",
    "            ))\n",
    "    ])\n",
    "train.data = trainPipe.transform(train.data)\n",
    "\n",
    "# capture outliers\n",
    "iqrOutliers = np.array(sorted(trainPipe.named_steps[\"outlier\"].outliers_))\n",
    "print(iqrOutliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T21:36:34.619944Z",
     "start_time": "2019-07-20T21:36:34.293930Z"
    }
   },
   "outputs": [],
   "source": [
    "# identify outliers using Isolation Forest\n",
    "clf = ensemble.IsolationForest(\n",
    "    behaviour=\"new\", max_samples=train.data.shape[0], random_state=0, contamination=0.01\n",
    ")\n",
    "clf.fit(train.data[train.data.columns])\n",
    "preds = clf.predict(train.data[train.data.columns])\n",
    "\n",
    "# evaluate index values\n",
    "mask = np.isin(preds, -1)  # np.in1d if np.isin is not available\n",
    "ifOutliers = np.array(train.data[mask].index)\n",
    "print(ifOutliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T21:36:39.328009Z",
     "start_time": "2019-07-20T21:36:34.622881Z"
    }
   },
   "outputs": [],
   "source": [
    "# identify outliers using IQR\n",
    "trainPipe = pipeline.Pipeline([\n",
    "    (\"outlier\",train.ExtendedIsoForest(\n",
    "                cols=train.data.columns,\n",
    "                nTrees=100,\n",
    "                sampleSize=256,\n",
    "                ExtensionLevel=1,\n",
    "                anomaliesRatio=0.02,\n",
    "                dropOutliers=False,))\n",
    "    ])\n",
    "train.data = trainPipe.transform(train.data)\n",
    "\n",
    "# capture outliers\n",
    "iqrOutliers = np.array(sorted(trainPipe.named_steps[\"outlier\"].outliers_))\n",
    "print(iqrOutliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T21:36:39.350224Z",
     "start_time": "2019-07-20T21:36:39.335392Z"
    }
   },
   "outputs": [],
   "source": [
    "# identify outliers that are identified in multiple algorithms\n",
    "# reduce(np.intersect1d, (iqrOutliers, ifOutliers, eifOutliers))\n",
    "reduce(np.intersect1d, (ifOutliers, eifOutliers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review outlier identification summary\n",
    "outlierSummary = train.outlierSummary(iqrOutliers=iqrOutliers,\n",
    "                             ifOutliers=ifOutliers,\n",
    "                             eifOutliers=eifOutliers\n",
    "                            )\n",
    "outlierSummary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'remove1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remove outlers from predictors and response\n",
    "# outliers = np.array([59,121])\n",
    "# train.data = train.data.drop(outliers)\n",
    "# train.target = train.target.drop(index=outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Data evaluation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Feature-importance'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T21:36:39.454739Z",
     "start_time": "2019-07-20T21:36:39.374063Z"
    }
   },
   "outputs": [],
   "source": [
    "# feature importance summary table\n",
    "featureImp = train.featureImportanceSummary()\n",
    "featureImp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rationality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Rationality'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T21:36:40.102808Z",
     "start_time": "2019-07-20T21:36:39.459505Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# percent difference summary\n",
    "dfDiff = abs(\n",
    "    (\n",
    "        ((valid.data.describe() + 1) - (train.data.describe() + 1))\n",
    "        / (train.data.describe() + 1)\n",
    "    )\n",
    "    * 100\n",
    ")\n",
    "dfDiff = dfDiff[dfDiff.columns].replace({0: np.nan})\n",
    "dfDiff[dfDiff < 0] = np.nan\n",
    "dfDiff = dfDiff.fillna(\"\")\n",
    "display(dfDiff)\n",
    "display(train.data[dfDiff.columns].describe())\n",
    "display(valid.data[dfDiff.columns].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value override"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Value override'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T21:36:40.123814Z",
     "start_time": "2019-07-20T21:36:40.108174Z"
    }
   },
   "outputs": [],
   "source": [
    "# change clearly erroneous value to what it probably was\n",
    "# exploreValid.data['GarageYrBlt'].replace({2207 : 2007}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous feature EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Continuous-feature-EDA3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Correlation3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T21:36:40.475944Z",
     "start_time": "2019-07-20T21:36:40.127617Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# correlation heat map with most highly correlated features relative to the target\n",
    "p = PrettierPlot()\n",
    "ax = p.makeCanvas()\n",
    "p.prettyCorrHeatmapTarget(df=train.data, target=train.target, thresh=0.2, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Modeling'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Data-preparation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T21:36:40.825475Z",
     "start_time": "2019-07-20T21:36:40.481098Z"
    },
    "code_folding": [
     15
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import training data\n",
    "dfTrain = pd.read_csv(\"../../data/kaggleTitanic/train.csv\")\n",
    "train = mlm.Machine(\n",
    "    data=dfTrain,\n",
    "    target=[\"Survived\"],\n",
    "    removeFeatures=[\"PassengerId\", \"Ticket\"],\n",
    "    overrideCat=[\"Pclass\", \"SibSp\", \"Parch\"],\n",
    "    targetType=\"categorical\",\n",
    ")\n",
    "\n",
    "\n",
    "# remove outliers\n",
    "outliers = np.array([27, 88, 258, 311, 341, 438, 679, 737, 742])\n",
    "train.data = train.data.drop(train.data.index[outliers])\n",
    "train.target = train.target.drop(index=outliers)\n",
    "\n",
    "### pipeline\n",
    "trainPipe = pipeline.Pipeline([\n",
    "        ('imputeMedian', train.ContextImputer(nullCol = 'Age', contextCol = 'Parch', strategy = 'median'))     \n",
    "        ,('imputeMode', train.ModeImputer(cols = ['Embarked']))\n",
    "        ,('customBin', train.CustomBinner(customBinDict = customBinDict))\n",
    "        ,('percentileBin', train.PercentileBinner(cols = ['Age','Fare'], percs = [10, 25, 50, 75, 90]))    \n",
    "        ,('encodeOrdinal', train.CustomOrdinalEncoder(encodings = ordCatCols))    \n",
    "        ,('dummyNominal', train.Dummies(cols = nomCatCols, dropFirst = True))\n",
    "        ,('skew', train.SkewTransform(cols = train.featureByDtype_['continuous'], skewMin = 0.75, pctZeroMax = 1.0))    \n",
    "    ])\n",
    "train.data = trainPipe.transform(train.data)\n",
    "\n",
    "# drop features\n",
    "train.featureDropper(cols=[\"Name\", \"Cabin\"])\n",
    "print('completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T21:36:41.160927Z",
     "start_time": "2019-07-20T21:36:40.833299Z"
    },
    "code_folding": [
     13
    ]
   },
   "outputs": [],
   "source": [
    "### import valid data\n",
    "dfValid = pd.read_csv(\"../../data/kaggleTitanic/test.csv\")\n",
    "valid = mlm.Machine(\n",
    "    data=dfValid,\n",
    "    removeFeatures=[\"PassengerId\", \"Ticket\"],\n",
    "    overrideCat=[\"Pclass\", \"SibSp\", \"Parch\"],\n",
    ")\n",
    "\n",
    "### pipeline\n",
    "validPipe = pipeline.Pipeline(\n",
    "    [\n",
    "        (\"imputeMedian\",valid.ContextImputer(nullCol=\"Age\",contextCol=\"Parch\",train=False,trainValue=trainPipe.named_steps[\"imputeMedian\"].trainValue_)),\n",
    "        (\"imputeMedian2\",valid.NumericalImputer(cols=[\"Fare\", \"Age\"], strategy=\"median\",train=False,trainValue=train.data)),\n",
    "        (\"customBin\", valid.CustomBinner(customBinDict=customBinDict)),\n",
    "        (\"percentileBin\",valid.PercentileBinner(train=False, trainValue=trainPipe.named_steps[\"percentileBin\"].trainValue_)),\n",
    "        (\"encodeOrdinal\", valid.CustomOrdinalEncoder(encodings=ordCatCols)),\n",
    "        (\"dummyNominal\", valid.Dummies(cols=nomCatCols, dropFirst=False)),\n",
    "        (\"levels\", valid.MissingDummies(trainCols=train.data.columns)),\n",
    "        (\"skew\",valid.SkewTransform(train=False, trainValue=trainPipe.named_steps[\"skew\"].trainValue_)),\n",
    "    ]\n",
    ")\n",
    "valid.data = validPipe.transform(valid.data)\n",
    "print('completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian hyper-parameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Bayesian-hyper-parameter-optimization'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T21:36:41.216961Z",
     "start_time": "2019-07-20T21:36:41.167069Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# parameter space\n",
    "allSpace = {\n",
    "    \"lightgbm.LGBMClassifier\": {\n",
    "        \"class_weight\": hp.choice(\"class_weight\", [None]),\n",
    "        \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.4, 0.7),\n",
    "        \"boosting_type\": hp.choice(\"boosting_type\", [\"dart\"]),\n",
    "        \"subsample\": hp.uniform(\"subsample\", 0.5, 1),\n",
    "        \"learning_rate\": hp.uniform(\"learning_rate\", 0.15, 0.25),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(4, 20, dtype=int)),\n",
    "        \"min_child_samples\": hp.quniform(\"min_child_samples\", 50, 150, 5),\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 4000, 10, dtype=int)),\n",
    "        \"num_leaves\": hp.quniform(\"num_leaves\", 30, 70, 1),\n",
    "        \"reg_alpha\": hp.uniform(\"reg_alpha\", 0.75, 1.25),\n",
    "        \"reg_lambda\": hp.uniform(\"reg_lambda\", 0.0, 1.0),\n",
    "        \"subsample_for_bin\": hp.quniform(\"subsample_for_bin\", 100000, 350000, 20000),\n",
    "    },\n",
    "    \"linear_model.LogisticRegression\": {\n",
    "        \"C\": hp.uniform(\"C\", 0.04, 0.1),\n",
    "        \"penalty\": hp.choice(\"penalty\", [\"l1\"]),\n",
    "    },\n",
    "    \"xgboost.XGBClassifier\": {\n",
    "        \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.4, 0.7),\n",
    "        \"gamma\": hp.quniform(\"gamma\", 0.0, 10, 0.05),\n",
    "        \"learning_rate\": hp.quniform(\"learning_rate\", 0.01, 0.2, 0.01),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 15, dtype=int)),\n",
    "        \"min_child_weight\": hp.quniform(\"min_child_weight\", 2.5, 7.5, 1),\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 4000, 10, dtype=int)),\n",
    "        \"subsample\": hp.uniform(\"subsample\", 0.4, 0.7),\n",
    "    },\n",
    "    \"ensemble.RandomForestClassifier\": {\n",
    "        \"bootstrap\": hp.choice(\"bootstrap\", [True, False]),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 10, dtype=int)),\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 8000, 10, dtype=int)),\n",
    "        \"max_features\": hp.choice(\"max_features\", [\"sqrt\"]),\n",
    "        \"min_samples_split\": hp.choice(\n",
    "            \"min_samples_split\", np.arange(15, 25, dtype=int)\n",
    "        ),\n",
    "        \"min_samples_leaf\": hp.choice(\"min_samples_leaf\", np.arange(2, 20, dtype=int)),\n",
    "    },\n",
    "    \"ensemble.GradientBoostingClassifier\": {\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 4000, 10, dtype=int)),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 11, dtype=int)),\n",
    "        \"max_features\": hp.choice(\"max_features\", [\"sqrt\"]),\n",
    "        \"learning_rate\": hp.quniform(\"learning_rate\", 0.01, 0.09, 0.01),\n",
    "        \"loss\": hp.choice(\"loss\", [\"deviance\", \"exponential\"]),\n",
    "        \"min_samples_split\": hp.choice(\n",
    "            \"min_samples_split\", np.arange(2, 40, dtype=int)\n",
    "        ),\n",
    "        \"min_samples_leaf\": hp.choice(\"min_samples_leaf\", np.arange(2, 40, dtype=int)),\n",
    "    },\n",
    "    \"ensemble.AdaBoostClassifier\": {\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 4000, 10, dtype=int)),\n",
    "        \"learning_rate\": hp.quniform(\"learning_rate\", 0.1, 0.25, 0.01),\n",
    "        \"algorithm\": hp.choice(\"algorithm\", [\"SAMME\"]),\n",
    "    },\n",
    "    \"naive_bayes.BernoulliNB\": {\"alpha\": hp.uniform(\"alpha\", 0.01, 2)},\n",
    "    \"ensemble.BaggingClassifier\": {\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 4000, 10, dtype=int)),\n",
    "        \"max_samples\": hp.uniform(\"max_samples\", 0.01, 0.3),\n",
    "    },\n",
    "    \"ensemble.ExtraTreesClassifier\": {\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 4000, 10, dtype=int)),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 15, dtype=int)),\n",
    "        \"min_samples_split\": hp.choice(\n",
    "            \"min_samples_split\", np.arange(4, 30, dtype=int)\n",
    "        ),\n",
    "        \"min_samples_leaf\": hp.choice(\"min_samples_leaf\", np.arange(2, 20, dtype=int)),\n",
    "        \"max_features\": hp.choice(\"max_features\", [\"auto\"]),\n",
    "        \"criterion\": hp.choice(\"criterion\", [\"entropy\"]),\n",
    "    },\n",
    "    \"svm.SVC\": {\n",
    "        \"C\": hp.uniform(\"C\", 4, 15),\n",
    "        \"decision_function_shape\": hp.choice(\"decision_function_shape\", [\"ovr\"]),\n",
    "        \"gamma\": hp.uniform(\"gamma\", 0.00000001, 1.5),\n",
    "    },\n",
    "    \"neighbors.KNeighborsClassifier\": {\n",
    "        \"algorithm\": hp.choice(\"algorithm\", [\"ball_tree\", \"brute\"]),\n",
    "        \"n_neighbors\": hp.choice(\"n_neighbors\", np.arange(1, 15, dtype=int)),\n",
    "        \"weights\": hp.choice(\"weights\", [\"uniform\"]),\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T06:10:48.964508Z",
     "start_time": "2019-07-20T04:43:54.172908Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# execute bayesian optimization grid search\n",
    "analysis = \"titanic\"\n",
    "train.execBayesOptimSearch(\n",
    "    allSpace=allSpace,\n",
    "    resultsDir=\"{}_hyperopt_{}.csv\".format(rundate, analysis),\n",
    "    X=train.data,\n",
    "    y=train.target,\n",
    "    scoring=\"accuracy\",\n",
    "    n_folds=5,\n",
    "    n_jobs=4,\n",
    "    iters=100,\n",
    "    verbose=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model loss by iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Model-loss-by-iteration'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T21:36:54.668306Z",
     "start_time": "2019-07-20T21:36:54.259967Z"
    }
   },
   "outputs": [],
   "source": [
    "# read scores summary table\n",
    "analysis = \"titanic\"\n",
    "\n",
    "resultsRaw = pd.read_csv(\"{}_hyperopt_{}.csv\".format(rundate, analysis), na_values=\"nan\")\n",
    "resultsDict = train.unpackRawParams(resultsRaw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T21:30:59.839867Z",
     "start_time": "2019-07-20T21:30:54.281216Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loss plot\n",
    "train.lossPlot(resultsAsDict=resultsDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter selection by iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Parameter-selection-by-iteration'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-19T01:47:36.876677Z",
     "start_time": "2019-07-19T01:47:23.026068Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# estimator parameter plots\n",
    "train.paramPlot(resultsAsDict=resultsDict, allSpace=allSpace, nIter=100, chartProp=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T04:23:45.411641Z",
     "start_time": "2019-07-18T04:23:44.926648Z"
    }
   },
   "outputs": [],
   "source": [
    "sampleSpace = {\n",
    "                'param': hp.uniform('param', np.log(0.4), np.log(0.6))\n",
    "#     \"\": 0.000001 + hp.uniform(\"gamma\", 0.000001, 10)\n",
    "    #             'param2': hp.loguniform('param2', np.log(0.001), np.log(0.01))\n",
    "}\n",
    "\n",
    "train.samplePlot(sampleSpace, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model performance evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Model-performance-evaluation'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T21:36:59.047576Z",
     "start_time": "2019-07-20T21:36:59.013711Z"
    }
   },
   "outputs": [],
   "source": [
    "def topModels(resultsRaw, numModels):\n",
    "    models = {}\n",
    "    for estimator in resultsRaw[\"estimator\"].unique():\n",
    "        estDf = resultsRaw[resultsRaw[\"estimator\"] == estimator].sort_values(\n",
    "            [\"mean\"], ascending=[False]\n",
    "        )[\"iteration\"][:numModels]\n",
    "        models[estimator] = estDf.values.tolist()\n",
    "    return models\n",
    "\n",
    "models = topModels(resultsRaw=resultsRaw, numModels=1)\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Classification-report'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T21:37:19.341306Z",
     "start_time": "2019-07-20T21:37:02.406940Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create classification reports\n",
    "for estimator, ixs in models.items():\n",
    "    for ix in ixs:\n",
    "        # extract params and instantiate model\n",
    "        params = train.bayesOptimModelBuilder(\n",
    "            resultsRaw=resultsRaw, estimator=estimator, iteration=ix\n",
    "        )\n",
    "        model = eval(\"{0}(**{1})\".format(estimator, params))\n",
    "        \n",
    "        # fit model and make predictions\n",
    "        model.fit(train.data, train.target)\n",
    "        yPred = model.predict(train.data)\n",
    "        print('*' * 50)\n",
    "        print(\"Model: {}\\nParameter set: {}\\n\".format(estimator.split('.')[1], ix))\n",
    "        print(metrics.classification_report(train.target, yPred, labels = [0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Confusion-matrix'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T21:37:46.517652Z",
     "start_time": "2019-07-20T21:37:25.243187Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# confusion matrices\n",
    "for estimator, ixs in models.items():\n",
    "    for ix in ixs:\n",
    "        # extract params and instantiate model\n",
    "        params = train.bayesOptimModelBuilder(\n",
    "            resultsRaw=resultsRaw, estimator=estimator, iteration=ix\n",
    "        )\n",
    "        model = eval(\"{0}(**{1})\".format(estimator, params))\n",
    "        \n",
    "        # fit model and make predictions\n",
    "        model.fit(train.data, train.target)\n",
    "        yPred = model.predict(train.data)\n",
    "        \n",
    "        # visualize results with confusion matrix\n",
    "        p = PrettierPlot()\n",
    "        ax = p.makeCanvas(title=\"Model: {}\\nParameter set: {}\".format(estimator.split('.')[1], ix), xLabel=\"Predicted\", yLabel=\"Actual\", yShift=0.5, xShift = 0.35)\n",
    "        p.prettyConfusionMatrix(yTrue=train.target, yPred=yPred, labels = ['Survived','Died'], ax=ax)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'ROC-curve'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T21:39:34.730469Z",
     "start_time": "2019-07-20T21:39:34.726577Z"
    }
   },
   "outputs": [],
   "source": [
    "params['probability'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T21:39:38.860324Z",
     "start_time": "2019-07-20T21:39:38.853696Z"
    }
   },
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T21:41:46.343614Z",
     "start_time": "2019-07-20T21:41:11.833287Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ROC curve\n",
    "for estimator, ixs in models.items():\n",
    "    for ix in ixs:\n",
    "        # extract params and instantiate model\n",
    "        params = train.bayesOptimModelBuilder(\n",
    "            resultsRaw=resultsRaw, estimator=estimator, iteration=ix\n",
    "        )\n",
    "        if estimator == 'svm.SVC':\n",
    "            params['probability'] = True\n",
    "        \n",
    "        model = eval(\"{0}(**{1})\".format(estimator, params))\n",
    "        \n",
    "        # fit model and make predictions\n",
    "        model.fit(train.data, train.target)\n",
    "        yPred = model.predict(train.data)\n",
    "        \n",
    "        # plot ROC curves\n",
    "        p = PrettierPlot(chartProp=12,plotOrientation=\"square\")\n",
    "        ax = p.makeCanvas(\n",
    "            title=\"Model: {}\\nParameter set: {}\".format(estimator.split('.')[1], ix),\n",
    "            xLabel=\"false positive rate\",\n",
    "            yLabel=\"true positive rate\",\n",
    "            yShift=0.64,\n",
    "        )\n",
    "        p.prettyRocCurve(\n",
    "            model=model,\n",
    "            XTrain=train.data,\n",
    "            yTrain=train.target,\n",
    "            linecolor=style.styleHexMid[0],\n",
    "            ax=ax,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T21:43:38.706148Z",
     "start_time": "2019-07-20T21:42:43.060338Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cross-validated ROC curve\n",
    "for estimator, ixs in models.items():\n",
    "    for ix in ixs:\n",
    "        # extract params and instantiate model\n",
    "        params = train.bayesOptimModelBuilder(\n",
    "            resultsRaw=resultsRaw, estimator=estimator, iteration=ix\n",
    "        )\n",
    "        if estimator == 'svm.SVC':\n",
    "            params['probability'] = True\n",
    "        \n",
    "        model = eval(\"{0}(**{1})\".format(estimator, params))\n",
    "        \n",
    "        # fit model and make predictions\n",
    "        model.fit(train.data, train.target)\n",
    "        yPred = model.predict(train.data)\n",
    "        \n",
    "        # plot and ROC curve using only two features from the breast cancer dataset\n",
    "        cv = list(\n",
    "            model_selection.StratifiedKFold(n_splits=3, random_state=1).split(train.data, train.target)\n",
    "        )\n",
    "\n",
    "        # plot ROC curves\n",
    "        p = PrettierPlot(chartProp=12,plotOrientation=\"square\")\n",
    "        ax = p.makeCanvas(\n",
    "            title=\"Model: {}\\nParameter set: {}\".format(estimator.split('.')[1], ix),\n",
    "            xLabel=\"false positive rate\",\n",
    "            yLabel=\"true positive rate\",\n",
    "            yShift=0.62,\n",
    "        )\n",
    "        for i, (trainIx, testIx) in enumerate(cv):\n",
    "            XTrainCV = train.data.iloc[trainIx]#.values\n",
    "            yTrainCV = train.target.iloc[trainIx]#.values\n",
    "\n",
    "            p.prettyRocCurve(\n",
    "                model=model,\n",
    "                XTrain=XTrainCV,\n",
    "                yTrain=yTrainCV,\n",
    "                linecolor=style.styleHexMid[i],\n",
    "                ax=ax,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model explanability\n",
    "\n",
    "https://www.kaggle.com/learn/machine-learning-explainability\n",
    "https://www.kaggle.com/dansbecker/partial-dependence-plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Feature-importance'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Permutation-importance'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T02:02:16.726938Z",
     "start_time": "2019-07-01T02:02:15.740Z"
    }
   },
   "outputs": [],
   "source": [
    "# permutation importance\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "perm = PermutationImportance(my_model, random_state=1).fit(val_X, val_y)\n",
    "eli5.show_weights(perm, feature_names=val_X.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Partial-plots'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T02:02:16.728503Z",
     "start_time": "2019-07-01T02:02:15.747Z"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "from matplotlib import pyplot as plt\n",
    "from pdpbox import pdp, get_dataset, info_plots\n",
    "\n",
    "# Create the data that we will plot\n",
    "pdp_goals = pdp.pdp_isolate(\n",
    "    model=tree_model, dataset=val_X, model_features=feature_names, feature=\"Goal Scored\"\n",
    ")\n",
    "\n",
    "# plot it\n",
    "pdp.pdp_plot(pdp_goals, \"Goal Scored\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T02:02:16.729970Z",
     "start_time": "2019-07-01T02:02:15.753Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_to_plot = \"Distance Covered (Kms)\"\n",
    "pdp_dist = pdp.pdp_isolate(\n",
    "    model=tree_model,\n",
    "    dataset=val_X,\n",
    "    model_features=feature_names,\n",
    "    feature=feature_to_plot,\n",
    ")\n",
    "\n",
    "pdp.pdp_plot(pdp_dist, feature_to_plot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T02:02:16.731442Z",
     "start_time": "2019-07-01T02:02:15.759Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build Random Forest model\n",
    "rf_model = RandomForestClassifier(random_state=0).fit(train_X, train_y)\n",
    "\n",
    "pdp_dist = pdp.pdp_isolate(\n",
    "    model=rf_model, dataset=val_X, model_features=feature_names, feature=feature_to_plot\n",
    ")\n",
    "\n",
    "pdp.pdp_plot(pdp_dist, feature_to_plot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T02:02:16.733388Z",
     "start_time": "2019-07-01T02:02:15.764Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2D plots\n",
    "# Similar to previous PDP plot except we use pdp_interact instead of pdp_isolate and pdp_interact_plot instead of pdp_isolate_plot\n",
    "features_to_plot = [\"Goal Scored\", \"Distance Covered (Kms)\"]\n",
    "inter1 = pdp.pdp_interact(\n",
    "    model=tree_model,\n",
    "    dataset=val_X,\n",
    "    model_features=feature_names,\n",
    "    features=features_to_plot,\n",
    ")\n",
    "\n",
    "pdp.pdp_interact_plot(\n",
    "    pdp_interact_out=inter1, feature_names=features_to_plot, plot_type=\"contour\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'SHAP-values'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T02:02:16.735337Z",
     "start_time": "2019-07-01T02:02:15.771Z"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "row_to_show = 5\n",
    "data_for_prediction = val_X.iloc[\n",
    "    row_to_show\n",
    "]  # use 1 row of data here. Could use multiple rows if desired\n",
    "data_for_prediction_array = data_for_prediction.values.reshape(1, -1)\n",
    "\n",
    "\n",
    "my_model.predict_proba(data_for_prediction_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T02:02:16.737217Z",
     "start_time": "2019-07-01T02:02:15.778Z"
    }
   },
   "outputs": [],
   "source": [
    "import shap  # package used to calculate Shap values\n",
    "\n",
    "# Create object that can calculate shap values\n",
    "explainer = shap.TreeExplainer(my_model)\n",
    "\n",
    "# Calculate Shap values\n",
    "shap_values = explainer.shap_values(data_for_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T02:02:16.738957Z",
     "start_time": "2019-07-01T02:02:15.784Z"
    }
   },
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T02:02:16.740929Z",
     "start_time": "2019-07-01T02:02:15.789Z"
    }
   },
   "outputs": [],
   "source": [
    "# use Kernel SHAP to explain test set predictions\n",
    "k_explainer = shap.KernelExplainer(my_model.predict_proba, train_X)\n",
    "k_shap_values = k_explainer.shap_values(data_for_prediction)\n",
    "shap.force_plot(k_explainer.expected_value[1], k_shap_values[1], data_for_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T02:02:16.747234Z",
     "start_time": "2019-07-01T02:02:15.795Z"
    }
   },
   "outputs": [],
   "source": [
    "shap.DeepExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T02:02:16.748842Z",
     "start_time": "2019-07-01T02:02:15.800Z"
    }
   },
   "outputs": [],
   "source": [
    "import shap  # package used to calculate Shap values\n",
    "\n",
    "# Create object that can calculate shap values\n",
    "explainer = shap.TreeExplainer(my_model)\n",
    "\n",
    "# calculate shap values. This is what we will plot.\n",
    "# Calculate shap_values for all of val_X rather than a single row, to have more data for plot.\n",
    "shap_values = explainer.shap_values(val_X)\n",
    "\n",
    "# Make plot. Index of [1] is explained in text below.\n",
    "shap.summary_plot(shap_values[1], val_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T02:02:16.750211Z",
     "start_time": "2019-07-01T02:02:15.806Z"
    }
   },
   "outputs": [],
   "source": [
    "import shap  # package used to calculate Shap values\n",
    "\n",
    "# Create object that can calculate shap values\n",
    "explainer = shap.TreeExplainer(my_model)\n",
    "\n",
    "# calculate shap values. This is what we will plot.\n",
    "shap_values = explainer.shap_values(X)\n",
    "\n",
    "# make plot.\n",
    "shap.dependence_plot(\n",
    "    \"Ball Possession %\", shap_values[1], X, interaction_index=\"Goal Scored\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Stacking'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primary models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Primary-models'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T04:08:08.084300Z",
     "start_time": "2019-07-18T04:08:08.069443Z"
    }
   },
   "outputs": [],
   "source": [
    "resultsRaw[resultsRaw[\"estimator\"] == \"xgboost.XGBClassifier\"].sort_values(\n",
    "    [\"mean\"], ascending=[False]\n",
    ")[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T04:08:42.303668Z",
     "start_time": "2019-07-18T04:08:42.292269Z"
    }
   },
   "outputs": [],
   "source": [
    "def topParamSelector(resultsRaw, num):\n",
    "    models = {}\n",
    "    for estimator in resultsRaw[\"estimator\"].unique():\n",
    "        estDf = resultsRaw[resultsRaw[\"estimator\"] == estimator].sort_values(\n",
    "            [\"mean\"], ascending=[False]\n",
    "        )[\"iteration\"][:num]\n",
    "        models[estimator] = estDf.values.tolist()\n",
    "    return models\n",
    "\n",
    "\n",
    "models = topParamSelector(resultsRaw=resultsRaw, num=1)\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T04:08:46.667940Z",
     "start_time": "2019-07-18T04:08:46.519318Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get out-of-fold predictions\n",
    "oofTrain, oofValid, columns = train.modelStacker(\n",
    "    models=models,\n",
    "    resultsRaw=resultsRaw,\n",
    "    XTrain=train.data.values,\n",
    "    yTrain=train.target,\n",
    "    XValid=valid.data.values,\n",
    "    nFolds=2,\n",
    "    nJobs=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T02:02:16.758058Z",
     "start_time": "2019-07-01T02:02:15.833Z"
    }
   },
   "outputs": [],
   "source": [
    "# view correlations of predictions\n",
    "p = PrettierPlot()\n",
    "ax = p.makeCanvas()\n",
    "p.prettyCorrHeatmap(df=pd.DataFrame(oofTrain, columns=columns), annot=True, ax=ax, vmin=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Meta-model'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T02:02:16.777169Z",
     "start_time": "2019-07-01T02:02:15.841Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# parameter space\n",
    "allSpace = {\n",
    "    \"lightgbm.LGBMClassifier\": {\n",
    "        \"class_weight\": hp.choice(\"class_weight\", [None]),\n",
    "        \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.4, 0.7),\n",
    "        \"boosting_type\": hp.choice(\"boosting_type\", [\"dart\"]),\n",
    "        \"subsample\": hp.uniform(\"subsample\", 0.5, 1),\n",
    "        \"learning_rate\": hp.uniform(\"learning_rate\", 0.15, 0.25),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(4, 20, dtype=int)),\n",
    "        \"min_child_samples\": hp.quniform(\"min_child_samples\", 50, 150, 5),\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 4000, 10, dtype=int)),\n",
    "        \"num_leaves\": hp.quniform(\"num_leaves\", 30, 70, 1),\n",
    "        \"reg_alpha\": hp.uniform(\"reg_alpha\", 0.75, 1.25),\n",
    "        \"reg_lambda\": hp.uniform(\"reg_lambda\", 0.0, 1.0),\n",
    "        \"subsample_for_bin\": hp.quniform(\"subsample_for_bin\", 100000, 350000, 20000),\n",
    "    },\n",
    "    \"linear_model.LogisticRegression\": {\n",
    "        \"C\": hp.uniform(\"C\", 0.04, 0.1),\n",
    "        \"penalty\": hp.choice(\"penalty\", [\"l1\"]),\n",
    "    },\n",
    "    \"xgboost.XGBClassifier\": {\n",
    "        \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.4, 0.7),\n",
    "        \"gamma\": hp.quniform(\"gamma\", 0.0, 10, 0.05),\n",
    "        \"learning_rate\": hp.quniform(\"learning_rate\", 0.01, 0.2, 0.01),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 15, dtype=int)),\n",
    "        \"min_child_weight\": hp.quniform(\"min_child_weight\", 2.5, 7.5, 1),\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 4000, 10, dtype=int)),\n",
    "        \"subsample\": hp.uniform(\"subsample\", 0.4, 0.7),\n",
    "    },\n",
    "    \"ensemble.RandomForestClassifier\": {\n",
    "        \"bootstrap\": hp.choice(\"bootstrap\", [True, False]),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 10, dtype=int)),\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 8000, 10, dtype=int)),\n",
    "        \"max_features\": hp.choice(\"max_features\", [\"sqrt\"]),\n",
    "        \"min_samples_split\": hp.choice(\n",
    "            \"min_samples_split\", np.arange(15, 25, dtype=int)\n",
    "        ),\n",
    "        \"min_samples_leaf\": hp.choice(\"min_samples_leaf\", np.arange(2, 20, dtype=int)),\n",
    "    },\n",
    "    \"ensemble.GradientBoostingClassifier\": {\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 4000, 10, dtype=int)),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 11, dtype=int)),\n",
    "        \"max_features\": hp.choice(\"max_features\", [\"sqrt\"]),\n",
    "        \"learning_rate\": hp.quniform(\"learning_rate\", 0.01, 0.09, 0.01),\n",
    "        \"loss\": hp.choice(\"loss\", [\"deviance\", \"exponential\"]),\n",
    "        \"min_samples_split\": hp.choice(\n",
    "            \"min_samples_split\", np.arange(2, 40, dtype=int)\n",
    "        ),\n",
    "        \"min_samples_leaf\": hp.choice(\"min_samples_leaf\", np.arange(2, 40, dtype=int)),\n",
    "    },\n",
    "    \"ensemble.AdaBoostClassifier\": {\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 4000, 10, dtype=int)),\n",
    "        \"learning_rate\": hp.quniform(\"learning_rate\", 0.1, 0.25, 0.01),\n",
    "        \"algorithm\": hp.choice(\"algorithm\", [\"SAMME\"]),\n",
    "    },\n",
    "    \"naive_bayes.BernoulliNB\": {\"alpha\": hp.uniform(\"alpha\", 0.01, 2)},\n",
    "    \"ensemble.BaggingClassifier\": {\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 4000, 10, dtype=int)),\n",
    "        \"max_samples\": hp.uniform(\"max_samples\", 0.01, 0.3),\n",
    "    },\n",
    "    \"ensemble.ExtraTreesClassifier\": {\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 4000, 10, dtype=int)),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 15, dtype=int)),\n",
    "        \"min_samples_split\": hp.choice(\n",
    "            \"min_samples_split\", np.arange(4, 30, dtype=int)\n",
    "        ),\n",
    "        \"min_samples_leaf\": hp.choice(\"min_samples_leaf\", np.arange(2, 20, dtype=int)),\n",
    "        \"max_features\": hp.choice(\"max_features\", [\"auto\"]),\n",
    "        \"criterion\": hp.choice(\"criterion\", [\"entropy\"]),\n",
    "    },\n",
    "    \"svm.SVC\": {\n",
    "        \"C\": hp.uniform(\"C\", 0.00000001, 15),\n",
    "        \"decision_function_shape\": hp.choice(\"decision_function_shape\", [\"ovr\", \"ovo\"]),\n",
    "        \"gamma\": hp.uniform(\"gamma\", 0.00000001, 1.5),\n",
    "    },\n",
    "    \"neighbors.KNeighborsClassifier\": {\n",
    "        \"algorithm\": hp.choice(\"algorithm\", [\"ball_tree\", \"brute\"]),\n",
    "        \"n_neighbors\": hp.choice(\"n_neighbors\", np.arange(1, 15, dtype=int)),\n",
    "        \"weights\": hp.choice(\"weights\", [\"uniform\"]),\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T02:02:16.779160Z",
     "start_time": "2019-07-01T02:02:15.847Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# execute bayesian optimization grid search\n",
    "train.execBayesOptimSearch(\n",
    "    allSpace=allSpace,\n",
    "    resultsDir=\"data/{}_hyperopt_meta_{}_2.csv\".format(rundate, analysis),\n",
    "    X=oofTrain,\n",
    "    y=train.target,\n",
    "    scoring=\"accuracy\",\n",
    "    n_folds=8,\n",
    "    n_jobs=8,\n",
    "    iters=3000,\n",
    "    verbose=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T02:02:16.781225Z",
     "start_time": "2019-07-01T02:02:15.853Z"
    }
   },
   "outputs": [],
   "source": [
    "# read scores summary table\n",
    "resultsMetaDf = pd.read_csv(\n",
    "    \"data/20190423_hyperopt_meta_titanic_2.csv\", na_values=\"nan\"\n",
    ")\n",
    "resultsMeta = train.unpackParams(resultsMetaDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T02:02:16.783163Z",
     "start_time": "2019-07-01T02:02:15.858Z"
    }
   },
   "outputs": [],
   "source": [
    "# loss plot\n",
    "train.lossPlot(resultsDf=resultsMeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T02:02:16.784904Z",
     "start_time": "2019-07-01T02:02:15.864Z"
    }
   },
   "outputs": [],
   "source": [
    "# estimator parameter plots\n",
    "train.paramPlot(results=resultsMeta, allSpace=allSpace, nIter=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Submission'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Standard'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T02:02:16.787282Z",
     "start_time": "2019-07-01T02:02:15.874Z"
    }
   },
   "outputs": [],
   "source": [
    "## standard model fit and predict\n",
    "# select estimator and iteration\n",
    "# estimator = 'ensemble.RandomForestClassifier'\n",
    "# iteration = 1955\n",
    "# estimator = 'xgboost.XGBClassifier'\n",
    "# iteration = 2097\n",
    "estimator = \"lightgbm.LGBMClassifier\"\n",
    "iteration = 2264\n",
    "\n",
    "# extract params and instantiate model\n",
    "params = train.bayesOptimModelBuilder(\n",
    "    resultsDf=resultsDf, estimator=estimator, iteration=iteration\n",
    ")\n",
    "model = eval(\"{0}(**{1})\".format(estimator, params))\n",
    "\n",
    "# fit model and make predictions\n",
    "model.fit(train.data, train.target)\n",
    "yPred = model.predict(valid.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T02:02:16.789392Z",
     "start_time": "2019-07-01T02:02:15.880Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate prediction submission file\n",
    "my_submission = pd.DataFrame({\"PassengerId\": dfValid.PassengerId, \"Survived\": yPred})\n",
    "my_submission.to_csv(\"data/submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Stack'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T02:02:16.791607Z",
     "start_time": "2019-07-01T02:02:15.888Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resultsMetaDf.sort_values([\"mean\"], ascending=[False])[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T02:02:16.794001Z",
     "start_time": "2019-07-01T02:02:15.894Z"
    }
   },
   "outputs": [],
   "source": [
    "# best second level learning model\n",
    "# estimator = 'xgboost.XGBClassifier'\n",
    "# estimator = 'ensemble.RandomForestClassifier'\n",
    "# estimator = 'ensemble.GradientBoostingClassifier'\n",
    "estimator = \"svm.SVC\"\n",
    "\n",
    "iteration = 2436\n",
    "\n",
    "# extract params and instantiate model\n",
    "params = train.paramExtractor(\n",
    "    resultsDf=resultsMetaDf, estimator=estimator, iteration=iteration\n",
    ")\n",
    "model = eval(\"{0}(**{1})\".format(estimator, params))\n",
    "\n",
    "model.fit(oofTrain, train.target)\n",
    "yPred = model.predict(oofValid)\n",
    "print(sum(yPred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T02:02:16.796268Z",
     "start_time": "2019-07-01T02:02:15.899Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate prediction submission file\n",
    "my_submission = pd.DataFrame({\"PassengerId\": dfValid.PassengerId, \"Survived\": yPred})\n",
    "my_submission.to_csv(\"data/submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
