{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Chapter 16 - Modeling Sequential Data Using Recurrent Neural Networks__\n",
    "\n",
    "1. [Introducing sequential data](#Introducing-sequential-data)\n",
    "    1. [Modeling sequential data – order matters](#Modeling-sequential-data–order-matters)\n",
    "    1. [Representing sequences](#Representing-sequences)\n",
    "    1. [The different categories of sequence modeling](#The-different-categories-of-sequence-modeling)\n",
    "1. [RNNs for modeling sequences](#RNNs-for-modeling-sequences)\n",
    "    1. [Understanding the structure and flow of an RNN](#Understanding-the-structure-and-flow-of-an-RNN)\n",
    "    1. [Computing activations in an RNN](#Computing-activations-in-an-RNN)\n",
    "    1. [The challenges of learning long-range interactions](#The-challenges-of-learning-long-range-interactions)\n",
    "1. [Implementing a multilayer RNN for sequence modeling in TensorFlow](#Implementing-a-multilayer-RNN-for-sequence-modeling-in-TensorFlow)\n",
    "    1. [Project one - performing sentiment analysis of IMDb movie reviews using multilayer RNNs](#performing-sentiment-analysis-of-IMDb-movie-reviews-using-multilayer-RNNs)\n",
    "        1. [Preparing the data](#Preparing-the-data)\n",
    "        1. [Embedding](#Embedding)\n",
    "        1. [Building an RNN model](#Building-an-RNN-model)\n",
    "1. [](#)\n",
    "1. [](#)\n",
    "1. [](#)\n",
    "1. [](#)\n",
    "1. [](#)\n",
    "1. [](#)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-15T19:27:03.668187Z",
     "start_time": "2018-12-15T19:26:59.897013Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Standard libary and settings\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "import itertools\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "dataPath = os.path.abspath(os.path.join('../../Data'))\n",
    "modulePath = os.path.abspath(os.path.join('../../CustomModules'))\n",
    "sys.path.append(modulePath) if modulePath not in sys.path else None\n",
    "from IPython.core.display import display, HTML; display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "\n",
    "# Data extensions and settings\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold = np.inf, suppress = True)\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.options.display.float_format = '{:,.6f}'.format\n",
    "\n",
    "\n",
    "# Modeling extensions\n",
    "import sklearn.base as base\n",
    "import sklearn.cluster as cluster\n",
    "import sklearn.datasets as datasets\n",
    "import sklearn.decomposition as decomposition\n",
    "import sklearn.discriminant_analysis as discriminant_analysis\n",
    "import sklearn.ensemble as ensemble\n",
    "import sklearn.feature_extraction as feature_extraction\n",
    "import sklearn.feature_selection as feature_selection\n",
    "import sklearn.linear_model as linear_model\n",
    "import sklearn.metrics as metrics\n",
    "import sklearn.model_selection as model_selection\n",
    "import sklearn.neighbors as neighbors\n",
    "import sklearn.pipeline as pipeline\n",
    "import sklearn.preprocessing as preprocessing\n",
    "import sklearn.svm as svm\n",
    "import sklearn.tree as tree\n",
    "import sklearn.utils as utils\n",
    "\n",
    "\n",
    "# Visualization extensions and settings\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Custom extensions and settings\n",
    "from quickplot import qp, qpUtil, qpStyle\n",
    "from mlTools import powerGridSearch\n",
    "sns.set(rc = qpStyle.rcGrey)\n",
    "\n",
    "\n",
    "# Magic functions\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Introducing-sequential-data'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing sequential data\n",
    "\n",
    "This chapter explores the unique properties of sequences compared to other kinds of data. We will explore how we can represent sequential data and the various models for analyzing sequential data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Modeling-sequential-data–order-matters'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling sequential data – order matters\n",
    "\n",
    "One major unique aspect of sequential data is that the elements appear in a certain order and are not independent of each other. The contrasts with data and algorithms that we have dealth with up to this point, in that previous models assume that the data is independent and identically distributed (IID). But with sequential data, by definition, order matters. This is not necessarily a problem, and in fact, the order can yield meaningful information. We just need a different approach and different tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Representing sequences'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representing sequences\n",
    "\n",
    "In this chaper, sequences will be represented as $\\big(x^1, x^2,...,x^T\\big)$, where the superscript indices indicate the order of the instances, and the length of the sequence is $T$. For example, in time-series data, sample $\\textbf{x}^T$ belongs to a particular time $t$. Further, if the data is labeled, the labels also follow a form where order matters: $\\big(y^1, y^2,...,y^T\\big)$.\n",
    "\n",
    "The MLP and CNN models built in the last few chapters are not capable of handling the order of the input simples. Recurrent neural networks (RNNs) are designed to model sequences that remember past information and process new events in light of that history."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'The-different-categories-of-sequence-modeling'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The different categories of sequence modeling\n",
    "\n",
    "Sequence modeling can be applied to, among other things, language translateion, image cpationing, and text generation. Sequential data comes in many forms, and the nature of the input and output data determines the type. If neither the input nor the output data is sequenced, then this is simply a standard dataset, any of the methods covered in previous chapter may be used (depending on the problem).\n",
    "\n",
    "If either the input or output data is sequenced, then it can be identified by one of these three categories:\n",
    "\n",
    "- Many-to-one: The input data is sequenced, but the output is a vector of a fixed size, not a sequence. For example, sentiment analysis takes text data as an input and outputs a class label.\n",
    "- One-to-many: The input data is in a standard format (not sequenced) but the output is a seuqnce. For example, in image captioning, the input is an image and the ouput is an English phrase.\n",
    "- Many-to-many: Both the input and output arrays are sequences. This category can be sub-divided into subcategories based on whether the input or output is synchronized or not. An example of synchronized many-to-many is video classification, where each from in a video is labeled. An example of delayed many-to-many is language translation, i.e. an English sentence is translated by a machine into its equivalent in German"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'RNNs-for-modeling-sequences'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNs for modeling sequences\n",
    "\n",
    "This sections describes the foundations of RNNs, including typical structure, dat flow, neuron activation, and typical challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Understanding-the-structure-and-flow-of-an-RNN'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the structure and flow of an RNN\n",
    "\n",
    "In a feedforward network, information flows from the input layer, to the hidden layer(s), then to the output layer. In an RNN, the hidden layer gets its input from both the input layer and the hidden layer from the previous time step. This flow of information in adjacent time steps in the hidden layer allows the network to use its 'memory of past events'. This can be envisioned as a loop, which in graph notation is referred to as a recurrent edge. This can be visualized as:\n",
    "\n",
    "$$\n",
    "\\textbf{x}^t \\rightarrow \\textbf{h}^t \\rightarrow \\textbf{y}^t \n",
    "$$\n",
    "\n",
    "where $\\textbf{x}^t$ is the input data at the $t$ point in the sequence, $\\textbf{h}^t$ is the hidden layer at point $t$, and $\\textbf{y}^t$ at point $t$. This can be unfolded to reveal how other data points observed at adjacent time steps are structured relative to point $t$:\n",
    "\n",
    "$$\n",
    "\\textbf{x}^{t-1} \\rightarrow \\textbf{h}^{t-1} \\rightarrow \\textbf{y}^{t-1}\n",
    "\\\\\n",
    "\\downarrow\n",
    "\\\\\n",
    "\\textbf{x}^t \\rightarrow \\textbf{h}^t \\rightarrow \\textbf{y}^t \n",
    "\\\\\n",
    "\\downarrow\n",
    "\\\\\n",
    "    \\textbf{x}^{t+1} \\rightarrow \\textbf{h}^{t+1} \\rightarrow \\textbf{y}^{t+1}\n",
    "$$\n",
    "\n",
    "RNNs can have multiple hidden layers as well.\n",
    "\n",
    "In a standard neural network, each hidden unit only receives one input - the net input associated with the input layer. RNNs, conversely, neurons in the hidden layer receive two distinct inputs - the net input from the input layer and the net input of the same hidden layer neuron from the previous time step $t-1$. At $t=0$, the first time step, the hidden units are initialized to zeros, or small random numbers. Then for $t>0$, the hidden units get input from the data point at the current time $\\textbf{x}^t$ and the previous values of the hidden units at $t-1$, $\\textbf{h}^{t-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Computing-activations-in-an-RNN'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing activations in an RNN\n",
    "\n",
    "Each directed edge (connection between boxes) of an RNN is associated with a weight matrix, and these weights do not depend on time $t$. These weights are shared across the time axis. The different weight matrices in a single layer RNN are:\n",
    "\n",
    "$$\n",
    "\\textbf{W}_{xh}: \\mbox{the weight matrix between the input} \\ \\textbf{x}^t \\mbox{and the hidden layer } \\ \\textbf{h}\n",
    "\\\\\n",
    "\\textbf{W}_{hh}: \\mbox{the weight matrix associated with the recurrent edge}\n",
    "\\\\\n",
    "\\textbf{W}_{hy}: \\mbox{the weight matrix between the hidden layer and the output layer}\n",
    "$$\n",
    "\n",
    "Again, these weight matrices apply to the current point in the sequence $t$, as well as to $t-1$ and $t+1$.\n",
    "\n",
    "The activations are computed similar to how this is handled in feed forward networks. For example, in the hidden layer, the net input $\\textbf{z}_h$ is computed through a linear combination determined by summing the multiplications of the weight matrices with the corresponding vectors, and adding the bias unit:\n",
    "\n",
    "$$\n",
    "\\textbf{z}_h^t = \\textbf{W}_{xh}\\textbf{x}^t + \\textbf{W}_{hh}\\textbf{h}^{t-1} + \\textbf{b}_h\n",
    "$$\n",
    "\n",
    "Then the activations of the hidden units at the time step $t$ are calculated using:\n",
    "\n",
    "$$\n",
    "\\textbf{h}^t = \\phi_h\\big(\\textbf{z}_h^t\\big) = \\phi_h\\big(\\textbf{W}_{xh}\\textbf{x}^t + \\textbf{W}_{hh}\\textbf{h}^{t-1} + \\textbf{b}_h\\big)\n",
    "$$\n",
    "\n",
    "where $\\phi_h(\\cdot)$ is the activation function.\n",
    "\n",
    "Once the activations of the hidden units at the current time step are calculated, the activations of the output units are calculated by:\n",
    "$$\n",
    "\\textbf{y}^t = \\phi_y\\big(\\textbf{W}_{hy}\\textbf{h}^t + \\textbf{b}_y\\big)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'The-challenges-of-learning-long-range-interactions'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The challenges of learning long-range interactions\n",
    "\n",
    "Backpropagation through time (BPTT) is the process for optimizing the weights in an RNN. The basic idea is that the overall loss $L$ is the sum of all loss functions calculated at times $t$ = 1 to $t$ = $T$. \n",
    "\n",
    "$$\n",
    "L = \\sum^T_{t=1}L^t\n",
    "$$\n",
    "\n",
    "The loss at time 1:$t$ is dependent on the hidden units at all time steps that were evaluated before 1:$t$, so the gradient is calculated as follows:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L^t}{\\partial\\textbf{W}_{hh}} = \\frac{\\partial L^t}{\\partial\\textbf{y}^{t}} \\times \\frac{\\partial \\textbf{y}^t}{\\partial\\textbf{h}^{t}} \\times \\Bigg(\\sum^t_{k=1}\\frac{\\partial \\textbf{h}^t}{\\partial\\textbf{h}^{k}} \\times \\frac{\\partial \\textbf{h}^k}{\\partial\\textbf{h}_{hh}}\\Bigg)\n",
    "$$\n",
    "\n",
    "In this formula $\\frac{\\partial \\textbf{h}^t}{\\partial\\textbf{h}^{k}}$ is computed as multiplication of adjacent time steps:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\textbf{h}^t}{\\partial\\textbf{h}^{k}} = \\prod^t_{i=k+1}\\frac{\\partial \\textbf{h}^i}{\\partial\\textbf{h}^{i-1}}\n",
    "$$\n",
    "\n",
    "Calculation of the term $\\frac{\\partial \\textbf{h}^t}{\\partial\\textbf{h}^{k}}$ introduces a few challenges. Namely, the so-called vanishing/exploding gradient. This term has $t-k$ multiplications, so multiply the $w$ weight a total of $t - k$ times results in a factor $w^{t-k}$. As a result, if $\\lvert w\\rvert$ < 1, this factor becomes very small when $t-k$ is large. On the other hand, if $\\lvert w\\rvert$ > 1, then $w^{t-k}$ becomes very large when $t-k$ is large. This means that we prefer $w$ to be equal to 1.\n",
    "\n",
    "There are two solutions to this problem:\n",
    "\n",
    "- Truncated backpropagation through time (TBPTT): clips the gradients above a given threshold. This solves exploding gradient issues, but the truncation limits the number of steps the gradient can effectively flow back and update weights properly\n",
    "- Long short-term memory (LSTM): introduced to overcome the vanishing gradient problem. More successful in modeling long-range sequences than TBPTT. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Implementing-a-multilayer-RNN-for-sequence-modeling-in-TensorFlow'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a multilayer RNN for sequence modeling in TensorFlow\n",
    "\n",
    "The rest of this notebook will explore RNN implementations to address two common tasks, sentiment analysis and language models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'performing-sentiment-analysis-of-IMDb-movie-reviews-using-multilayer-RNNs'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project one – performing sentiment analysis of IMDb movie reviews using multilayer RNNs\n",
    "\n",
    "In chapter 8, we implemented a model to determine the sentiment of movie reiews on IMDb. This project will leverage an RNN model to do the same task. This is an example of a many-to-one problem, where we are given a document of text and need to return a single label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Preparing-the-data'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data\n",
    "\n",
    "This dataset contains two columns, one with the movie reviews, and another with the sentiment label of 0 or 1. The text component of these movie reviews are sequences of words, so we want to build an RNN to process the words in sequence and then classify the entire sequence to the 0 or 1 class.\n",
    "\n",
    "To make this dataset ready for the neural network, it needs to be encoded into numeric values. First, we need to find the unique words in the entire dataset. This is not the same as preparing a bag-of-words model, as we are only interested in the set of unique words, and we don't need the counts necessarily. Second we create a mapping by way of a dictionary where we pair each unique word with a unique integer number. This will convert the entire text into a list of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T04:51:22.282888Z",
     "start_time": "2018-12-16T04:51:21.110310Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load ImdbReviews dataset\n",
    "\n",
    "import pyprind\n",
    "from string import punctuation\n",
    "import re\n",
    "\n",
    "df = pd.read_csv(dataPath + '/ImdbReviews.csv', encoding = 'utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T04:51:26.842818Z",
     "start_time": "2018-12-16T04:51:26.801892Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hi for all the people who have seen this wonde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I recently bought the DVD, forgetting just how...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "1  OK... so... I really like Kris Kristofferson a...          0\n",
       "2  ***SPOILER*** Do not read this, if you think a...          0\n",
       "3  hi for all the people who have seen this wonde...          1\n",
       "4  I recently bought the DVD, forgetting just how...          0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "\n",
    "df[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T05:04:04.746261Z",
     "start_time": "2018-12-16T04:59:25.269827Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting words occurrences\n",
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:04:36\n",
      "Map reiew to ints\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', '.', ',', 'and', 'a']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:03\n"
     ]
    }
   ],
   "source": [
    "# Separate the words and count each words occurrence\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "counts = Counter()\n",
    "pbar = pyprind.ProgBar(len(df['review']), title = 'Counting words occurrences')\n",
    "\n",
    "for i, review in enumerate(df['review']):\n",
    "    text = ''.join([c if c not in punctuation else ' ' + c + ' ' for c in review]).lower()\n",
    "    df.loc[i, 'review'] = text\n",
    "    pbar.update()\n",
    "    counts.update(text.split())\n",
    "    \n",
    "# Create a mapping of each unique word to an integer\n",
    "word_counts = sorted(counts, key = counts.get, reverse = True)\n",
    "print(word_counts[:5])\n",
    "word_to_int = {word: ii for ii, word in enumerate(word_counts, 1)}\n",
    "\n",
    "mapped_reviews = []\n",
    "pbar = pyprind.ProgBar(len(df['review']), title = 'Map review to ints')\n",
    "\n",
    "for review in df['review']:\n",
    "    mapped_reviews.append([word_to_int[word] for word in review.split()])\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process effectively conerted sequences of words into sequences of integers, but these sequences have different lengths. For this dataset to be ready for an RNN, the sequences need to hae the same length. To accomplish this, we define a paramter called sequence_length that will be set to 200. Sequences that hae fewer than 200 words will be left-padded with zeros, while sequences longer than 200 words will be trimmed so that only the last 200 values will be used. This preprocessing step is implemented in two steps:\n",
    "\n",
    "1. Create a matrix of zeros, where each row corresponds to a sequence of size 200\n",
    "2. Fill the index of words in each sequence for the right-hand side of the matrix. If a sequence has a length of only 150, then the first 50 elements of the row would stay zero. It's worth noting that the value chosen for sequence_length is a hyperparameter than can be tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T05:13:33.807704Z",
     "start_time": "2018-12-16T05:13:32.842747Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create matrix of same-length sequences\n",
    "\n",
    "sequence_length = 200\n",
    "\n",
    "sequences = np.zeros((len(mapped_reviews), sequence_length), dtype = int)\n",
    "for i, row in enumerate(mapped_reviews):\n",
    "    review_arr = np.array(row)\n",
    "    sequences[i, -len(row):] = review_arr[-sequence_length:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and test sets\n",
    "# Note - the reviews were shuffled prior to being saved in a .csv\n",
    "\n",
    "XTrain = sequences[:25000,:]\n",
    "yTrain = df.loc[:25000, 'sentiment'].values\n",
    "XTest = sequences[25000:, :]\n",
    "yTest = df.loc[25000:, 'sentiment'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T05:21:28.324849Z",
     "start_time": "2018-12-16T05:21:28.318544Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generator helper function for mini-batching\n",
    "\n",
    "def create_batch_generator(x, y = None, batch_size = 64):\n",
    "    n_batches = len(x)//batch_size\n",
    "    x = x[:n_batches * batch_size]\n",
    "    if y is not None:\n",
    "        y = y[:n_batches * batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        if y is not None:\n",
    "            yield x[ii:ii + batch_size], y[ii:ii + batch_size]\n",
    "        else:\n",
    "            yield x[ii: ii + batch_size]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Embedding'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding\n",
    "\n",
    "The data prep stage above created same-length sequences where the elements are integers that correspond to the indices of unique words. Now we need to convert this to input features. The wrong way to do it would be to apply on-ehot encoding to convert indices into ectors of zeros and ones. Each word would be mapped to a vector with a size equal to the number of unique words in the dataset. This is less than ideal, because the number of unique words can rise to the tens of thousands. A model trained on features like this may suffer from the curse of dimensionality. Further, these features would be very sparse, since all values are zero except one.\n",
    "\n",
    "A better approach would be to map each word to a vector of fixed size with real-valued elements (not integers necessarily). With this approach, we can instead use finite-sized vector to represent and infinite number of real number. This is the idea behind the embedding, which is a feature-learning technique that can be utilized to automatically learn the salient features in the data. Given the value of a parameter unique_words, we can choose the size of the embedding vectors to be much smaller than the number of unique words in the corpus. The advantages of emebedding over one-hot encoding for these problems are:\n",
    "\n",
    "1. A reduction in dimensionality decreases the effect of the curse of dimensionality\n",
    "2. The extraction of salient features since the embedding layer in a neural network is trainable\n",
    "\n",
    "To create an embedding layer, we feed in tf_x as the input layer, which is comprised of vocabulary indices. We create a matrix of size $[n\\_words \\times embedding\\_size]$ as a tensor variable with randomly initialized values between [-1,1]. then we use tf.nn.embedding_lookup to loo up the row in the embedding matrix associated with each element of tf_x:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T05:38:32.687300Z",
     "start_time": "2018-12-16T05:38:24.499540Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "embedding = tf.Variable(tf.random_uniform(shape = (n_words, embedding_size), minval = -1, maxval = 1))\n",
    "embed_x = tf.nn.embedding_lookup(embedding, tf_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Building-an-RNN-model'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building an RNN model\n",
    "\n",
    "The SentimentRNN class that we will create has the following methods:\n",
    "\n",
    "- A constructor to set the model parameters and create a computation graph.\n",
    "- A build method that declares three placeholder for input data, input labels, and the kee-probability for the dropout process in the hidden layer. It also creates an embedding layer and creates embedded representations as input.\n",
    "- A train method that creates a session that launches a graph, iterates through mini-batches of data, run for a # of epochs, minimizing the cost along the way before saing the model\n",
    "- A predict method that creates a new session with the model as of the latest checkpoint saved at the end of the training process, and carries out the predictions on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-16T17:38:54.197871Z",
     "start_time": "2018-12-16T17:38:54.186806Z"
    }
   },
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "class SentimentRNN():\n",
    "    def __init__(self, n_words, seq_len = 200, lstm_size = 256, num_layers = 1, batch_size = 64\n",
    "                 , learning_rate = 0.0001, embed_size = 200):\n",
    "        \"\"\"\n",
    "        n_words - must be set equal to the number of unique words (+1, since we use \n",
    "                    zero to fill sequences with a size less than 200) and it's used\n",
    "                    to create the embedding layer, along with embed_size\n",
    "        embed_size - used with n_words to create the embedding layer\n",
    "        seq_len- must be set according to the length of the sequences that were created\n",
    "                    in the preprocessing steps above\n",
    "        lstm_size - a hyperparameter that determines the number of hidden units in each RNN layer\n",
    "        \"\"\"\n",
    "        self.n_words = n_words\n",
    "        self.seq_len = seq_len\n",
    "        self.lstm_size = lstm_size # number of hidden units\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.embed_size = embed_size\n",
    "        \n",
    "        self.g = tf.Graph()\n",
    "        with self.g.as_default():\n",
    "            tf.set_random_seed(123)\n",
    "            self.build()\n",
    "            self.saver = tf.train.Saver()\n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "    \n",
    "    def build(self):\n",
    "        tf_x = tf.placeholder(tf.int32, shape = (self.batch_size, self.seq_len), name = 'tf_x')\n",
    "        tf_y = tf.placeholder(tf.float32, shape = (self.batch_size), name = 'tf_y')\n",
    "        tf_keepprob = tf.placeholder(tf.float32, name = 'tf_keepprob')\n",
    "        \n",
    "        # create embedding layer\n",
    "        embedding = tf.Variable(tf.random_uniform((self.n_words, self.embed_size)\n",
    "                                                  , minval = -1, maxval = 1)\n",
    "                                        , name = 'embedding')\n",
    "        embed_x = tf.nn.embedding_lookup(embedding, tf_x, name = 'embeded_x')\n",
    "        \n",
    "        # define LSTM cell and stack together\n",
    "        cells = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.DropoutWrapper(\n",
    "                                                tf.contrib.rnn.BasicLSTMCell(self.lstm_size)\n",
    "                                                ,output_keep_prob = tf_keepprob)\n",
    "                                            for i in range(self.num_layers)])\n",
    "        \n",
    "        # define the initial state\n",
    "        self.initial_state = cells.zero_state(self.batch_size, tf.float32)\n",
    "        print('  << initial state >>  ', self.initial_state)\n",
    "        \n",
    "        lstm_outputs, self.final_state = tf.nn.dynamic_rnn(cells, embed_x\n",
    "                                                           , initial_state = self.initial_state)\n",
    "        \n",
    "        # lstm output shape = [batch_size x max_time x cells.output_size]\n",
    "        print('\\n  << lstm_output >>', lstm_outputs)\n",
    "        print('\\n  << final state >>', self.final_state)\n",
    "        \n",
    "        logits = tf.layers.dense(inputs = lstm_outputs[:, -1], units = 1, activation = None, name = 'logits')\n",
    "        logits = tf.squeeze(logits, name = 'logits_squeezed')\n",
    "        print('\\n  << logits    >>', logits)\n",
    "        \n",
    "        y_proba = tf.nn.sigmoid(logits, name = 'probabilities')\n",
    "        predictions = {'probabilities' : y_proba\n",
    "                       ,'labels' : tf.cast(tf.round(y_proba), tf.int32, name = 'labels')}\n",
    "        print('\\n  << predictions >>', predictions)\n",
    "        \n",
    "        # define cost function\n",
    "        cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                                labels = tf_y, logits = logits), name = 'cost')\n",
    "        \n",
    "        # define optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        train_op = optimizer.minimize(cost, name = 'train_op')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the build method, we create three placeholder for the input, output, and dropout keep-probability. Then we add the embedding layer, which builds the embedded representation of the unique words. Next within the build method, we built the RNN network. This was done in three steps.\n",
    "\n",
    "1. Define multilayer RNN cells\n",
    "2. Define initial state of these cells\n",
    "3. Create and RNN specified by the RNN cells in their initial states\n",
    "\n",
    "These three steps are unpacked in further detail below:\n",
    "\n",
    "__Step 1: Define multilayer RNN cells__\n",
    "\n",
    "The first step is to define the multilayer RNN cells, which was accomplished using a TensorFlow wrapper ckass to define the LSTM cells - BasicLSTMCell. These can be stacked together to form a multilayer RNN using the MultiRNNCell wrapper class. The process of stacking RNN cells with a dropout stage has three nested steps. Described from the inside out:\n",
    "\n",
    "1. Create RNN cells using tf.contrib.rnn.BasicLSTMCell\n",
    "2. Apply dropout to the RNN cells using tf.contrib.rnn.DropoutWrapper\n",
    "3. Make a list of such cells according to the desired number of RNN layer and pass this list to tf.contrib.rnn.MultiRNNCell\n",
    "\n",
    "This process is completed using a list comprehension in the implementation above.\n",
    "\n",
    "__Step 2: defining the initiatl states for the RNN cells__\n",
    "\n",
    "In the architecture of LSTM cells, there are three types of inputs - input data $\\textbf{x}^t$, activations of hidden units from the previous time step $\\textbf{x}^{t-1}$, and the cell state of the previous time step $\\textbf{C}^{t-1}$.\n",
    "\n",
    "In the above implementation $\\textbf{x}^t$ is the embedded embed_x data tensor. We also need to specify the previous state of the cells. If we're starting a new input sequence, we initialize the cell state to a zero state, then for each seubsequent time step we need to store the updated state of the cells to use in the following time step. The initial state in the implementation above is set by calling cells.zero_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = ''></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = ''></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = ''></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
