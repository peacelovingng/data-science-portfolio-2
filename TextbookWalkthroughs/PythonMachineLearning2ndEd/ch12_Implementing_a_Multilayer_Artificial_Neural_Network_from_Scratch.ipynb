{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Chapter 12 - Implementing a Multilayer Artificial Neural Network from Scratch__\n",
    "\n",
    "1. [Modeling complex functions with artificial neural networks](#Modeling-complex-functions-with-artificial-neural-networks)\n",
    "    1. [Activating a neural network via forward propagation](#Activating-a-neural-network-via-forward-propagation)\n",
    "1. [Classifying handwritten digits](#Classifying-handwritten-digits)\n",
    "1. [Implementing a multilayer perceptron](#Implementing-a-multilayer-perceptron)\n",
    "    1. [Homegrown implementation](#Homegrown-implementation)\n",
    "1. [](#)\n",
    "1. [](#)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libary and settings\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "import itertools\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "dataPath = os.path.abspath(os.path.join('../../Data'))\n",
    "modulePath = os.path.abspath(os.path.join('../../CustomModules'))\n",
    "sys.path.append(modulePath) if modulePath not in sys.path else None\n",
    "from IPython.core.display import display, HTML; display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "\n",
    "# Data extensions and settings\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold = np.inf, suppress = True)\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.options.display.float_format = '{:,.6f}'.format\n",
    "\n",
    "\n",
    "# Modeling extensions\n",
    "import sklearn.base as base\n",
    "import sklearn.cluster as cluster\n",
    "import sklearn.datasets as datasets\n",
    "import sklearn.decomposition as decomposition\n",
    "import sklearn.discriminant_analysis as discriminant_analysis\n",
    "import sklearn.ensemble as ensemble\n",
    "import sklearn.feature_extraction as feature_extraction\n",
    "import sklearn.feature_selection as feature_selection\n",
    "import sklearn.linear_model as linear_model\n",
    "import sklearn.metrics as metrics\n",
    "import sklearn.model_selection as model_selection\n",
    "import sklearn.neighbors as neighbors\n",
    "import sklearn.pipeline as pipeline\n",
    "import sklearn.preprocessing as preprocessing\n",
    "import sklearn.svm as svm\n",
    "import sklearn.tree as tree\n",
    "import sklearn.utils as utils\n",
    "\n",
    "\n",
    "# Visualization extensions and settings\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Custom extensions and settings\n",
    "from quickplot import qp, qpUtil, qpStyle\n",
    "from mlTools import powerGridSearch\n",
    "sns.set(rc = qpStyle.rcGrey)\n",
    "\n",
    "\n",
    "# Magic functions\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Modeling-complex-functions-with-artificial-neural-networks'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling complex functions with artificial neural networks\n",
    "\n",
    "A fully connected network, also known as a multilayer perceptron (MLP), has one input layer of neurons, one hidden layer and one output layer. The units in the hidden layer are fully connected to the input layer, and the output layer is fully connected to the hidden layer. If more than one hidden layer is present then the MLP is considered to be a deep artificial neural network.\n",
    "\n",
    "Each neuron, or activation unit, can be identified by its position amongst the other activation neurons and the layer in which it appear - $a_i^l$ is the $i$th neuron in the $l$th layer. For simplicity, this walkthrough will use the $l$ values of $in, h, out$ to describe the input, hidden and output layer. So $a_i^{out}$ is the $i$th activation unit of the outer layer. The input and hidden layers each have bias units, $a_0^{in}$ and $a_0^{out}$ and these are set to one. This means the input layer is just the input values plus the bias unit:\n",
    "\n",
    "$$\n",
    "a^{in} \n",
    "= \n",
    "\\begin{bmatrix} a_0^{in} \\\\ a_1^{in} \\\\ \\vdots \\\\  a_m^{in} \\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix} 1 \\\\ x_1^{in} \\\\ \\vdots \\\\  x_m^{in} \\end{bmatrix}\n",
    "$$ \n",
    "\n",
    "Each activation unit in layer $l$ is connected to all of the units in layer $l$ + 1 by a weight coefficient. As an example, the connection between the $k$th unit layer $l$ to the $j$th in layer $l$ + 1 is written as $w_{k,j}^l$. So the weight matrix that connects the input layer to the hidden layer is $\\mathbf{W}^{h}$, the weight matrix that connects the hidden layer to the output layer is $\\mathbf{W}^{out}$. The weight matrix that connects, for example, the input and hidden layers is $\\mathbf{W}^h \\in \\mathbb{R}^{m \\times d}$, where $d$ is the number of hidden units and $m$ is the numnber of input units (including bias).\n",
    "\n",
    "Having one unit in the output layer is sufficient for a binary classificaiton task, but having more than one enables multiclass classification through one-hot vector representation of the multiclass labels.:\n",
    "$$\n",
    "0 \n",
    "= \n",
    "\\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}\n",
    "1 \n",
    "= \n",
    "\\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}\n",
    "2 \n",
    "= \n",
    "\\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Activating-a-neural-network-via-forward-propagation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activating a neural network via forward propagation\n",
    "\n",
    "The MLP learning procedure in three steps:\n",
    "\n",
    "1. Starting at the input layer, forward propagate the patterns of the training data through the network to generate an output\n",
    "2. Using the output, calculate the error to be minimized using a cost function\n",
    "3. Backpropagate the error, find its derivative with respect to each weight in the network, then update the model\n",
    "\n",
    "In feedforward networks, each layer serves as the input to the next layer without any loops. This contrasts with recurrent neural networks. The three steps above are repeated for multiple epochs to learn the best weights, and then forward propagation is used to the calculate the network output and apply a threshold function to obtain the predicted class labels represented in the one-hot format above. Describing each step in more detail:\n",
    "\n",
    "The first activation unit in the hidden layer $a_1^{h}$ is connected to all units in the input layer, and is calculated by:\n",
    "\n",
    "$$\n",
    "z_1^h = a_0^{in}w_{0,1}^h + a_1^{in}w_{1,1}^h + ... a_m^{in}w_{m,1}^h\n",
    "$$\n",
    "$$\n",
    "a_1^h = \\phi\\big(z_1^h\\big)\n",
    "$$\n",
    "$z_1^h$ is the net input and $\\phi(\\cdot)$ is the activation function that acts on $z_1^h$. This activation function need to be differentiable to learn the weights that connect the neurons using a gradient-based approach. Non-linear activation function are also possible and are used to solve complex problems like image classification. One familiar non-linear activation function is the sigmoid function, which arose in the context of logistic regression:\n",
    "\n",
    "$$\n",
    "\\phi(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "This is an S-shaped curve that maps the input $z$ onto a logistic distribution that ranges from 0 to 1 and cross the y-axis at $z$ = 0. Given this, we can think of each neuron as logistic regression units that return values in the continuous range of 0 to 1. To describe this activation function in linear algebra notation:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\textbf{z}^{h} = \\textbf{a}^{in}\\textbf{W}^h\n",
    "\\\\\n",
    "\\textbf{a}^h = \\phi\\big(\\textbf{z}^h\\big)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "$\\textbf{a}^{in}$ is the 1 x $m$ dimensional feature vector for a sample $\\textbf{x}^{in}$, plus the bias unit. $\\textbf{W}^{h}$ is the $m$ x $d$ dimensional weight matrix where $d$ is the number of units in the hidden layer. Through matrix-vector multiplication, we obtain a 1 x $d$ dimensional net input vector $\\textbf{z}^h$ to be used to calculate the activation $\\textbf{a}^{h}$ ($\\textbf{a}^{h} \\in \\mathbb{R}^{1 \\times d}$). This computation can be generalized to all $n$ samples in the training set by:\n",
    "\n",
    "$$\n",
    "\\textbf{Z}^{h} = \\textbf{A}^{in}\\textbf{W}^{h}\n",
    "$$\n",
    "\n",
    "In this representation, $\\textbf{A}^{in}$ is an $n$ x $m$ matrix, and the matrix-matrix multiplication results in an $n$ x $d$ dimensional net input matrix $\\textbf{Z}^{h}$. Lastly, apply the activation function $\\phi(\\cdot)$ to each value in the net input matrix to get the $n$ x $d$ dimensional matrix $\\textbf{A}^{h}$ for the next layer, which in this case is the output layer:\n",
    "\n",
    "$$\n",
    "A^h = \\phi\\big(\\textbf{Z}^{h}\\big)\n",
    "$$\n",
    "\n",
    "Just as above, we can write the activation function of the output layer in vectorized form for multiple samples:\n",
    "$$\n",
    "\\textbf{Z}^{out} = \\textbf{A}^{h}\\textbf{W}^{out}\n",
    "$$\n",
    "\n",
    "In this last step, we multiply the $d$ x $t4 matrix $\\textbf{W}^{out}$ (where $t$ is the number of output units) by the $n$ by $d$ dimensional matrix \\textbf{A}^{h} to obtain the $n$ by $t$ dimensional matrix \\textbf{Z}^{out}, where the columns in this matrix represent the outputs for each sample. The last step is to apply the sgmoid activation function to obtain the continuous valued ouput of the network:\n",
    "\n",
    "$$\n",
    "\\textbf{A}^{out} = \\phi\\big(\\textbf{Z}^{out}\\big), \\textbf{A}^{out} \\in \\mathbb{R}^{n \\times t}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Classifying-handwritten-digits'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying handwritten digits\n",
    "\n",
    "Implement and train our first MLP to classify handwritten digits from the Mixed National Institute of Standards and Techngology (MNIST). It consists of handwritten digits from 250 people - half high school students and half Census Bureau employees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read image files into numpy arrays\n",
    "\n",
    "import struct\n",
    "\n",
    "def load_mnist(path, kind = 'train'):\n",
    "    \"\"\"\n",
    "    This returns two arrays. images is an n x m dimensional array, where n is the\n",
    "    number of samples and m is the number of features, which is this context is the\n",
    "    number of pixels. The training data includes 60,000 digits and the test data includes\n",
    "    10,000 digits. The second array labels includes the target variable, which takes on\n",
    "    an integer value between 0 and 9.    \n",
    "    \n",
    "    The images in this dataset are 28 x 28 pixels in size, and each pixel is represented\n",
    "    by a gray scale intensity value. This function unrolls the 28 x 28 pixels into a\n",
    "    one-dimensional row vector of length 784, which is represented in the images array.\n",
    "    \"\"\"    \n",
    "    labels_path = os.path.join(path, '%s-labels-idx1-ubyte' % kind)\n",
    "    images_path = os.path.join(path, '%s-images-idx1-ubyte' % kind)\n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II', lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath, dtype = np.uint8)\n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack('>IIII', imgpath.read(16))\n",
    "        images = np.fromfile(imgpath, dtype = np.uint8).reshape(len(labels, 784))\n",
    "        \n",
    "        # Normalize pixel values to range from -1 to 1 rather than 0 to 255\n",
    "        images = ((images / 255.) - 0.5) * 2\n",
    "    return images, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "XTrain, yTrain = load_mnist('', kind = 'train')\n",
    "print('Rows: {}, columns: {}'.format(XTrain.shape[0], XTrain.shape[0]))\n",
    "\n",
    "XTest, yTest = load_mnist('', kind = 't10k')\n",
    "print('Rows: {}, columns: {}'.format(XTest.shape[0], XTest.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize samples digits\n",
    "\n",
    "fig, ax = plt.subplots(nrows = 2, ncols = 5, sharex = True, sharey = True)\n",
    "\n",
    "ax = ax.flatten()\n",
    "for i in range(10):\n",
    "    img = XTrain[yTrain == i][0].reshape(28,28)\n",
    "    ax[i].imshow(img, cmap = 'Greys')\n",
    "    \n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize multiple samples of the same digit\n",
    "\n",
    "fig, ax = plt.subplots(nrows = 5, ncols = 5, sharex = True, sharey = True)\n",
    "\n",
    "ax = ax.flatten()\n",
    "for i in range(25):\n",
    "    img = XTrain[yTrain == 7][i].reshape(28,28)\n",
    "    ax[i].imshow(img, cmap = 'Greys')\n",
    "    \n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficiently save image files to avoid reloading and to save space\n",
    "\n",
    "np.savez_compressed('mnist_Scaled.npz'\n",
    "                   ,XTrain = XTrain\n",
    "                   ,yTrain = yTrain\n",
    "                   ,XTest = XTest\n",
    "                   ,yTest = yTest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed images from compressed files\n",
    "\n",
    "mnist = np.load('mnist_scaled.npz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review files\n",
    "\n",
    "mnist.files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load retrieved compressed files into arrays\n",
    "\n",
    "XTrain, yTrain, XTest, yTest = [mnist[f] for f in mnist.files]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Implementing-a-multilayer-perceptron'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a multilayer perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Homegrown-implementation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homegrown implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "class NeuralNetMLP():\n",
    "    \"\"\"\n",
    "    Feedforward neural network / Multilayer perceptron classifier\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = ''></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = ''></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = ''></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = ''></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = ''></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = ''></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = ''></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = ''></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = ''></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = ''></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
