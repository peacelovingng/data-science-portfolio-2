{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Kaggle competition - house prices__\n",
    "\n",
    "1. [Kaggle competition - house prices](#Kaggle-competition-house-prices)\n",
    "1. [Import](#Import)\n",
    "    1. [Tools](#Tools)\n",
    "    1. [Data](#Data)    \n",
    "1. [Initial EDA](#Initial-EDA)\n",
    "    1. [Categorical feature EDA](#Categorical-feature-EDA)\n",
    "        1. [Univariate & feature vs. target](#Univariate-&-feature-vs.-target)\n",
    "    1. [Continuous feature EDA](#Continuous-feature-EDA)\n",
    "        1. [Univariate & feature vs. target](#Univariate-&-feature-vs.-target2)\n",
    "        1. [Correlation](#Correlation)\n",
    "            1. [Correlation (all samples)](#Correlation-all-samples)\n",
    "            1. [Correlation (top vs. target)](#Correlation-top-vs-target)\n",
    "        1. [Pair plot](#Pair-plot)\n",
    "    1. [Faceting](#Faceting)\n",
    "        1. [Categorical by categorical](#Categorical-by-categorical)\n",
    "        1. [Categorical by numerical](#Categorical-by-numerical)\n",
    "        1. [Numerical by numerical](#Numerical-by-numerical)        \n",
    "    1. [Target variable evaluation](#Target-variable-evaluation)    \n",
    "1. [Data cleaning](#Data-cleaning)\n",
    "    1. [Outliers (preliminary)](#Outliers-preliminary)\n",
    "        1. [Training](#Training5)\n",
    "        1. [Validation](#Validation5)\n",
    "    1. [Missing data](#Missing-data)\n",
    "        1. [Evaluate](#Evaluate1)\n",
    "        1. [Training](#Training1)\n",
    "        1. [Validation](#Validation1)\n",
    "    1. [Engineering](#Engineering)\n",
    "        1. [Evaluate](#Evaluate3)\n",
    "        1. [Training](#Training3)\n",
    "        1. [Validation](#Validation3)\n",
    "    1. [Encoding](#Encoding)\n",
    "        1. [Evaluate](#Evaluate2)\n",
    "        1. [Training](#Training2)\n",
    "        1. [Validation](#Validation2)\n",
    "    1. [Transformation](#Transformation)\n",
    "        1. [Evaluate](#Evaluate4)\n",
    "        1. [Training](#Training4)\n",
    "        1. [Validation](#Validation4)\n",
    "    1. [Outliers (final)](#Outliers-final)\n",
    "        1. [Training](#Training6)\n",
    "1. [Data evaluation](#Data-evaluation)\n",
    "    1. [Feature importance](#Feature-importance)    \n",
    "    1. [Rationality](#Rationality)\n",
    "    1. [Value override](#Value-override)\n",
    "    1. [Continuous feature EDA](#Continuous-feature-EDA3)\n",
    "        1. [Univariate & feature vs. target](#Univariate-&-feature-vs.-target3)\n",
    "        1. [Correlation](#Correlation3)\n",
    "            1. [Correlation (top vs. target)](#Correlation-top-vs-target3)\n",
    "1. [Modeling](#Modeling)\n",
    "    1. [Prepare training data](#Prepare-training-data)\n",
    "    1. [Prepare validation data](#Prepare-validation-data)\n",
    "    1. [GridSearch](#GridSearch)\n",
    "        1. [Evaluation](#Evaluation)\n",
    "        1. [Model explanability](#Model-explanability)\n",
    "            1. [Permutation importance](#Permutation-importance)\n",
    "            1. [Partial plots](#Partial-plots)\n",
    "            1. [SHAP values](#SHAP-values)\n",
    "    1. [Stacking](#Stacking)\n",
    "        1. [Primary models](#Primary-models)\n",
    "        1. [Meta model](#Meta-model)                \n",
    "1. [Submission](#Submission)\n",
    "    1. [Stack](#Stack)\n",
    "    1. [Standard](#Standard)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle competition - Titanic\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Kaggle-competition-house-prices'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Import'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Tools'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:58:09.135366Z",
     "start_time": "2019-07-13T18:58:08.020376Z"
    }
   },
   "outputs": [],
   "source": [
    "# standard libary and settings\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "import itertools\n",
    "import csv\n",
    "import ast\n",
    "from timeit import default_timer as timer\n",
    "global ITERATION\n",
    "import time\n",
    "from functools import reduce\n",
    "rundate = time.strftime('%Y%m%d')\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "from IPython.core.display import display, HTML; display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "# data extensions and settings\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold = np.inf, suppress = True)\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.options.display.float_format = '{:,.6f}'.format\n",
    "\n",
    "# modeling extensions\n",
    "import sklearn.base as base\n",
    "import sklearn.cluster as cluster\n",
    "import sklearn.datasets as datasets\n",
    "import sklearn.decomposition as decomposition\n",
    "import sklearn.discriminant_analysis as discriminant_analysis\n",
    "import sklearn.ensemble as ensemble\n",
    "import sklearn.feature_extraction as feature_extraction\n",
    "import sklearn.feature_selection as feature_selection\n",
    "import sklearn.gaussian_process as gaussian_process\n",
    "import sklearn.linear_model as linear_model\n",
    "import sklearn.kernel_ridge as kernel_ridge\n",
    "import sklearn.metrics as metrics\n",
    "import sklearn.model_selection as model_selection\n",
    "import sklearn.naive_bayes as naive_bayes\n",
    "import sklearn.neighbors as neighbors\n",
    "import sklearn.pipeline as pipeline\n",
    "import sklearn.preprocessing as preprocessing\n",
    "import sklearn.svm as svm\n",
    "import sklearn.tree as tree\n",
    "import sklearn.utils as utils\n",
    "\n",
    "import eif as iso\n",
    "\n",
    "from scipy import stats, special\n",
    "import xgboost\n",
    "import lightgbm\n",
    "import catboost\n",
    "\n",
    "from hyperopt import hp, tpe, Trials, fmin, STATUS_OK\n",
    "from hyperopt.pyll.stochastic import sample\n",
    "\n",
    "# visualization extensions and settings\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# custom extensions and settings\n",
    "sys.path.append('/home/mlmachine') if '/home/mlmachine' not in sys.path else None\n",
    "sys.path.append('/home/prettierplot') if '/home/prettierplot' not in sys.path else None\n",
    "\n",
    "import mlmachine as mlm\n",
    "from prettierplot.plotter import PrettierPlot\n",
    "import prettierplot.style as style\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:58:09.155168Z",
     "start_time": "2019-07-13T18:58:09.137486Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load data and print dimensions\n",
    "dfTrain = pd.read_csv('/home/data-science-portfolio/data/kaggleTitanic/train.csv')\n",
    "dfValid = pd.read_csv('/home/data-science-portfolio/data/kaggleTitanic/test.csv')\n",
    "\n",
    "print('Training data dimensions: {}'.format(dfTrain.shape))\n",
    "print('Validation data dimensions: {}'.format(dfValid.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:58:09.185144Z",
     "start_time": "2019-07-13T18:58:09.158326Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# display info and first 5 rows\n",
    "dfTrain.info()\n",
    "display(dfTrain[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:58:09.193758Z",
     "start_time": "2019-07-13T18:58:09.187478Z"
    }
   },
   "outputs": [],
   "source": [
    "# review counts of different column types\n",
    "dfTrain.dtypes.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:58:09.206278Z",
     "start_time": "2019-07-13T18:58:09.195826Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load training data into mlmachine\n",
    "train = mlm.Machine(data = dfTrain\n",
    "                   ,target = ['Survived']\n",
    "                   ,removeFeatures = ['PassengerId','Ticket']                      \n",
    "                   ,overrideCat = ['Pclass','SibSp','Parch']\n",
    "                   ,targetType = 'categorical'\n",
    "    )\n",
    "print(train.data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:58:09.216556Z",
     "start_time": "2019-07-13T18:58:09.209089Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load training data into mlmachine\n",
    "valid = mlm.Machine(data = dfValid\n",
    "                   ,removeFeatures = ['PassengerId','Ticket']                      \n",
    "                   ,overrideCat = ['Pclass','SibSp','Parch']\n",
    "    )\n",
    "print(valid.data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Initial-EDA'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical feature EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Categorical-feature-EDA'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate & feature vs. target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Univariate-&-feature-vs.-target'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:58:11.072339Z",
     "start_time": "2019-07-13T18:58:09.218704Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# categorical features\n",
    "train.edaCatTargetCatFeat(skipCols = ['Name','Cabin'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous feature EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Continuous-feature-EDA'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate & feature vs. target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Univariate-&-feature-vs.-target2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:58:12.918475Z",
     "start_time": "2019-07-13T18:58:11.077062Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# continuous features\n",
    "train.edaCatTargetNumFeat()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Correlation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation (all samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Correlation-all-samples'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:58:13.293203Z",
     "start_time": "2019-07-13T18:58:12.925730Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# correlation heat map \n",
    "p = PrettierPlot()\n",
    "ax = p.makeCanvas()\n",
    "p.prettyCorrHeatmap(df = train.data\n",
    "                   ,annot = True\n",
    "                   ,ax = ax\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation (top vs. target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Correlation-top-vs-target'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:58:13.570291Z",
     "start_time": "2019-07-13T18:58:13.294881Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# correlation heat map with most highly correlated features relative to the target\n",
    "p = PrettierPlot()\n",
    "ax = p.makeCanvas()\n",
    "p.prettyCorrHeatmapTarget(df = train.data\n",
    "                         ,target = train.target\n",
    "                         ,thresh = 0.2\n",
    "                         ,annot = True \n",
    "                         ,ax = ax\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Remarks - There are three pairs of highly correlated features:\n",
    "    - 'GarageArea' and 'GarageCars'\n",
    "    - 'TotRmsAbvGrd' and 'GrLivArea'\n",
    "    - '1stFlrSF' and 'TotalBsmtSF\n",
    "This makes sense, given what each feature represents and how each pair items relate to each other. We likely only need one feature from each pair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pair plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Pair-plot'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:58:20.029141Z",
     "start_time": "2019-07-13T18:58:13.575953Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pair plot\n",
    "p = PrettierPlot(chartProp = 12)\n",
    "p.prettyPairPlot(df = train.data\n",
    "                ,diag_kind = 'auto'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:58:28.951688Z",
     "start_time": "2019-07-13T18:58:20.030843Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pair plot\n",
    "p = PrettierPlot(chartProp = 12)\n",
    "p.prettyPairPlot(df = train.data.dropna()\n",
    "                ,diag_kind = 'kde'\n",
    "                ,target = train.target\n",
    "                ,cols = ['Age','Fare','Pclass','Parch','SibSp']\n",
    "                ,legendLabels = ['Died','Survived'] \n",
    "                ,bbox = (2.0, 0.0) \n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faceting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Faceting'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical by categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Categorical-by-categorical'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:58:29.367694Z",
     "start_time": "2019-07-13T18:58:28.955168Z"
    }
   },
   "outputs": [],
   "source": [
    "# facet Pclass vs Embarked\n",
    "p = PrettierPlot(chartProp = 12)\n",
    "ax = p.makeCanvas(title = 'Survivorship, embark location by passenger class', yShift = 0.7)\n",
    "p.prettyFacetTwoCatBar(df = train.edaData(train.data, train.target)\n",
    "                      ,x = 'Embarked'\n",
    "                      ,y = train.target.name\n",
    "                      ,split = 'Pclass'\n",
    "                      ,yUnits = 'ff'\n",
    "                      ,ax = ax\n",
    "    )\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:58:29.774500Z",
     "start_time": "2019-07-13T18:58:29.370690Z"
    }
   },
   "outputs": [],
   "source": [
    "# facet Pclass vs Embarked\n",
    "p = PrettierPlot(chartProp = 12)\n",
    "ax = p.makeCanvas(title = 'Survivorship, passenger class by gender', yShift = 0.7)\n",
    "p.prettyFacetTwoCatBar(df = train.edaData(train.data, train.target)\n",
    "                      ,x = 'Pclass'\n",
    "                      ,y = train.target.name\n",
    "                      ,split = 'Sex'\n",
    "                      ,yUnits = 'ff'\n",
    "                      ,ax = ax\n",
    "    )\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:58:30.123819Z",
     "start_time": "2019-07-13T18:58:29.779228Z"
    }
   },
   "outputs": [],
   "source": [
    "# facet Pclass vs Embarked\n",
    "p = PrettierPlot(chartProp = 12)\n",
    "ax = p.makeCanvas(title = 'Survivorship,embark location by gender', yShift = 0.7)\n",
    "p.prettyFacetTwoCatBar(df = train.edaData(train.data, train.target)\n",
    "                      ,x = 'Embarked'\n",
    "                      ,y = train.target.name\n",
    "                      ,split = 'Sex'\n",
    "                      ,yUnits = 'ff'\n",
    "                      ,ax = ax\n",
    "    )\n",
    "                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical by numerical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Categorical-by-numerical'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:58:31.847734Z",
     "start_time": "2019-07-13T18:58:30.127478Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "p = PrettierPlot()\n",
    "p.prettyFacetCatNumHist(df = train.edaData(train.data, train.target)\n",
    "                       ,split = train.target.name\n",
    "                       ,legendLabels = ['Died','Lived']\n",
    "                       ,catRow = 'Sex'\n",
    "                       ,catCol = 'Embarked'\n",
    "                       ,numCol = 'Age'\n",
    "                       ,bbox = (1.9, 1.0)\n",
    "                       ,height = 4\n",
    "                       ,aspect = 1\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:58:33.454314Z",
     "start_time": "2019-07-13T18:58:31.849591Z"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "p = PrettierPlot(chartProp = 15)\n",
    "p.prettyFacetCatNumScatter(df = train.edaData(train.data, train.target)\n",
    "                       ,split = train.target.name\n",
    "                       ,legendLabels = ['Died','Lived']\n",
    "                       ,catRow = 'Sex'\n",
    "                       ,catCol = 'Embarked'\n",
    "                       ,xNum = 'Fare'\n",
    "                       ,yNum = 'Age'\n",
    "                       ,bbox = (1.9, 1.0)\n",
    "                       ,height = 4\n",
    "                       ,aspect = 1\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical by numerical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Numerical-by-numerical'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:58:34.110541Z",
     "start_time": "2019-07-13T18:58:33.456536Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "p = PrettierPlot()\n",
    "p.prettyFacetTwoCatPoint(df = train.edaData(train.data, train.target)\n",
    "                        ,x = 'Sex'\n",
    "                        ,y = train.target.name \n",
    "                        ,split = 'Pclass' \n",
    "                        ,catRow = 'Embarked'\n",
    "                        ,aspect = 1.0\n",
    "                        ,height = 5\n",
    "                        ,bbox = (1.3, 1.2)\n",
    "                        ,legendLabels = ['1st class','2nd class', '3rd class']\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:58:34.595490Z",
     "start_time": "2019-07-13T18:58:34.112368Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "p = PrettierPlot()\n",
    "p.prettyFacetTwoCatPoint(df = train.edaData(train.data, train.target).dropna(subset = ['Embarked'])\n",
    "                        ,x = 'Embarked'\n",
    "                        ,y = train.target.name \n",
    "                        ,split = 'Pclass' \n",
    "                        ,catRow = 'Sex'\n",
    "                        ,aspect = 1.0\n",
    "                        ,height = 5\n",
    "                        ,bbox = (1.5, 0.8)\n",
    "                        ,legendLabels = ['1st class','2nd class', '3rd class']\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target variable evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Target-variable-evaluation'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:58:34.607747Z",
     "start_time": "2019-07-13T18:58:34.598968Z"
    }
   },
   "outputs": [],
   "source": [
    "# null score\n",
    "pd.Series(train.target).value_counts(normalize = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Data-cleaning'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers (preliminary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Outliers-preliminary'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Training5'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:58:34.627437Z",
     "start_time": "2019-07-13T18:58:34.610070Z"
    }
   },
   "outputs": [],
   "source": [
    "# identify columns that have zero missing values\n",
    "nonNull = train.data.columns[train.data.isnull().sum() == 0].values.tolist()\n",
    "\n",
    "# identify intersection between non-null columns and continuous columns\n",
    "nonNullNumCol = list(set(nonNull).intersection(train.featureByDtype_['continuous']))\n",
    "print(nonNull)\n",
    "print(nonNullNumCol)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:58:34.666248Z",
     "start_time": "2019-07-13T18:58:34.633738Z"
    }
   },
   "outputs": [],
   "source": [
    "# identify outliers using IQR\n",
    "trainPipe = pipeline.Pipeline([\n",
    "        ('outlier', train.OutlierIQR(outlierCount = 2, iqrStep = 1.5, features = ['Age','SibSp','Parch','Fare'], dropOutliers = False))     \n",
    "    ])\n",
    "train.data = trainPipe.transform(train.data)\n",
    "\n",
    "# capture outliers\n",
    "iqrOutliers = np.array(sorted(trainPipe.named_steps['outlier'].outliers_))\n",
    "print(iqrOutliers)\n",
    "\n",
    "# remove outliers\n",
    "# train.target = np.delete(train.target, trainPipe.named_steps['outlier'].outliers_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:58:34.976035Z",
     "start_time": "2019-07-13T18:58:34.674970Z"
    }
   },
   "outputs": [],
   "source": [
    "# identify outliers using Isolation Forest\n",
    "clf = ensemble.IsolationForest(behaviour = 'new'\n",
    "                              ,max_samples = train.data.shape[0]\n",
    "                              ,random_state = 0\n",
    "                              ,contamination = 0.02\n",
    "    )\n",
    "clf.fit(train.data[['SibSp','Parch','Fare']])\n",
    "preds = clf.predict(train.data[['SibSp','Parch','Fare']])\n",
    "# np.unique(preds, return_counts = True)\n",
    "\n",
    "# evaluate index values\n",
    "mask = np.isin(preds, -1)  # np.in1d if np.isin is not available\n",
    "ifOutliers = np.where(mask)\n",
    "print(ifOutliers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:58:38.811672Z",
     "start_time": "2019-07-13T18:58:34.979299Z"
    }
   },
   "outputs": [],
   "source": [
    "# identify outliers using Extended Isolation Forest\n",
    "if_eif = iso.iForest(train.data[['SibSp','Parch','Fare']].values\n",
    "                    ,ntrees = 100\n",
    "                    ,sample_size = 256\n",
    "                    ,ExtensionLevel = 1\n",
    "    )\n",
    "\n",
    "# calculate anomaly scores\n",
    "anomalies_ratio = 0.02\n",
    "anomaly_scores = if_eif.compute_paths(X_in = train.data[['SibSp','Parch','Fare']].values)\n",
    "anomaly_scores_sorted = np.argsort(anomaly_scores)\n",
    "eifOutliers = np.array(anomaly_scores_sorted[-int(np.ceil(anomalies_ratio * train.data.shape[0])):])\n",
    "print(sorted(eifOutliers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:58:38.817598Z",
     "start_time": "2019-07-13T18:58:38.813943Z"
    }
   },
   "outputs": [],
   "source": [
    "# identify outliers that are identified in multiple algorithms\n",
    "# reduce(np.intersect1d, (iqrOutliers, ifOutliers, eifOutliers))\n",
    "outliers = reduce(np.intersect1d, (ifOutliers, eifOutliers))\n",
    "print(outliers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:58:38.838065Z",
     "start_time": "2019-07-13T18:58:38.819852Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# remove outlers from predictors and response\n",
    "outliers = np.array([27,  88, 258, 311, 341, 438, 679, 737, 742])\n",
    "train.data = train.data.drop(train.data.index[outliers])\n",
    "train.target = train.target.drop(index = outliers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Validation5'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Missing-data'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Evaluate1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:58:39.086652Z",
     "start_time": "2019-07-13T18:58:38.840060Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate missing data\n",
    "train.edaMissingSummary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:58:39.343466Z",
     "start_time": "2019-07-13T18:58:39.089045Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate missing data\n",
    "valid.edaMissingSummary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:58:39.348906Z",
     "start_time": "2019-07-13T18:58:39.346018Z"
    }
   },
   "outputs": [],
   "source": [
    "# missingdata_df = merged_df.columns[merged_df.isnull().any()].tolist()\n",
    "# msno.matrix(merged_df[missingdata_df])\n",
    "\n",
    "# msno.bar(merged_df[missingdata_df], color=\"blue\", log=True, figsize=(30,18))\n",
    "\n",
    "# # \n",
    "# msno.heatmap(merged_df[missingdata_df], figsize=(20,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:58:39.370148Z",
     "start_time": "2019-07-13T18:58:39.351273Z"
    }
   },
   "outputs": [],
   "source": [
    "# compare feature with missing data\n",
    "train.missingColCompare(train.data, valid.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Training1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:58:39.657909Z",
     "start_time": "2019-07-13T18:58:39.372260Z"
    }
   },
   "outputs": [],
   "source": [
    "# apply imputations to missing data in training dataset\n",
    "trainPipe = pipeline.Pipeline([\n",
    "        ('imputeMedian', train.ContextImputer(nullCol = 'Age', contextCol = 'Parch', strategy = 'median'))     \n",
    "        ,('imputeMode', train.ModeImputer(cols = ['Embarked']))\n",
    "    ])\n",
    "train.data = trainPipe.transform(train.data)\n",
    "train.edaMissingSummary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Validation1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:58:39.906033Z",
     "start_time": "2019-07-13T18:58:39.660537Z"
    }
   },
   "outputs": [],
   "source": [
    "# apply imputations to missing data in validation dataset\n",
    "validPipe = pipeline.Pipeline([\n",
    "        ('imputeMedian', valid.ContextImputer(nullCol = 'Age', contextCol = 'Parch', train = False, trainDf = trainPipe.named_steps['imputeMedian'].fillDf))\n",
    "        ,('imputeMedian2', valid.NumericalImputer(cols = ['Fare','Age'], strategy = 'median'))    \n",
    "    ])\n",
    "valid.data = validPipe.transform(valid.data)\n",
    "valid.edaMissingSummary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Engineering'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Evaluate3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Training3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:58:40.208186Z",
     "start_time": "2019-07-13T18:58:39.909300Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# parse titles to learn gender, and identify rare titles which may convey status\n",
    "title = [i.split(',')[1].split('.')[0].strip() for i in train.data['Name']]\n",
    "train.data['Title'] = pd.Series(title)\n",
    "train.data['Title'] = train.data['Title'].replace(['Lady','the Countess','Countess','Capt','Col','Don','Dr'\n",
    "                                              ,'Major','Rev','Sir','Jonkheer','Dona'], 'Rare')\n",
    "train.data['Title'] = train.data['Title'].map({'Master' : 0, 'Miss' : 1, 'Ms' : 1 , 'Mme' : 1, 'Mlle' : 1, 'Mrs' : 1, 'Mr' : 2, 'Rare' : 3})\n",
    "\n",
    "# distill cabin feature\n",
    "train.data['CabinQuarter'] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in train.data['Cabin']])\n",
    "\n",
    "# family size features and binning\n",
    "train.data['FamilySize'] = train.data['SibSp'] + train.data['Parch'] + 1\n",
    "\n",
    "customBinDict = {'Age' : [16, 32, 48, 64]\n",
    "                 ,'FamilySize' : [1, 2, 4]\n",
    "          }\n",
    "\n",
    "trainPipe = pipeline.Pipeline([\n",
    "        ('customBin', train.CustomBinner(customBinDict = customBinDict))\n",
    "        ,('percentileBin', train.PercentileBinner(cols = ['Age','Fare'], percs = [25, 50, 75]))    \n",
    "    ])\n",
    "train.data = trainPipe.transform(train.data)\n",
    "\n",
    "# drop features\n",
    "train.featureDropper(cols = ['Name','Cabin'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:58:40.229813Z",
     "start_time": "2019-07-13T18:58:40.210409Z"
    }
   },
   "outputs": [],
   "source": [
    "# print new columns\n",
    "for col in train.data.columns:\n",
    "    if col not in train.featureByDtype_['categorical'] and col not in train.featureByDtype_['continuous']:\n",
    "        print(col)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:58:40.250030Z",
     "start_time": "2019-07-13T18:58:40.235755Z"
    }
   },
   "outputs": [],
   "source": [
    "# append new continuous features\n",
    "for col in ['FamilySize']:\n",
    "    train.featureByDtype_['continuous'].append(col)\n",
    "\n",
    "# append new categorical features\n",
    "for col in ['AgeCustomBin','AgePercBin','FarePercBin','FamilySize','FamilySizeCustomBin','Title','CabinQuarter']:\n",
    "    train.featureByDtype_['categorical'].append(col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:58:46.295229Z",
     "start_time": "2019-07-13T18:58:40.252077Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate additional features\n",
    "train.edaCatTargetCatFeat()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Validation3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:58:46.490577Z",
     "start_time": "2019-07-13T18:58:46.297164Z"
    }
   },
   "outputs": [],
   "source": [
    "# parse titles to learn gender, and identify rare titles which may convey status\n",
    "title = [i.split(',')[1].split('.')[0].strip() for i in valid.data['Name']]\n",
    "valid.data['Title'] = pd.Series(title)\n",
    "valid.data['Title'] = valid.data['Title'].replace(['Lady','the Countess','Countess','Capt','Col','Don','Dr'\n",
    "                                              ,'Major','Rev','Sir','Jonkheer','Dona'], 'Rare')\n",
    "valid.data['Title'] = valid.data['Title'].map({'Master' : 0, 'Miss' : 1, 'Ms' : 1 , 'Mme' : 1, 'Mlle' : 1, 'Mrs' : 1, 'Mr' : 2, 'Rare' : 3})\n",
    "\n",
    "# distill cabin feature\n",
    "valid.data['CabinQuarter'] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in valid.data['Cabin']])\n",
    "\n",
    "# additional features\n",
    "valid.data['FamilySize'] = valid.data['SibSp'] + valid.data['Parch'] + 1\n",
    "\n",
    "validPipe = pipeline.Pipeline([\n",
    "        ('customBin', valid.CustomBinner(customBinDict = customBinDict))\n",
    "        ,('percentileBin', valid.PercentileBinner(train = False, trainDict = trainPipe.named_steps['percentileBin'].trainDict_))    \n",
    "    ])\n",
    "valid.data = validPipe.transform(valid.data)\n",
    "\n",
    "# drop features\n",
    "valid.featureDropper(cols = ['Name','Cabin'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:58:46.498954Z",
     "start_time": "2019-07-13T18:58:46.492612Z"
    }
   },
   "outputs": [],
   "source": [
    "# print new columns\n",
    "for col in valid.data.columns:\n",
    "    if col not in valid.featureByDtype_['categorical'] and col not in valid.featureByDtype_['continuous']:\n",
    "        print(col)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:58:46.512779Z",
     "start_time": "2019-07-13T18:58:46.501494Z"
    }
   },
   "outputs": [],
   "source": [
    "# append new continuous features\n",
    "for col in ['FamilySize']:\n",
    "    valid.featureByDtype_['continuous'].append(col)\n",
    "\n",
    "# append new categorical features\n",
    "for col in ['AgeCustomBin','AgePercBin','FarePercBin','FamilySize','FamilySizeCustomBin','Title','CabinQuarter']:\n",
    "    valid.featureByDtype_['categorical'].append(col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Encoding'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Evaluate2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:58:46.542457Z",
     "start_time": "2019-07-13T18:58:46.514944Z"
    }
   },
   "outputs": [],
   "source": [
    "# counts of unique values in training data string columns\n",
    "train.data[train.featureByDtype_['categorical']].apply(pd.Series.nunique, axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:58:46.567062Z",
     "start_time": "2019-07-13T18:58:46.547495Z"
    }
   },
   "outputs": [],
   "source": [
    "# print unique values in each categorical columns\n",
    "for col in train.data[train.featureByDtype_['categorical']]:\n",
    "    try:\n",
    "        print(col, np.unique(train.data[col]))\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:58:46.585397Z",
     "start_time": "2019-07-13T18:58:46.569914Z"
    }
   },
   "outputs": [],
   "source": [
    "# counts of unique values in validation data string columns\n",
    "valid.data[valid.featureByDtype_['categorical']].apply(pd.Series.nunique, axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:58:46.604811Z",
     "start_time": "2019-07-13T18:58:46.588582Z"
    }
   },
   "outputs": [],
   "source": [
    "# print unique values in each categorical columns\n",
    "for col in valid.data[valid.featureByDtype_['categorical']]:\n",
    "    if col not in ['Name','Cabin']:\n",
    "        print(col, np.unique(valid.data[col]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:58:46.632612Z",
     "start_time": "2019-07-13T18:58:46.609358Z"
    }
   },
   "outputs": [],
   "source": [
    "# identify values that are present in the training data but not the validation data, and vice versa\n",
    "for col in train.featureByDtype_['categorical']:\n",
    "    if col not in ['Name','Cabin']:\n",
    "        trainValues = train.data[col].unique()\n",
    "        validValues = valid.data[col].unique()\n",
    "\n",
    "        trainDiff = set(trainValues) - set(validValues)\n",
    "        validDiff = set(validValues) - set(trainValues)\n",
    "\n",
    "        if len(trainDiff) > 0 or len(validDiff) > 0:\n",
    "            print('\\n\\n*** ' + col)\n",
    "            print('Value present in training data, not in validation data')\n",
    "            print(trainDiff)\n",
    "            print('Value present in validation data, not in training data')\n",
    "            print(validDiff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pclass [1 2 3] - ordinal\n",
    "\n",
    "Sex ['female' 'male'] - nominal\n",
    "\n",
    "Embarked ['C' 'Q' 'S'] - nominal\n",
    "\n",
    "HasCabin [0 1] - nominal\n",
    "\n",
    "Title [0 1 2 3] - nominal\n",
    "\n",
    "CabinQuarter ['A' 'B' 'C' 'D' 'E' 'F' 'G' 'X'] - nominal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Training2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:58:46.686305Z",
     "start_time": "2019-07-13T18:58:46.638604Z"
    }
   },
   "outputs": [],
   "source": [
    "# ordinal column encoding instructions\n",
    "ordCatCols = {\n",
    "    'Pclass' : {1 : 1, 2 : 2, 3 : 3}\n",
    "    }\n",
    "\n",
    "# nominal columns\n",
    "nomCatCols = ['Embarked','Sex','CabinQuarter','Title']\n",
    "\n",
    "# apply encodings to training data\n",
    "trainPipe = pipeline.Pipeline([\n",
    "        ('encodeOrdinal', train.CustomOrdinalEncoder(encodings = ordCatCols))    \n",
    "        ,('dummyNominal', train.Dummies(cols = nomCatCols, dropFirst = True))\n",
    "    ])\n",
    "train.data = trainPipe.transform(train.data)\n",
    "train.data[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Validation2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:58:46.736460Z",
     "start_time": "2019-07-13T18:58:46.690684Z"
    }
   },
   "outputs": [],
   "source": [
    "# apply encodings to validation data\n",
    "validPipe = pipeline.Pipeline([\n",
    "        ('encodeOrdinal', valid.CustomOrdinalEncoder(encodings = ordCatCols))    \n",
    "        ,('dummyNominal', valid.Dummies(cols = nomCatCols, dropFirst = False))\n",
    "        ,('levels', valid.MissingDummies(trainCols = train.data.columns))    \n",
    "    ])\n",
    "valid.data = validPipe.transform(valid.data)\n",
    "valid.data[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Transformation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Evaluate4'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:58:46.782369Z",
     "start_time": "2019-07-13T18:58:46.739992Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate skew of continuous features - training data\n",
    "train.skewSummary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:58:46.820987Z",
     "start_time": "2019-07-13T18:58:46.785789Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate skew of continuous features - validation data\n",
    "valid.skewSummary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Training4'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:58:47.696677Z",
     "start_time": "2019-07-13T18:58:46.824103Z"
    }
   },
   "outputs": [],
   "source": [
    "# skew correct in training dataset, which also learns te best lambda value for each columns\n",
    "trainPipe = pipeline.Pipeline([\n",
    "        ('skew', train.SkewTransform(cols = train.featureByDtype_['continuous'], skewMin = 0.75, pctZeroMax = 1.0))\n",
    "    ])\n",
    "train.data = trainPipe.transform(train.data)\n",
    "train.skewSummary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Validation4'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:58:47.785179Z",
     "start_time": "2019-07-13T18:58:47.699117Z"
    }
   },
   "outputs": [],
   "source": [
    "# skew correction in validation dataset using lambdas learned on training data\n",
    "validPipe = pipeline.Pipeline([\n",
    "        ('skew', valid.SkewTransform(train = False, trainDict = trainPipe.named_steps['skew'].colValueDict_))\n",
    "    ])\n",
    "valid.data = validPipe.transform(valid.data)\n",
    "valid.skewSummary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers (final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Outliers-final'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Training6'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:58:47.850812Z",
     "start_time": "2019-07-13T18:58:47.787953Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# identify outliers using IQR\n",
    "trainPipe = pipeline.Pipeline([\n",
    "        ('outlier', train.OutlierIQR(outlierCount = 5, iqrStep = 1.5, features = train.data.columns, dropOutliers = False))     \n",
    "    ])\n",
    "train.data = trainPipe.transform(train.data)\n",
    "\n",
    "# capture outliers\n",
    "iqrOutliers = np.array(sorted(trainPipe.named_steps['outlier'].outliers_))\n",
    "print(iqrOutliers)\n",
    "\n",
    "# train.target = np.delete(train.target, trainPipe.named_steps['outlier'].outliers_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:58:48.202843Z",
     "start_time": "2019-07-13T18:58:47.857892Z"
    }
   },
   "outputs": [],
   "source": [
    "# identify outliers using Isolation Forest\n",
    "clf = ensemble.IsolationForest(behaviour = 'new'\n",
    "                              ,max_samples = train.data.shape[0]\n",
    "                              ,random_state = 0\n",
    "                              ,contamination = 0.01\n",
    "    )\n",
    "clf.fit(train.data[train.data.columns])\n",
    "preds = clf.predict(train.data[train.data.columns])\n",
    "# np.unique(preds, return_counts = True)\n",
    "\n",
    "# evaluate index values\n",
    "mask = np.isin(preds, -1)  # np.in1d if np.isin is not available\n",
    "ifOutliers = np.where(mask)\n",
    "print(ifOutliers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:58:52.776865Z",
     "start_time": "2019-07-13T18:58:48.205343Z"
    }
   },
   "outputs": [],
   "source": [
    "# identify outliers using Extended Isolation Forest\n",
    "if_eif = iso.iForest(train.data.values\n",
    "                    ,ntrees = 100\n",
    "                    ,sample_size = 256\n",
    "                    ,ExtensionLevel = 1\n",
    "    )\n",
    "\n",
    "# calculate anomaly scores\n",
    "anomalies_ratio = 0.01\n",
    "anomaly_scores = if_eif.compute_paths(X_in = train.data.values)\n",
    "anomaly_scores_sorted = np.argsort(anomaly_scores)\n",
    "eifOutliers = np.array(anomaly_scores_sorted[-int(np.ceil(anomalies_ratio * train.data.shape[0])):])\n",
    "print(sorted(eifOutliers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:58:52.786557Z",
     "start_time": "2019-07-13T18:58:52.779559Z"
    }
   },
   "outputs": [],
   "source": [
    "# identify outliers that are identified in multiple algorithms\n",
    "# reduce(np.intersect1d, (iqrOutliers, ifOutliers, eifOutliers))\n",
    "reduce(np.intersect1d, (ifOutliers, eifOutliers))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Data evaluation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Feature-importance'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:58:52.833073Z",
     "start_time": "2019-07-13T18:58:52.790149Z"
    }
   },
   "outputs": [],
   "source": [
    "# feature importance summary table\n",
    "featureImp = train.featureImportanceSummary()\n",
    "featureImp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rationality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Rationality'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:58:53.546015Z",
     "start_time": "2019-07-13T18:58:52.838847Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# percent difference summary\n",
    "dfDiff = abs((((valid.data.describe() + 1) - (train.data.describe() + 1)) / (train.data.describe() + 1)) * 100)\n",
    "dfDiff = dfDiff[dfDiff.columns].replace({0 : np.nan})\n",
    "dfDiff[dfDiff < 0] = np.nan\n",
    "dfDiff = dfDiff.fillna('')\n",
    "display(dfDiff)\n",
    "display(train.data.describe())\n",
    "display(valid.data.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value override"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Value override'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:58:53.552801Z",
     "start_time": "2019-07-13T18:58:53.548691Z"
    }
   },
   "outputs": [],
   "source": [
    "# change clearly erroneous value to what it probably was\n",
    "# exploreValid.data['GarageYrBlt'].replace({2207 : 2007}, inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous feature EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Continuous-feature-EDA3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate & feature vs. target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Univariate-&-feature-vs.-target3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:58:56.281775Z",
     "start_time": "2019-07-13T18:58:53.558036Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# continuous features\n",
    "train.edaCatTargetNumFeat()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Correlation3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation (top vs. target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Correlation-top-vs-target3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:58:56.657799Z",
     "start_time": "2019-07-13T18:58:56.285150Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# correlation heat map with most highly correlated features relative to the target\n",
    "p = PrettierPlot()\n",
    "ax = p.makeCanvas()\n",
    "p.prettyCorrHeatmapTarget(df = train.data\n",
    "                         ,target = train.target\n",
    "                         ,thresh = 0.2\n",
    "                         ,ax = ax\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Modeling'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Prepare-training-data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:58:57.213569Z",
     "start_time": "2019-07-13T18:58:56.662391Z"
    }
   },
   "outputs": [],
   "source": [
    "# import training data\n",
    "dfTrain = pd.read_csv('/home/data-science-portfolio/data/kaggleTitanic/train.csv')\n",
    "train = mlm.Machine(data = dfTrain\n",
    "                   ,target = ['Survived']\n",
    "                   ,removeFeatures = ['PassengerId','Ticket']                      \n",
    "                   ,overrideCat = ['Pclass','SibSp','Parch']\n",
    "                   ,targetType = 'categorical'\n",
    "    )\n",
    "\n",
    "### feature engineering\n",
    "# parse titles to learn gender, and identify rare titles which may convey status\n",
    "title = [i.split(',')[1].split('.')[0].strip() for i in train.data['Name']]\n",
    "train.data['Title'] = pd.Series(title)\n",
    "train.data['Title'] = train.data['Title'].replace(['Lady','the Countess','Countess','Capt','Col','Don','Dr'\n",
    "                                              ,'Major','Rev','Sir','Jonkheer','Dona'], 'Rare')\n",
    "train.data['Title'] = train.data['Title'].map({'Master' : 0, 'Miss' : 1, 'Ms' : 1 , 'Mme' : 1, 'Mlle' : 1, 'Mrs' : 1, 'Mr' : 2, 'Rare' : 3})\n",
    "\n",
    "# distill cabin feature\n",
    "train.data['CabinQuarter'] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in train.data['Cabin']])\n",
    "\n",
    "# family size features\n",
    "train.data['FamilySize'] = train.data['SibSp'] + train.data['Parch'] + 1\n",
    "\n",
    "# custom bin specifications\n",
    "customBinDict = {'Age' : [16, 32, 48, 64]\n",
    "                ,'FamilySize' : [1, 2, 4]\n",
    "          }\n",
    "# categorical column specifications\n",
    "ordCatCols = {\n",
    "    'Pclass' : {1 : 1, 2 : 2, 3 : 3}\n",
    "    }\n",
    "nomCatCols = ['Embarked','Sex','CabinQuarter','Title']\n",
    "\n",
    "# remove outliers\n",
    "outliers = np.array([27, 88, 258, 311, 341, 438, 679, 737, 742])\n",
    "train.data = train.data.drop(train.data.index[outliers])\n",
    "train.target = train.target.drop(index = outliers)\n",
    "\n",
    "### pipeline\n",
    "trainPipe = pipeline.Pipeline([\n",
    "        ('imputeMedian', train.ContextImputer(nullCol = 'Age', contextCol = 'Parch', strategy = 'median'))     \n",
    "        ,('imputeMode', train.ModeImputer(cols = ['Embarked']))\n",
    "        ,('customBin', train.CustomBinner(customBinDict = customBinDict))\n",
    "        ,('percentileBin', train.PercentileBinner(cols = ['Age','Fare'], percs = [25, 50, 75]))    \n",
    "        ,('encodeOrdinal', train.CustomOrdinalEncoder(encodings = ordCatCols))    \n",
    "        ,('dummyNominal', train.Dummies(cols = nomCatCols, dropFirst = True))\n",
    "        ,('skew', train.SkewTransform(cols = train.featureByDtype_['continuous'], skewMin = 0.75, pctZeroMax = 1.0))    \n",
    "    ])\n",
    "train.data = trainPipe.transform(train.data)\n",
    "\n",
    "# drop features\n",
    "train.featureDropper(cols = ['Name','Cabin'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Prepare-validation-data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:58:57.450364Z",
     "start_time": "2019-07-13T18:58:57.215241Z"
    }
   },
   "outputs": [],
   "source": [
    "### import valid data\n",
    "dfValid = pd.read_csv('/home/data-science-portfolio/data/kaggleTitanic/test.csv')\n",
    "valid = mlm.Machine(data = dfValid\n",
    "                   ,removeFeatures = ['PassengerId','Ticket']                      \n",
    "                   ,overrideCat = ['Pclass','SibSp','Parch']\n",
    "    )\n",
    "\n",
    "### feature engineering\n",
    "# parse titles to learn gender, and identify rare titles which may convey status\n",
    "title = [i.split(',')[1].split('.')[0].strip() for i in valid.data['Name']]\n",
    "valid.data['Title'] = pd.Series(title)\n",
    "valid.data['Title'] = valid.data['Title'].replace(['Lady','the Countess','Countess','Capt','Col','Don','Dr'\n",
    "                                              ,'Major','Rev','Sir','Jonkheer','Dona'], 'Rare')\n",
    "valid.data['Title'] = valid.data['Title'].map({'Master' : 0, 'Miss' : 1, 'Ms' : 1 , 'Mme' : 1, 'Mlle' : 1, 'Mrs' : 1, 'Mr' : 2, 'Rare' : 3})\n",
    "\n",
    "# distill cabin feature\n",
    "valid.data['CabinQuarter'] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in valid.data['Cabin']])\n",
    "\n",
    "# additional features\n",
    "valid.data['FamilySize'] = valid.data['SibSp'] + valid.data['Parch'] + 1\n",
    "\n",
    "### pipeline\n",
    "validPipe = pipeline.Pipeline([\n",
    "        ('imputeMedian', valid.ContextImputer(nullCol = 'Age', contextCol = 'Parch', train = False, trainDf = trainPipe.named_steps['imputeMedian'].fillDf))\n",
    "        ,('imputeMedian2', valid.NumericalImputer(cols = ['Fare','Age'], strategy = 'median'))    \n",
    "        ,('customBin', valid.CustomBinner(customBinDict = customBinDict))\n",
    "        ,('percentileBin', valid.PercentileBinner(train = False, trainDict = trainPipe.named_steps['percentileBin'].trainDict_))   \n",
    "        ,('encodeOrdinal', valid.CustomOrdinalEncoder(encodings = ordCatCols))    \n",
    "        ,('dummyNominal', valid.Dummies(cols = nomCatCols, dropFirst = False))\n",
    "        ,('levels', valid.MissingDummies(trainCols = train.data.columns))    \n",
    "        ,('skew', valid.SkewTransform(train = False, trainDict = trainPipe.named_steps['skew'].colValueDict_))    \n",
    "    ])\n",
    "valid.data = validPipe.transform(valid.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'GridSearch'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T02:02:16.715785Z",
     "start_time": "2019-07-01T02:02:15.701Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    " # parameter space\n",
    "allSpace = {\n",
    "            'lightgbm.LGBMClassifier' : {\n",
    "                'class_weight' : hp.choice('class_weight', [None])\n",
    "                ,'colsample_bytree': hp.uniform('colsample_bytree', 0.4, 0.7)\n",
    "                ,'boosting_type' : hp.choice('boosting_type', ['dart'])                \n",
    "                ,'subsample': hp.uniform('subsample', 0.5, 1)\n",
    "                ,'learning_rate' : hp.uniform('learning_rate', 0.15, 0.25)\n",
    "                ,'max_depth' : hp.choice('max_depth', np.arange(4, 20, dtype = int))\n",
    "                ,'min_child_samples' : hp.quniform('min_child_samples', 50, 150, 5)\n",
    "                ,'n_estimators' : hp.choice('n_estimators', np.arange(100, 4000, 10, dtype = int))\n",
    "                ,'num_leaves': hp.quniform('num_leaves', 30, 70, 1)\n",
    "                ,'reg_alpha': hp.uniform('reg_alpha', 0.75, 1.25)\n",
    "                ,'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0)\n",
    "                ,'subsample_for_bin': hp.quniform('subsample_for_bin', 100000, 350000, 20000)\n",
    "            }\n",
    "            ,'linear_model.LogisticRegression' : {\n",
    "                'C': hp.uniform('C', 0.04, 0.1)\n",
    "                ,'penalty': hp.choice('penalty', ['l1'])\n",
    "            }\n",
    "            ,'xgboost.XGBClassifier' : {\n",
    "                'colsample_bytree' : hp.uniform('colsample_bytree', 0.4, 0.7)\n",
    "                ,'gamma' : hp.quniform('gamma', 0.0, 10, 0.05)\n",
    "                ,'learning_rate' : hp.quniform('learning_rate', 0.01, 0.2, 0.01)\n",
    "                ,'max_depth' : hp.choice('max_depth', np.arange(2, 15, dtype = int))\n",
    "                ,'min_child_weight': hp.quniform ('min_child_weight', 2.5, 7.5, 1)\n",
    "                ,'n_estimators' : hp.choice('n_estimators', np.arange(100, 4000, 10, dtype = int))\n",
    "                ,'subsample': hp.uniform ('subsample', 0.4, 0.7)\n",
    "            }\n",
    "            ,'ensemble.RandomForestClassifier' : {\n",
    "                'bootstrap' : hp.choice('bootstrap', [True, False])\n",
    "                ,'max_depth' : hp.choice('max_depth', np.arange(2, 10, dtype = int))\n",
    "                ,'n_estimators' : hp.choice('n_estimators', np.arange(100, 8000, 10, dtype = int))\n",
    "                ,'max_features' : hp.choice('max_features', ['sqrt'])\n",
    "                ,'min_samples_split' : hp.choice('min_samples_split', np.arange(15, 25, dtype = int))\n",
    "                ,'min_samples_leaf' : hp.choice('min_samples_leaf', np.arange(2, 20, dtype = int))\n",
    "            }\n",
    "            ,'ensemble.GradientBoostingClassifier' : {\n",
    "                'n_estimators' : hp.choice('n_estimators', np.arange(100, 4000, 10, dtype = int))\n",
    "                ,'max_depth' : hp.choice('max_depth', np.arange(2, 11, dtype = int))\n",
    "                ,'max_features' : hp.choice('max_features', ['sqrt'])    \n",
    "                ,'learning_rate' : hp.quniform('learning_rate', 0.01, 0.09, 0.01)\n",
    "                ,'loss' : hp.choice('loss', ['deviance','exponential'])    \n",
    "                ,'min_samples_split' : hp.choice('min_samples_split', np.arange(2, 40, dtype = int))\n",
    "                ,'min_samples_leaf' : hp.choice('min_samples_leaf', np.arange(2, 40, dtype = int))\n",
    "            }\n",
    "            ,'ensemble.AdaBoostClassifier' : {\n",
    "                'n_estimators' : hp.choice('n_estimators', np.arange(100, 4000, 10, dtype = int))\n",
    "                ,'learning_rate' : hp.quniform('learning_rate', 0.1, 0.25, 0.01)\n",
    "                ,'algorithm' : hp.choice('algorithm', ['SAMME'])                    \n",
    "            }\n",
    "            ,'naive_bayes.BernoulliNB' : {\n",
    "                'alpha' :  hp.uniform('alpha', 0.01, 2)\n",
    "            }\n",
    "            ,'ensemble.BaggingClassifier' : {\n",
    "                'n_estimators' : hp.choice('n_estimators', np.arange(100, 4000, 10, dtype = int))\n",
    "                ,'max_samples' : hp.uniform('max_samples', 0.01, 0.3)                    \n",
    "            }\n",
    "            ,'ensemble.ExtraTreesClassifier' : {\n",
    "                'n_estimators' : hp.choice('n_estimators', np.arange(100, 4000, 10, dtype = int))\n",
    "                ,'max_depth' : hp.choice('max_depth', np.arange(2, 15, dtype = int))\n",
    "                ,'min_samples_split' : hp.choice('min_samples_split', np.arange(4, 30, dtype = int))\n",
    "                ,'min_samples_leaf' : hp.choice('min_samples_leaf', np.arange(2, 20, dtype = int))\n",
    "                ,'max_features' : hp.choice('max_features', ['auto'])\n",
    "                ,'criterion' : hp.choice('criterion', ['entropy'])\n",
    "            }\n",
    "            ,'svm.SVC' : {\n",
    "                'C' : hp.uniform('C', 4, 15)\n",
    "                ,'decision_function_shape' : hp.choice('decision_function_shape', ['ovr'])\n",
    "                ,'gamma' : hp.uniform('gamma', 0.00000001, 1.5)\n",
    "            }\n",
    "            ,'neighbors.KNeighborsClassifier' : {\n",
    "                'algorithm' : hp.choice('algorithm', ['ball_tree','brute'])\n",
    "                ,'n_neighbors' : hp.choice('n_neighbors', np.arange(1, 15, dtype = int))\n",
    "                ,'weights' : hp.choice('weights', ['uniform'])\n",
    "            }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T02:02:16.717419Z",
     "start_time": "2019-07-01T02:02:15.706Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# execute bayesian optimization grid search\n",
    "analysis = 'titanic'\n",
    "train.execBayesOptimSearch(allSpace = allSpace\n",
    "                          ,resultsDir = 'data/{}_hyperopt_{}.csv'.format(rundate, analysis)\n",
    "                          ,X = train.data\n",
    "                          ,y = train.target\n",
    "                          ,scoring = 'accuracy'\n",
    "                          ,n_folds = 8\n",
    "                          ,n_jobs = 16\n",
    "                          ,iters = 1000\n",
    "                          ,verbose = 0\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T02:02:16.719472Z",
     "start_time": "2019-07-01T02:02:15.713Z"
    }
   },
   "outputs": [],
   "source": [
    "# read scores summary table\n",
    "resultsDf = pd.read_csv('data/20190423_hyperopt_titanic.csv', na_values = 'nan')\n",
    "results = train.unpackParams(resultsDf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions using test set\n",
    "pipe.fit(XTrain, yTrain)\n",
    "yPred = pipe.predict(XTest)\n",
    "\n",
    "# visualize results with confusion matrix\n",
    "p = ptp.plotter.PrettierPlot()\n",
    "ax = p.makeCanvas(title = '', xLabel = 'Predicted', yLabel = 'Actual', yShift = 0.5)\n",
    "p.prettyConfusionMatrix(yTest = yTest\n",
    "                       ,yPred = yPred\n",
    "                       ,ax = ax\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot and ROC curve using only two features from the breast cancer dataset\n",
    "from scipy import interp\n",
    "\n",
    "pipe = pipeline.make_pipeline(preprocessing.StandardScaler()\n",
    "                             ,decomposition.PCA(n_components = 2)\n",
    "                             ,linear_model.LogisticRegression(penalty = 'l2', random_state = 1, C = 100.0)\n",
    "    )\n",
    "XTrain2 = XTrain[:, [4, 14]]\n",
    "cv = list(model_selection.StratifiedKFold(n_splits = 3,random_state = 1).split(XTrain,yTrain))\n",
    "\n",
    "# plot ROC curves\n",
    "p = ptp.plotter.PrettierPlot(chartProp = 12)\n",
    "ax = p.makeCanvas(title = '', xLabel = 'false positive rate', yLabel = 'true positive rate'\n",
    "                 ,yShift = 0.82, plotOrientation = 'square'\n",
    "    )\n",
    "for i, (train, test) in enumerate(cv):\n",
    "    XTrainCV = XTrain2[train]    \n",
    "    yTrainCV = yTrain[train]\n",
    "    XTestCV = XTrain2[test]    \n",
    "    yTestCV = yTrain[test]\n",
    "\n",
    "    p.prettyRocCurve(model = pipe\n",
    "                    ,xTrain = XTrainCV\n",
    "                    ,yTrain = yTrainCV\n",
    "                    ,xTest = XTestCV\n",
    "                    ,yTest = yTestCV\n",
    "                    ,linecolor = ptp.style.styleHexMid[i]\n",
    "                    ,ax = ax\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T02:02:16.721312Z",
     "start_time": "2019-07-01T02:02:15.718Z"
    }
   },
   "outputs": [],
   "source": [
    "# loss plot\n",
    "train.lossPlot(resultsDf = results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T02:02:16.722729Z",
     "start_time": "2019-07-01T02:02:15.724Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# estimator parameter plots\n",
    "train.paramPlot(results = results, allSpace = allSpace, nIter = 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T02:02:16.724269Z",
     "start_time": "2019-07-01T02:02:15.729Z"
    }
   },
   "outputs": [],
   "source": [
    "sampleSpace = {\n",
    "#             'param': hp.uniform('param', np.log(0.4), np.log(0.6))\n",
    "            '' : 0.000001 + hp.uniform('gamma', 0.000001, 10)\n",
    "#             'param2': hp.loguniform('param2', np.log(0.001), np.log(0.01))\n",
    "        }\n",
    "\n",
    "train.samplePlot(sampleSpace, 1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Evaluation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model explanability\n",
    "\n",
    "https://www.kaggle.com/learn/machine-learning-explainability\n",
    "https://www.kaggle.com/dansbecker/partial-dependence-plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Feature-importance'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Permutation importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Permutation-importance'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T02:02:16.726938Z",
     "start_time": "2019-07-01T02:02:15.740Z"
    }
   },
   "outputs": [],
   "source": [
    "# permutation importance\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "perm = PermutationImportance(my_model, random_state=1).fit(val_X, val_y)\n",
    "eli5.show_weights(perm, feature_names = val_X.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partial plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Partial-plots'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T02:02:16.728503Z",
     "start_time": "2019-07-01T02:02:15.747Z"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "from matplotlib import pyplot as plt\n",
    "from pdpbox import pdp, get_dataset, info_plots\n",
    "\n",
    "# Create the data that we will plot\n",
    "pdp_goals = pdp.pdp_isolate(model=tree_model, dataset=val_X, model_features=feature_names, feature='Goal Scored')\n",
    "\n",
    "# plot it\n",
    "pdp.pdp_plot(pdp_goals, 'Goal Scored')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T02:02:16.729970Z",
     "start_time": "2019-07-01T02:02:15.753Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_to_plot = 'Distance Covered (Kms)'\n",
    "pdp_dist = pdp.pdp_isolate(model=tree_model, dataset=val_X, model_features=feature_names, feature=feature_to_plot)\n",
    "\n",
    "pdp.pdp_plot(pdp_dist, feature_to_plot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T02:02:16.731442Z",
     "start_time": "2019-07-01T02:02:15.759Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build Random Forest model\n",
    "rf_model = RandomForestClassifier(random_state=0).fit(train_X, train_y)\n",
    "\n",
    "pdp_dist = pdp.pdp_isolate(model=rf_model, dataset=val_X, model_features=feature_names, feature=feature_to_plot)\n",
    "\n",
    "pdp.pdp_plot(pdp_dist, feature_to_plot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T02:02:16.733388Z",
     "start_time": "2019-07-01T02:02:15.764Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2D plots\n",
    "# Similar to previous PDP plot except we use pdp_interact instead of pdp_isolate and pdp_interact_plot instead of pdp_isolate_plot\n",
    "features_to_plot = ['Goal Scored', 'Distance Covered (Kms)']\n",
    "inter1  =  pdp.pdp_interact(model=tree_model, dataset=val_X, model_features=feature_names, features=features_to_plot)\n",
    "\n",
    "pdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=features_to_plot, plot_type='contour')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SHAP values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'SHAP-values'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T02:02:16.735337Z",
     "start_time": "2019-07-01T02:02:15.771Z"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "row_to_show = 5\n",
    "data_for_prediction = val_X.iloc[row_to_show]  # use 1 row of data here. Could use multiple rows if desired\n",
    "data_for_prediction_array = data_for_prediction.values.reshape(1, -1)\n",
    "\n",
    "\n",
    "my_model.predict_proba(data_for_prediction_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T02:02:16.737217Z",
     "start_time": "2019-07-01T02:02:15.778Z"
    }
   },
   "outputs": [],
   "source": [
    "import shap  # package used to calculate Shap values\n",
    "\n",
    "# Create object that can calculate shap values\n",
    "explainer = shap.TreeExplainer(my_model)\n",
    "\n",
    "# Calculate Shap values\n",
    "shap_values = explainer.shap_values(data_for_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T02:02:16.738957Z",
     "start_time": "2019-07-01T02:02:15.784Z"
    }
   },
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T02:02:16.740929Z",
     "start_time": "2019-07-01T02:02:15.789Z"
    }
   },
   "outputs": [],
   "source": [
    "# use Kernel SHAP to explain test set predictions\n",
    "k_explainer = shap.KernelExplainer(my_model.predict_proba, train_X)\n",
    "k_shap_values = k_explainer.shap_values(data_for_prediction)\n",
    "shap.force_plot(k_explainer.expected_value[1], k_shap_values[1], data_for_prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T02:02:16.747234Z",
     "start_time": "2019-07-01T02:02:15.795Z"
    }
   },
   "outputs": [],
   "source": [
    "shap.DeepExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T02:02:16.748842Z",
     "start_time": "2019-07-01T02:02:15.800Z"
    }
   },
   "outputs": [],
   "source": [
    "import shap  # package used to calculate Shap values\n",
    "\n",
    "# Create object that can calculate shap values\n",
    "explainer = shap.TreeExplainer(my_model)\n",
    "\n",
    "# calculate shap values. This is what we will plot.\n",
    "# Calculate shap_values for all of val_X rather than a single row, to have more data for plot.\n",
    "shap_values = explainer.shap_values(val_X)\n",
    "\n",
    "# Make plot. Index of [1] is explained in text below.\n",
    "shap.summary_plot(shap_values[1], val_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T02:02:16.750211Z",
     "start_time": "2019-07-01T02:02:15.806Z"
    }
   },
   "outputs": [],
   "source": [
    "import shap  # package used to calculate Shap values\n",
    "\n",
    "# Create object that can calculate shap values\n",
    "explainer = shap.TreeExplainer(my_model)\n",
    "\n",
    "# calculate shap values. This is what we will plot.\n",
    "shap_values = explainer.shap_values(X)\n",
    "\n",
    "# make plot.\n",
    "shap.dependence_plot('Ball Possession %', shap_values[1], X, interaction_index=\"Goal Scored\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Stacking'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primary models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Primary-models'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T02:02:16.752325Z",
     "start_time": "2019-07-01T02:02:15.816Z"
    }
   },
   "outputs": [],
   "source": [
    "resultsDf[resultsDf['estimator'] == 'xgboost.XGBClassifier'].sort_values(['mean'], ascending = [False])[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T02:02:16.754134Z",
     "start_time": "2019-07-01T02:02:15.821Z"
    }
   },
   "outputs": [],
   "source": [
    "def topParamSelector(resultsDf, num):\n",
    "    models = {}\n",
    "    for estimator in resultsDf['estimator'].unique():\n",
    "        estDf = resultsDf[resultsDf['estimator'] == estimator].sort_values(['mean'], ascending = [False])['iteration'][:num]\n",
    "        models[estimator] = estDf.values.tolist()\n",
    "    return models\n",
    "models = topParamSelector(resultsDf = resultsDf, num = 1)\n",
    "models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T02:02:16.756375Z",
     "start_time": "2019-07-01T02:02:15.827Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get out-of-fold predictions\n",
    "oofTrain, oofValid, columns =\\\n",
    "    train.modelStacker(models = models\n",
    "                      ,resultsDf = resultsDf\n",
    "                      ,XTrain = train.data.values\n",
    "                      ,yTrain = train.target\n",
    "                      ,XValid = valid.data.values\n",
    "                      ,nFolds = 2\n",
    "                      ,nJobs = 16\n",
    "    )\n",
    "                                                             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T02:02:16.758058Z",
     "start_time": "2019-07-01T02:02:15.833Z"
    }
   },
   "outputs": [],
   "source": [
    "# view correlations of predictions\n",
    "sns.set_style('whitegrid')\n",
    "p = qp.plotter.QuickPlot()\n",
    "ax = p.makeCanvas(position = 111)\n",
    "p.qpCorrHeatmap(df = pd.DataFrame(oofTrain, columns = columns)\n",
    "               ,annot = True\n",
    "               ,ax = ax\n",
    "               ,vmin = 0\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Meta-model'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T02:02:16.777169Z",
     "start_time": "2019-07-01T02:02:15.841Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    " # parameter space\n",
    "allSpace = {\n",
    "            'lightgbm.LGBMClassifier' : {\n",
    "                'class_weight' : hp.choice('class_weight', [None])\n",
    "                ,'colsample_bytree': hp.uniform('colsample_bytree', 0.4, 0.7)\n",
    "                ,'boosting_type' : hp.choice('boosting_type', ['dart'])                \n",
    "                ,'subsample': hp.uniform('subsample', 0.5, 1)\n",
    "                ,'learning_rate' : hp.uniform('learning_rate', 0.15, 0.25)\n",
    "                ,'max_depth' : hp.choice('max_depth', np.arange(4, 20, dtype = int))\n",
    "                ,'min_child_samples' : hp.quniform('min_child_samples', 50, 150, 5)\n",
    "                ,'n_estimators' : hp.choice('n_estimators', np.arange(100, 4000, 10, dtype = int))\n",
    "                ,'num_leaves': hp.quniform('num_leaves', 30, 70, 1)\n",
    "                ,'reg_alpha': hp.uniform('reg_alpha', 0.75, 1.25)\n",
    "                ,'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0)\n",
    "                ,'subsample_for_bin': hp.quniform('subsample_for_bin', 100000, 350000, 20000)\n",
    "            }\n",
    "            ,'linear_model.LogisticRegression' : {\n",
    "                'C': hp.uniform('C', 0.04, 0.1)\n",
    "                ,'penalty': hp.choice('penalty', ['l1'])\n",
    "            }\n",
    "            ,'xgboost.XGBClassifier' : {\n",
    "                'colsample_bytree' : hp.uniform('colsample_bytree', 0.4, 0.7)\n",
    "                ,'gamma' : hp.quniform('gamma', 0.0, 10, 0.05)\n",
    "                ,'learning_rate' : hp.quniform('learning_rate', 0.01, 0.2, 0.01)\n",
    "                ,'max_depth' : hp.choice('max_depth', np.arange(2, 15, dtype = int))\n",
    "                ,'min_child_weight': hp.quniform ('min_child_weight', 2.5, 7.5, 1)\n",
    "                ,'n_estimators' : hp.choice('n_estimators', np.arange(100, 4000, 10, dtype = int))\n",
    "                ,'subsample': hp.uniform ('subsample', 0.4, 0.7)\n",
    "            }\n",
    "            ,'ensemble.RandomForestClassifier' : {\n",
    "                'bootstrap' : hp.choice('bootstrap', [True, False])\n",
    "                ,'max_depth' : hp.choice('max_depth', np.arange(2, 10, dtype = int))\n",
    "                ,'n_estimators' : hp.choice('n_estimators', np.arange(100, 8000, 10, dtype = int))\n",
    "                ,'max_features' : hp.choice('max_features', ['sqrt'])\n",
    "                ,'min_samples_split' : hp.choice('min_samples_split', np.arange(15, 25, dtype = int))\n",
    "                ,'min_samples_leaf' : hp.choice('min_samples_leaf', np.arange(2, 20, dtype = int))\n",
    "            }\n",
    "            ,'ensemble.GradientBoostingClassifier' : {\n",
    "                'n_estimators' : hp.choice('n_estimators', np.arange(100, 4000, 10, dtype = int))\n",
    "                ,'max_depth' : hp.choice('max_depth', np.arange(2, 11, dtype = int))\n",
    "                ,'max_features' : hp.choice('max_features', ['sqrt'])    \n",
    "                ,'learning_rate' : hp.quniform('learning_rate', 0.01, 0.09, 0.01)\n",
    "                ,'loss' : hp.choice('loss', ['deviance','exponential'])    \n",
    "                ,'min_samples_split' : hp.choice('min_samples_split', np.arange(2, 40, dtype = int))\n",
    "                ,'min_samples_leaf' : hp.choice('min_samples_leaf', np.arange(2, 40, dtype = int))\n",
    "            }\n",
    "            ,'ensemble.AdaBoostClassifier' : {\n",
    "                'n_estimators' : hp.choice('n_estimators', np.arange(100, 4000, 10, dtype = int))\n",
    "                ,'learning_rate' : hp.quniform('learning_rate', 0.1, 0.25, 0.01)\n",
    "                ,'algorithm' : hp.choice('algorithm', ['SAMME'])                    \n",
    "            }\n",
    "            ,'naive_bayes.BernoulliNB' : {\n",
    "                'alpha' :  hp.uniform('alpha', 0.01, 2)\n",
    "            }\n",
    "            ,'ensemble.BaggingClassifier' : {\n",
    "                'n_estimators' : hp.choice('n_estimators', np.arange(100, 4000, 10, dtype = int))\n",
    "                ,'max_samples' : hp.uniform('max_samples', 0.01, 0.3)                    \n",
    "            }\n",
    "            ,'ensemble.ExtraTreesClassifier' : {\n",
    "                'n_estimators' : hp.choice('n_estimators', np.arange(100, 4000, 10, dtype = int))\n",
    "                ,'max_depth' : hp.choice('max_depth', np.arange(2, 15, dtype = int))\n",
    "                ,'min_samples_split' : hp.choice('min_samples_split', np.arange(4, 30, dtype = int))\n",
    "                ,'min_samples_leaf' : hp.choice('min_samples_leaf', np.arange(2, 20, dtype = int))\n",
    "                ,'max_features' : hp.choice('max_features', ['auto'])\n",
    "                ,'criterion' : hp.choice('criterion', ['entropy'])\n",
    "            }\n",
    "            'svm.SVC' : {\n",
    "                'C' : hp.uniform('C', 0.00000001, 15)\n",
    "                ,'decision_function_shape' : hp.choice('decision_function_shape', ['ovr','ovo'])\n",
    "                ,'gamma' : hp.uniform('gamma', 0.00000001, 1.5)\n",
    "            }\n",
    "            ,'neighbors.KNeighborsClassifier' : {\n",
    "                'algorithm' : hp.choice('algorithm', ['ball_tree','brute'])\n",
    "                ,'n_neighbors' : hp.choice('n_neighbors', np.arange(1, 15, dtype = int))\n",
    "                ,'weights' : hp.choice('weights', ['uniform'])\n",
    "            }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T02:02:16.779160Z",
     "start_time": "2019-07-01T02:02:15.847Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# execute bayesian optimization grid search\n",
    "train.execBayesOptimSearch(allSpace = allSpace\n",
    "                          ,resultsDir = 'data/{}_hyperopt_meta_{}_2.csv'.format(rundate, analysis)\n",
    "                          ,X = oofTrain\n",
    "                          ,y = train.target\n",
    "                          ,scoring = 'accuracy'\n",
    "                          ,n_folds = 8\n",
    "                          ,n_jobs = 8\n",
    "                          ,iters = 3000\n",
    "                          ,verbose = 0\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T02:02:16.781225Z",
     "start_time": "2019-07-01T02:02:15.853Z"
    }
   },
   "outputs": [],
   "source": [
    "# read scores summary table\n",
    "resultsMetaDf = pd.read_csv('data/20190423_hyperopt_meta_titanic_2.csv', na_values = 'nan')\n",
    "resultsMeta = train.unpackParams(resultsMetaDf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T02:02:16.783163Z",
     "start_time": "2019-07-01T02:02:15.858Z"
    }
   },
   "outputs": [],
   "source": [
    "# loss plot\n",
    "train.lossPlot(resultsDf = resultsMeta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T02:02:16.784904Z",
     "start_time": "2019-07-01T02:02:15.864Z"
    }
   },
   "outputs": [],
   "source": [
    "# estimator parameter plots\n",
    "train.paramPlot(results = resultsMeta, allSpace = allSpace, nIter = 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Submission'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Standard'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T02:02:16.787282Z",
     "start_time": "2019-07-01T02:02:15.874Z"
    }
   },
   "outputs": [],
   "source": [
    "## standard model fit and predict\n",
    "# select estimator and iteration\n",
    "# estimator = 'ensemble.RandomForestClassifier'\n",
    "# iteration = 1955\n",
    "# estimator = 'xgboost.XGBClassifier'\n",
    "# iteration = 2097\n",
    "estimator = 'lightgbm.LGBMClassifier'\n",
    "iteration = 2264\n",
    "\n",
    "# extract params and instantiate model\n",
    "params = train.paramExtractor(resultsDf = resultsDf, estimator = estimator, iteration = iteration)\n",
    "model = eval('{0}(**{1})'.format(estimator, params))\n",
    "\n",
    "# fit model and make predictions\n",
    "model.fit(train.data, train.target)\n",
    "yPred = model.predict(valid.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T02:02:16.789392Z",
     "start_time": "2019-07-01T02:02:15.880Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate prediction submission file\n",
    "my_submission = pd.DataFrame({'PassengerId': dfValid.PassengerId, 'Survived': yPred})\n",
    "my_submission.to_csv('data/submission.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Stack'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T02:02:16.791607Z",
     "start_time": "2019-07-01T02:02:15.888Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resultsMetaDf.sort_values(['mean'], ascending = [False])[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T02:02:16.794001Z",
     "start_time": "2019-07-01T02:02:15.894Z"
    }
   },
   "outputs": [],
   "source": [
    "# best second level learning model\n",
    "# estimator = 'xgboost.XGBClassifier'\n",
    "# estimator = 'ensemble.RandomForestClassifier'\n",
    "# estimator = 'ensemble.GradientBoostingClassifier'\n",
    "estimator = 'svm.SVC'\n",
    "\n",
    "iteration = 2436\n",
    "\n",
    "# extract params and instantiate model\n",
    "params = train.paramExtractor(resultsDf = resultsMetaDf, estimator = estimator, iteration = iteration)\n",
    "model = eval('{0}(**{1})'.format(estimator, params))\n",
    "\n",
    "model.fit(oofTrain, train.target)\n",
    "yPred = model.predict(oofValid)\n",
    "print(sum(yPred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T02:02:16.796268Z",
     "start_time": "2019-07-01T02:02:15.899Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate prediction submission file\n",
    "my_submission = pd.DataFrame({'PassengerId': dfValid.PassengerId, 'Survived': yPred})\n",
    "my_submission.to_csv('data/submission.csv', index = False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
