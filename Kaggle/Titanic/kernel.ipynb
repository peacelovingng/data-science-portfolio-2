{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Kaggle competition - house prices__\n",
    "\n",
    "1. [Kaggle competition - house prices](#Kaggle-competition-house-prices)\n",
    "1. [Import](#Import)\n",
    "    1. [Tools](#Tools)\n",
    "    1. [Data](#Data)    \n",
    "1. [Initial EDA](#Initial-EDA)\n",
    "    1. [Categorical feature EDA](#Categorical-feature-EDA)\n",
    "        1. [Univariate & feature vs. target](#Univariate-&-feature-vs.-target)\n",
    "    1. [Continuous feature EDA](#Continuous-feature-EDA)\n",
    "        1. [Univariate & feature vs. target](#Univariate-&-feature-vs.-target2)\n",
    "        1. [Correlation](#Correlation)\n",
    "            1. [Correlation (all samples)](#Correlation-all-samples)\n",
    "            1. [Correlation (top vs. target)](#Correlation-top-vs-target)\n",
    "        1. [Pair plot](#Pair-plot)\n",
    "    1. [Faceting](#Faceting)\n",
    "    1. [Target variable evaluation](#Target-variable-evaluation)    \n",
    "1. [Data cleaning](#Data-cleaning)\n",
    "    1. [Outliers (preliminary)](#Outliers-preliminary)\n",
    "        1. [Training](#Training5)\n",
    "        1. [Validation](#Validation5)\n",
    "    1. [Missing data](#Missing-data)\n",
    "        1. [Evaluate](#Evaluate1)\n",
    "        1. [Training](#Training1)\n",
    "        1. [Validation](#Validation1)\n",
    "    1. [Engineering](#Engineering)\n",
    "        1. [Evaluate](#Evaluate3)\n",
    "        1. [Training](#Training3)\n",
    "        1. [Validation](#Validation3)\n",
    "    1. [Encoding](#Encoding)\n",
    "        1. [Evaluate](#Evaluate2)\n",
    "        1. [Training](#Training2)\n",
    "        1. [Validation](#Validation2)\n",
    "    1. [Transformation](#Transformation)\n",
    "        1. [Evaluate](#Evaluate4)\n",
    "        1. [Training](#Training4)\n",
    "        1. [Validation](#Validation4)\n",
    "    1. [Outliers (final)](#Outliers-final)\n",
    "        1. [Training](#Training6)\n",
    "1. [Data evaluation](#Data-evaluation)\n",
    "    1. [Feature importance](#Feature-importance)    \n",
    "    1. [Rationality](#Rationality)\n",
    "    1. [Value override](#Value-override)\n",
    "    1. [Continuous feature EDA](#Continuous-feature-EDA3)\n",
    "        1. [Univariate & feature vs. target](#Univariate-&-feature-vs.-target3)\n",
    "        1. [Correlation](#Correlation3)\n",
    "            1. [Correlation (top vs. target)](#Correlation-top-vs-target3)\n",
    "1. [Modeling](#Modeling)\n",
    "    1. [Prepare training data](#Prepare-training-data)\n",
    "    1. [Prepare validation data](#Prepare-validation-data)\n",
    "    1. [GridSearch](#GridSearch)\n",
    "        1. [Evaluation](#Evaluation)\n",
    "        1. [Model explanability](#Model-explanability)\n",
    "            1. [Permutation importance](#Permutation-importance)\n",
    "            1. [Partial plots](#Partial-plots)\n",
    "            1. [SHAP values](#SHAP-values)\n",
    "    1. [Stacking](#Stacking)\n",
    "        1. [Primary models](#Primary-models)\n",
    "        1. [Meta model](#Meta-model)                \n",
    "1. [Submission](#Submission)\n",
    "    1. [Stack](#Stack)\n",
    "    1. [Standard](#Standard)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle competition - Titanic\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Kaggle-competition-house-prices'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Import'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Tools'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T13:48:24.272052Z",
     "start_time": "2019-04-11T13:48:16.527792Z"
    }
   },
   "outputs": [],
   "source": [
    "# Standard libary and settings\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "import itertools\n",
    "import csv\n",
    "import ast\n",
    "from timeit import default_timer as timer\n",
    "global ITERATION\n",
    "import time\n",
    "rundate = time.strftime('%Y%m%d')\n",
    "comp = 'titanic'\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "from IPython.core.display import display, HTML; display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "# Data extensions and settings\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold = np.inf, suppress = True)\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.options.display.float_format = '{:,.6f}'.format\n",
    "\n",
    "# Modeling extensions\n",
    "import sklearn.base as base\n",
    "import sklearn.cluster as cluster\n",
    "import sklearn.datasets as datasets\n",
    "import sklearn.decomposition as decomposition\n",
    "import sklearn.discriminant_analysis as discriminant_analysis\n",
    "import sklearn.ensemble as ensemble\n",
    "import sklearn.feature_extraction as feature_extraction\n",
    "import sklearn.feature_selection as feature_selection\n",
    "import sklearn.gaussian_process as gaussian_process\n",
    "import sklearn.linear_model as linear_model\n",
    "import sklearn.kernel_ridge as kernel_ridge\n",
    "import sklearn.metrics as metrics\n",
    "import sklearn.model_selection as model_selection\n",
    "import sklearn.naive_bayes as naive_bayes\n",
    "import sklearn.neighbors as neighbors\n",
    "import sklearn.pipeline as pipeline\n",
    "import sklearn.preprocessing as preprocessing\n",
    "import sklearn.svm as svm\n",
    "import sklearn.tree as tree\n",
    "import sklearn.utils as utils\n",
    "\n",
    "from scipy import stats, special\n",
    "import xgboost\n",
    "import lightgbm\n",
    "\n",
    "from hyperopt import hp, tpe, Trials, fmin, STATUS_OK\n",
    "from hyperopt.pyll.stochastic import sample\n",
    "\n",
    "# Visualization extensions and settings\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Custom extensions and settings\n",
    "sys.path.append('/main') if '/main' not in sys.path else None\n",
    "# sys.path.append('C:/Users/petersont/Atheneum/dev') if 'C:/Users/petersont/Atheneum/dev' not in sys.path else None\n",
    "sys.path.append('U:\\\\') if 'U:\\\\' not in sys.path else None\n",
    "\n",
    "import mlmachine as mlm\n",
    "import quickplot as qp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T13:48:24.360532Z",
     "start_time": "2019-04-11T13:48:24.277115Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load data and print dimensions\n",
    "dfTrain = pd.read_csv('data/train.csv')\n",
    "dfValid = pd.read_csv('data/test.csv')\n",
    "\n",
    "print('Training data dimensions: {}'.format(dfTrain.shape))\n",
    "print('Validation data dimensions: {}'.format(dfValid.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T13:48:24.430622Z",
     "start_time": "2019-04-11T13:48:24.362584Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display info and first 5 rows\n",
    "\n",
    "dfTrain.info()\n",
    "display(dfTrain[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T13:48:24.444504Z",
     "start_time": "2019-04-11T13:48:24.433904Z"
    }
   },
   "outputs": [],
   "source": [
    "# counts of columns types\n",
    "dfTrain.dtypes.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T13:48:24.509126Z",
     "start_time": "2019-04-11T13:48:24.450385Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load training data into ML machine\n",
    "importlib.reload(mlm)\n",
    "train = mlm.Machine(data = dfTrain\n",
    "                  ,target = ['Survived']\n",
    "                  ,removeFeatures = ['PassengerId','Ticket']                      \n",
    "                  ,overrideCat = ['Pclass','SibSp','Parch']\n",
    "                  ,targetType = 'categorical'\n",
    "                )\n",
    "print(train.X_.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T13:48:24.521486Z",
     "start_time": "2019-04-11T13:48:24.511153Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load training data into ML machine\n",
    "valid = mlm.Machine(data = dfValid\n",
    "                  ,removeFeatures = ['PassengerId','Ticket']                      \n",
    "                  ,overrideCat = ['Pclass','SibSp','Parch']\n",
    "                )\n",
    "print(valid.X_.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Initial-EDA'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical feature EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Categorical-feature-EDA'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate & feature vs. target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Univariate-&-feature-vs.-target'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T13:48:26.656314Z",
     "start_time": "2019-04-11T13:48:24.524146Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Categorical features\n",
    "train.edaCatTargetCatFeat(skipCols = ['Name','Cabin'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous feature EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Continuous-feature-EDA'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate & feature vs. target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Univariate-&-feature-vs.-target2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T13:48:28.468616Z",
     "start_time": "2019-04-11T13:48:26.658749Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Continuous features\n",
    "train.edaCatTargetNumFeat()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Correlation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation (all samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Correlation-all-samples'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T13:48:28.925501Z",
     "start_time": "2019-04-11T13:48:28.472451Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# correlation heat map \n",
    "p = qp.plotter.QuickPlot(fig = plt.figure(), chartProp = 15)\n",
    "ax = p.makeCanvas()\n",
    "p.qpCorrHeatmap(df = train.X_\n",
    "                ,target = train.y_\n",
    "                ,targetLabel = train.target[0]\n",
    "                ,cols = None\n",
    "                ,annot = True\n",
    "                ,ax = ax\n",
    "               )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation (top vs. target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Correlation-top-vs-target'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T13:48:29.180322Z",
     "start_time": "2019-04-11T13:48:28.928177Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# correlation heat map with most highly correlated features relative to the target\n",
    "p = qp.plotter.QuickPlot(fig = plt.figure(), chartProp = 10)\n",
    "ax = p.makeCanvas()\n",
    "p.qpCorrHeatmapRefine(df = train.X_\n",
    "                      ,target = train.y_\n",
    "                      ,targetLabel = train.target[0]\n",
    "                      ,cols = None\n",
    "                      ,annot = True\n",
    "                      ,thresh = 0.2\n",
    "                      ,ax = ax\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Remarks - There are three pairs of highly correlated features:\n",
    "    - 'GarageArea' and 'GarageCars'\n",
    "    - 'TotRmsAbvGrd' and 'GrLivArea'\n",
    "    - '1stFlrSF' and 'TotalBsmtSF\n",
    "This makes sense, given what each feature represents and how each pair items relate to each other. We likely only need one feature from each pair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pair plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Pair-plot'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T13:48:32.432556Z",
     "start_time": "2019-04-11T13:48:29.182251Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Pair plot\n",
    "p = qp.plotter.QuickPlot(fig = plt.figure(), chartProp = 10)\n",
    "p.qpPairPlot(df = train.X_\n",
    "             ,diag_kind = 'auto')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faceting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Faceting'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T13:48:32.697224Z",
     "start_time": "2019-04-11T13:48:32.436085Z"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "p = qp.plotter.QuickPlot(fig = plt.figure(), chartProp = 10)\n",
    "ax = p.makeCanvas(title = '', xLabel = '', yLabel = ''\n",
    "                  ,yShift = 0.8, position = 111)\n",
    "\n",
    "p.qpTwoCatBar(df = train.X_\n",
    "               ,x = 'Pclass'\n",
    "               ,hue = 'Embarked'\n",
    "               ,target = train.y_\n",
    "               ,targetLabel = train.target[0]\n",
    "               ,yUnits = 'p'\n",
    "               ,ax = ax)\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T13:48:34.371000Z",
     "start_time": "2019-04-11T13:48:32.704057Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "p = qp.plotter.QuickPlot(fig = plt.figure(), chartProp = 15)\n",
    "# ax = p.makeCanvas(title = '', xLabel = '', yLabel = ''\n",
    "#                   ,yShift = 0.8, position = 111)\n",
    "\n",
    "p.qpCatNumHistFacet(df = train.X_\n",
    "           ,target = train.y_\n",
    "           ,targetLabel = train.target[0]\n",
    "           ,catRow = 'Sex'\n",
    "           ,catCol = 'Embarked'\n",
    "           ,numCol = 'Age'\n",
    "           ,height = 3\n",
    "           ,aspect = 2\n",
    "           )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T13:48:36.034339Z",
     "start_time": "2019-04-11T13:48:34.378974Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "p = qp.plotter.QuickPlot(fig = plt.figure(), chartProp = 15)\n",
    "# ax = p.makeCanvas(title = '', xLabel = '', yLabel = ''\n",
    "#                   ,yShift = 0.8, position = 111)\n",
    "\n",
    "p.qpCatNumHistFacet(df = train.X_\n",
    "           ,target = train.y_\n",
    "           ,targetLabel = train.target[0]\n",
    "           ,catRow = 'Sex'\n",
    "           ,catCol = 'Pclass'\n",
    "           ,numCol = 'Age'\n",
    "           ,height = 3\n",
    "           ,aspect = 2\n",
    "           )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T13:48:36.973473Z",
     "start_time": "2019-04-11T13:48:36.042477Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p = qp.plotter.QuickPlot(fig = plt.figure(), chartProp = 15)\n",
    "\n",
    "p.qpTwoCatPointFacet(df = train.X_\n",
    "           ,target = train.y_\n",
    "           ,targetLabel = train.target[0]\n",
    "           ,catLine = 'Pclass'\n",
    "           ,catPoint = 'Sex'\n",
    "           ,catGrid = 'Embarked'\n",
    "           ,order = ['female','male'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T13:48:37.944295Z",
     "start_time": "2019-04-11T13:48:36.980746Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p = qp.plotter.QuickPlot(fig = plt.figure(), chartProp = 15)\n",
    "\n",
    "p.qpTwoCatPointFacet(df = train.X_\n",
    "           ,target = train.y_\n",
    "           ,targetLabel = train.target[0]\n",
    "           ,catLine = 'Sex'\n",
    "           ,catPoint = 'Pclass'\n",
    "           ,catGrid = 'Embarked'\n",
    "           ,order = ['female','male'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target variable evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Target-variable-evaluation'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T13:48:37.960825Z",
     "start_time": "2019-04-11T13:48:37.949564Z"
    }
   },
   "outputs": [],
   "source": [
    "# null score\n",
    "pd.Series(train.y_).value_counts(normalize = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Data-cleaning'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers (preliminary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Outliers-preliminary'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Training5'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonNull = train.X_.columns[train.X_.isnull().sum() == 0].values.tolist()\n",
    "nonNullNumCol = list(set(nonNull).intersection(train.featureByDtype_['continuous']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# trainPipe = pipeline.Pipeline([\n",
    "#     ('outlier', train.OutlierIQR(outlierCount = 2, iqrStep = 1.5, features = ['Age','SibSp','Parch','Fare'], dropOutliers = False))     \n",
    "#     ])\n",
    "# train.X_ = trainPipe.transform(train.X_)\n",
    "\n",
    "# np.array(sorted(trainPipe.named_steps['outlier'].outliers_))\n",
    "# # train.y_ = np.delete(train.y_, trainPipe.named_steps['outlier'].outliers_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T13:56:17.735000Z",
     "start_time": "2019-04-11T13:56:17.587238Z"
    }
   },
   "outputs": [],
   "source": [
    "# clf = ensemble.IsolationForest(behaviour = 'new'\n",
    "#                         ,max_samples = train.X_.shape[0]\n",
    "#                         ,random_state = 0\n",
    "#                         ,contamination = 0.02\n",
    "#                         )\n",
    "# clf.fit(train.X_[train.X_.columns.difference(['Embarked','Cabin','Sex','Name'])])\n",
    "# preds = clf.predict(train.X_[train.X_.columns.difference(['Embarked','Cabin','Sex','Name'])])\n",
    "# # np.unique(preds, return_counts = True)\n",
    "\n",
    "# mask = np.isin(preds, -1)  # np.in1d if np.isin is not available\n",
    "# idx = np.where(mask)\n",
    "# idx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import eif as iso\n",
    "# if_eif = iso.iForest(train.X_[train.X_.columns.difference(['Embarked','Cabin','Sex','Name'])].values\n",
    "#                  ,ntrees = 100\n",
    "#                  ,sample_size = 256\n",
    "#                  ,ExtensionLevel = 1\n",
    "#                 )\n",
    "\n",
    "# # calculate anomaly scores\n",
    "# anomalies_ratio = 0.009\n",
    "# anomaly_scores = if_eif.compute_paths(X_in = train.X_[train.X_.columns.difference(['Embarked','Cabin','Sex','Name'])].values)\n",
    "# anomaly_scores_sorted = np.argsort(anomaly_scores)\n",
    "# indices_with_preds = anomaly_scores_sorted[-int(np.ceil(anomalies_ratio * train.X_.shape[0])):]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Validation5'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Missing-data'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Evaluate1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T13:48:38.343929Z",
     "start_time": "2019-04-11T13:48:37.967633Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate missing data\n",
    "train.edaMissingSummary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T13:48:38.736806Z",
     "start_time": "2019-04-11T13:48:38.351096Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate missing data\n",
    "valid.edaMissingSummary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T13:48:38.745965Z",
     "start_time": "2019-04-11T13:48:38.740007Z"
    }
   },
   "outputs": [],
   "source": [
    "# missingdata_df = merged_df.columns[merged_df.isnull().any()].tolist()\n",
    "# msno.matrix(merged_df[missingdata_df])\n",
    "\n",
    "# msno.bar(merged_df[missingdata_df], color=\"blue\", log=True, figsize=(30,18))\n",
    "\n",
    "# # \n",
    "# msno.heatmap(merged_df[missingdata_df], figsize=(20,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T13:48:38.784600Z",
     "start_time": "2019-04-11T13:48:38.750369Z"
    }
   },
   "outputs": [],
   "source": [
    "# compare feature with missing data\n",
    "train.missingColCompare(train.X_, valid.X_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Training1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T13:48:39.087147Z",
     "start_time": "2019-04-11T13:48:38.788240Z"
    }
   },
   "outputs": [],
   "source": [
    "# impute training data\n",
    "trainPipe = pipeline.Pipeline([\n",
    "    ('imputeMedian', train.ContextImputer(nullCol = 'Age', contextCol = 'Parch', strategy = 'median'))     \n",
    "    ,('imputeMode', train.ModeImputer(cols = ['Embarked']))\n",
    "    ])\n",
    "train.X_ = trainPipe.transform(train.X_)\n",
    "train.edaMissingSummary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Validation1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T13:48:39.344678Z",
     "start_time": "2019-04-11T13:48:39.092204Z"
    }
   },
   "outputs": [],
   "source": [
    "# impute validation data\n",
    "validPipe = pipeline.Pipeline([\n",
    "    ('imputeMedian', valid.ContextImputer(nullCol = 'Age', contextCol = 'Parch', train = False, trainDf = trainPipe.named_steps['imputeMedian'].fillDf))\n",
    "    ,('imputeMedian2', valid.NumericalImputer(cols = ['Fare','Age'], strategy = 'median'))    \n",
    "    ])\n",
    "valid.X_ = validPipe.transform(valid.X_)\n",
    "valid.edaMissingSummary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Engineering'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Evaluate3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Training3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T02:30:13.028992Z",
     "start_time": "2019-04-09T02:30:12.805496Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Parse titles to learn gender, and identify rare titles which may convey status\n",
    "title = [i.split(',')[1].split('.')[0].strip() for i in train.X_['Name']]\n",
    "train.X_['Title'] = pd.Series(title)\n",
    "train.X_['Title'] = train.X_['Title'].replace(['Lady','the Countess','Countess','Capt','Col','Don','Dr','Major','Rev','Sir','Jonkheer','Dona']\n",
    "                                            ,'Rare')\n",
    "train.X_['Title'] = train.X_['Title'].map({'Master' : 0, 'Miss' : 1, 'Ms' : 1 , 'Mme' : 1, 'Mlle' : 1, 'Mrs' : 1, 'Mr' : 2, 'Rare' : 3})\n",
    "\n",
    "# Distill cabin feature\n",
    "train.X_['CabinQuarter'] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in train.X_['Cabin']])\n",
    "\n",
    "# Family size features and binning\n",
    "train.X_['FamilySize'] = train.X_['SibSp'] + train.X_['Parch'] + 1\n",
    "\n",
    "customBinDict = {'Age' : [16, 32, 48, 64]\n",
    "                 ,'FamilySize' : [1, 2, 4]\n",
    "          }\n",
    "\n",
    "trainPipe = pipeline.Pipeline([\n",
    "    ('customBin', train.CustomBinner(customBinDict = customBinDict))\n",
    "    ,('percentileBin', train.PercentileBinner(cols = ['Age','Fare'], percs = [25, 50, 75]))    \n",
    "    ])\n",
    "train.X_ = trainPipe.transform(train.X_)\n",
    "\n",
    "# drop features\n",
    "train.featureDropper(cols = ['Name','Cabin'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T02:30:13.039251Z",
     "start_time": "2019-04-09T02:30:13.031177Z"
    }
   },
   "outputs": [],
   "source": [
    "# print new columns\n",
    "for col in train.X_.columns:\n",
    "    if col not in train.featureByDtype_['categorical'] and col not in train.featureByDtype_['continuous']:\n",
    "        print(col)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T02:30:13.073283Z",
     "start_time": "2019-04-09T02:30:13.041818Z"
    }
   },
   "outputs": [],
   "source": [
    "# append new continuous features\n",
    "for col in ['FamilySize']:\n",
    "    train.featureByDtype_['continuous'].append(col)\n",
    "\n",
    "# append new categorical features\n",
    "for col in ['AgeCustomBin','AgePercBin','FarePercBin','FamilySize','FamilySizeCustomBin','Title','CabinQuarter']:\n",
    "    train.featureByDtype_['categorical'].append(col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T02:30:17.461119Z",
     "start_time": "2019-04-09T02:30:13.075439Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate additional features\n",
    "train.edaCatTargetCatFeat()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Validation3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T02:30:17.606263Z",
     "start_time": "2019-04-09T02:30:17.463121Z"
    }
   },
   "outputs": [],
   "source": [
    "# Parse titles to learn gender, and identify rare titles which may convey status\n",
    "title = [i.split(',')[1].split('.')[0].strip() for i in valid.X_['Name']]\n",
    "valid.X_['Title'] = pd.Series(title)\n",
    "valid.X_['Title'] = valid.X_['Title'].replace(['Lady','the Countess','Countess','Capt','Col','Don','Dr','Major','Rev','Sir','Jonkheer','Dona']\n",
    "                                            ,'Rare')\n",
    "valid.X_['Title'] = valid.X_['Title'].map({'Master' : 0, 'Miss' : 1, 'Ms' : 1 , 'Mme' : 1, 'Mlle' : 1, 'Mrs' : 1, 'Mr' : 2, 'Rare' : 3})\n",
    "\n",
    "# Distill cabin feature\n",
    "valid.X_['CabinQuarter'] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in valid.X_['Cabin']])\n",
    "\n",
    "# additional features\n",
    "valid.X_['FamilySize'] = valid.X_['SibSp'] + valid.X_['Parch'] + 1\n",
    "\n",
    "validPipe = pipeline.Pipeline([\n",
    "    ('customBin', valid.CustomBinner(customBinDict = customBinDict))\n",
    "    ,('percentileBin', valid.PercentileBinner(train = False, trainDict = trainPipe.named_steps['percentileBin'].trainDict_))    \n",
    "    ])\n",
    "valid.X_ = validPipe.transform(valid.X_)\n",
    "\n",
    "# drop features\n",
    "valid.featureDropper(cols = ['Name','Cabin'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T02:30:17.615837Z",
     "start_time": "2019-04-09T02:30:17.608439Z"
    }
   },
   "outputs": [],
   "source": [
    "# print new columns\n",
    "for col in valid.X_.columns:\n",
    "    if col not in valid.featureByDtype_['categorical'] and col not in valid.featureByDtype_['continuous']:\n",
    "        print(col)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T02:30:17.623812Z",
     "start_time": "2019-04-09T02:30:17.618971Z"
    }
   },
   "outputs": [],
   "source": [
    "# append new continuous features\n",
    "for col in ['FamilySize']:\n",
    "    valid.featureByDtype_['continuous'].append(col)\n",
    "\n",
    "# append new categorical features\n",
    "for col in ['AgeCustomBin','AgePercBin','FarePercBin','FamilySize','FamilySizeCustomBin','Title','CabinQuarter']:\n",
    "    valid.featureByDtype_['categorical'].append(col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Encoding'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Evaluate2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T02:30:17.692573Z",
     "start_time": "2019-04-09T02:30:17.625893Z"
    }
   },
   "outputs": [],
   "source": [
    "# counts of unique values in training data string columns\n",
    "train.X_[train.featureByDtype_['categorical']].apply(pd.Series.nunique, axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T02:30:17.714489Z",
     "start_time": "2019-04-09T02:30:17.694644Z"
    }
   },
   "outputs": [],
   "source": [
    "# print unique values in each categorical columns\n",
    "for col in train.X_[train.featureByDtype_['categorical']]:\n",
    "    print(col, np.unique(train.X_[col]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T02:30:17.734761Z",
     "start_time": "2019-04-09T02:30:17.717788Z"
    }
   },
   "outputs": [],
   "source": [
    "# counts of unique values in validation data string columns\n",
    "valid.X_[valid.featureByDtype_['categorical']].apply(pd.Series.nunique, axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T02:30:17.764963Z",
     "start_time": "2019-04-09T02:30:17.743951Z"
    }
   },
   "outputs": [],
   "source": [
    "# print unique values in each categorical columns\n",
    "for col in valid.X_[valid.featureByDtype_['categorical']]:\n",
    "    if col not in ['Name','Cabin']:\n",
    "        print(col, np.unique(valid.X_[col]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T02:30:17.788548Z",
     "start_time": "2019-04-09T02:30:17.768466Z"
    }
   },
   "outputs": [],
   "source": [
    "# \n",
    "for col in train.featureByDtype_['categorical']:\n",
    "    if col not in ['Name','Cabin']:\n",
    "        trainValues = train.X_[col].unique()\n",
    "        validValues = valid.X_[col].unique()\n",
    "\n",
    "        trainDiff = set(trainValues) - set(validValues)\n",
    "        validDiff = set(validValues) - set(trainValues)\n",
    "\n",
    "        if len(trainDiff) > 0 or len(validDiff) > 0:\n",
    "            print('\\n\\n*** ' + col)\n",
    "            print('Value present in training data, not in validation data')\n",
    "            print(trainDiff)\n",
    "            print('Value present in validation data, not in training data')\n",
    "            print(validDiff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pclass [1 2 3] - ordinal\n",
    "\n",
    "Sex ['female' 'male'] - nominal\n",
    "\n",
    "Embarked ['C' 'Q' 'S'] - nominal\n",
    "\n",
    "HasCabin [0 1] - nominal\n",
    "\n",
    "Title [0 1 2 3] - nominal\n",
    "\n",
    "CabinQuarter ['A' 'B' 'C' 'D' 'E' 'F' 'G' 'X'] - nominal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Training2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T02:30:17.851133Z",
     "start_time": "2019-04-09T02:30:17.792203Z"
    }
   },
   "outputs": [],
   "source": [
    "### ordinal columns\n",
    "ordCatCols = {\n",
    "    'Pclass' : {1 : 1, 2 : 2, 3 : 3}\n",
    "    }\n",
    "\n",
    "# encode categorical columns\n",
    "nomCatCols = ['Embarked','Sex','CabinQuarter','Title']\n",
    "trainPipe = pipeline.Pipeline([\n",
    "    ('encodeOrdinal', train.CustomOrdinalEncoder(encodings = ordCatCols))    \n",
    "    ,('dummyNominal', train.Dummies(cols = nomCatCols, dropFirst = True))\n",
    "    ])\n",
    "train.X_ = trainPipe.transform(train.X_)\n",
    "train.X_[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Validation2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T02:30:17.898454Z",
     "start_time": "2019-04-09T02:30:17.853309Z"
    }
   },
   "outputs": [],
   "source": [
    "# encode categorical columns\n",
    "validPipe = pipeline.Pipeline([\n",
    "    ('encodeOrdinal', valid.CustomOrdinalEncoder(encodings = ordCatCols))    \n",
    "    ,('dummyNominal', valid.Dummies(cols = nomCatCols, dropFirst = False))\n",
    "    ,('levels', valid.MissingDummies(trainCols = train.X_.columns))    \n",
    "    ])\n",
    "valid.X_ = validPipe.transform(valid.X_)\n",
    "valid.X_[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Transformation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Evaluate4'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T02:30:17.975582Z",
     "start_time": "2019-04-09T02:30:17.901254Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate skew of continuous features - Train\n",
    "train.skewSummary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T02:30:18.053253Z",
     "start_time": "2019-04-09T02:30:17.977952Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate skew of continuous features - Validation\n",
    "valid.skewSummary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Training4'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T02:30:18.665173Z",
     "start_time": "2019-04-09T02:30:18.066287Z"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "trainPipe = pipeline.Pipeline([\n",
    "    ('skew', train.SkewTransform(cols = train.featureByDtype_['continuous'], skewMin = 0.75, pctZeroMax = 1.0))\n",
    "    ])\n",
    "train.X_ = trainPipe.transform(train.X_)\n",
    "train.skewSummary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Validation4'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T02:30:18.775551Z",
     "start_time": "2019-04-09T02:30:18.667847Z"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "validPipe = pipeline.Pipeline([\n",
    "    ('skew', valid.SkewTransform(train = False, trainDict = trainPipe.named_steps['skew'].colValueDict_))\n",
    "    ])\n",
    "valid.X_ = validPipe.transform(valid.X_)\n",
    "valid.skewSummary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers (final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Outliers-final'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Training6'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonNull = train.X_.columns[train.X_.isnull().sum() == 0].values.tolist()\n",
    "nonNullNumCol = list(set(nonNull).intersection(train.featureByDtype_['continuous']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# trainPipe = pipeline.Pipeline([\n",
    "#     ('outlier', train.OutlierIQR(outlierCount = 2, iqrStep = 1.5, features = ['Age','SibSp','Parch','Fare'], dropOutliers = False))     \n",
    "#     ])\n",
    "# train.X_ = trainPipe.transform(train.X_)\n",
    "\n",
    "# np.array(sorted(trainPipe.named_steps['outlier'].outliers_))\n",
    "# # train.y_ = np.delete(train.y_, trainPipe.named_steps['outlier'].outliers_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-11T13:56:17.735000Z",
     "start_time": "2019-04-11T13:56:17.587238Z"
    }
   },
   "outputs": [],
   "source": [
    "# clf = ensemble.IsolationForest(behaviour = 'new'\n",
    "#                         ,max_samples = train.X_.shape[0]\n",
    "#                         ,random_state = 0\n",
    "#                         ,contamination = 0.02\n",
    "#                         )\n",
    "# clf.fit(train.X_[train.X_.columns.difference(['Embarked','Cabin','Sex','Name'])])\n",
    "# preds = clf.predict(train.X_[train.X_.columns.difference(['Embarked','Cabin','Sex','Name'])])\n",
    "# # np.unique(preds, return_counts = True)\n",
    "\n",
    "# mask = np.isin(preds, -1)  # np.in1d if np.isin is not available\n",
    "# idx = np.where(mask)\n",
    "# idx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import eif as iso\n",
    "# if_eif = iso.iForest(train.X_[train.X_.columns.difference(['Embarked','Cabin','Sex','Name'])].values\n",
    "#                  ,ntrees = 100\n",
    "#                  ,sample_size = 256\n",
    "#                  ,ExtensionLevel = 1\n",
    "#                 )\n",
    "\n",
    "# # calculate anomaly scores\n",
    "# anomalies_ratio = 0.009\n",
    "# anomaly_scores = if_eif.compute_paths(X_in = train.X_[train.X_.columns.difference(['Embarked','Cabin','Sex','Name'])].values)\n",
    "# anomaly_scores_sorted = np.argsort(anomaly_scores)\n",
    "# indices_with_preds = anomaly_scores_sorted[-int(np.ceil(anomalies_ratio * train.X_.shape[0])):]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Data evaluation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Feature-importance'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T02:30:18.826463Z",
     "start_time": "2019-04-09T02:30:18.777593Z"
    }
   },
   "outputs": [],
   "source": [
    "# feature importance summary table\n",
    "featureImp = train.featureImportanceSummary()\n",
    "featureImp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rationality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Rationality'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T02:30:19.335162Z",
     "start_time": "2019-04-09T02:30:18.871659Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# percent difference summary\n",
    "dfDiff = abs((((valid.X_.describe() + 1) - (train.X_.describe() + 1)) / (train.X_.describe() + 1)) * 100)\n",
    "dfDiff = dfDiff[dfDiff.columns].replace({0 : np.nan})\n",
    "dfDiff[dfDiff < 0] = np.nan\n",
    "dfDiff = dfDiff.fillna('')\n",
    "display(dfDiff)\n",
    "display(train.X_.describe())\n",
    "display(valid.X_.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value override"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Value override'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T02:30:19.340946Z",
     "start_time": "2019-04-09T02:30:19.337430Z"
    }
   },
   "outputs": [],
   "source": [
    "# change clearly erroneous value to what it probably was\n",
    "# exploreValid.X_['GarageYrBlt'].replace({2207 : 2007}, inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous feature EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Continuous-feature-EDA3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate & feature vs. target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Univariate-&-feature-vs.-target3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T02:30:37.264282Z",
     "start_time": "2019-04-09T02:30:34.875585Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Continuous features\n",
    "train.edaCatTargetNumFeat()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Correlation3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation (top vs. target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Correlation-top-vs-target3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T02:30:38.764481Z",
     "start_time": "2019-04-09T02:30:38.168310Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# correlation heat map with most highly correlated features relative to the target\n",
    "p = qp.plotter.QuickPlot(fig = plt.figure(), chartProp = 15)\n",
    "ax = p.makeCanvas()\n",
    "p.qpCorrHeatmapRefine(df = train.X_\n",
    "                      ,target = train.y_\n",
    "                      ,targetLabel = train.target[0]\n",
    "                      ,cols = None\n",
    "                      ,annot = True\n",
    "                      ,thresh = 0.25\n",
    "                      ,ax = ax\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Modeling'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Prepare-training-data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T02:44:46.274546Z",
     "start_time": "2019-04-10T02:44:45.749087Z"
    }
   },
   "outputs": [],
   "source": [
    "### import training data\n",
    "dfTrain = pd.read_csv('data/train.csv')\n",
    "train = mlm.Machine(data = dfTrain\n",
    "                  ,target = ['Survived']\n",
    "                  ,removeFeatures = ['PassengerId','Ticket']                      \n",
    "                  ,overrideCat = ['Pclass','SibSp','Parch']\n",
    "                  ,targetType = 'categorical'\n",
    "                )\n",
    "\n",
    "### feature engineering\n",
    "# Parse titles to learn gender, and identify rare titles which may convey status\n",
    "title = [i.split(',')[1].split('.')[0].strip() for i in train.X_['Name']]\n",
    "train.X_['Title'] = pd.Series(title)\n",
    "train.X_['Title'] = train.X_['Title'].replace(['Lady','the Countess','Countess','Capt','Col','Don','Dr','Major','Rev','Sir','Jonkheer','Dona']\n",
    "                                            ,'Rare')\n",
    "train.X_['Title'] = train.X_['Title'].map({'Master' : 0, 'Miss' : 1, 'Ms' : 1 , 'Mme' : 1, 'Mlle' : 1, 'Mrs' : 1, 'Mr' : 2, 'Rare' : 3})\n",
    "\n",
    "# Distill cabin feature\n",
    "train.X_['CabinQuarter'] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in train.X_['Cabin']])\n",
    "\n",
    "# Family size features\n",
    "train.X_['FamilySize'] = train.X_['SibSp'] + train.X_['Parch'] + 1\n",
    "\n",
    "# custom bin specifications\n",
    "customBinDict = {'Age' : [16, 32, 48, 64]\n",
    "                 ,'FamilySize' : [1, 2, 4]\n",
    "          }\n",
    "# categorical column specifications\n",
    "ordCatCols = {\n",
    "    'Pclass' : {1 : 1, 2 : 2, 3 : 3}\n",
    "    }\n",
    "nomCatCols = ['Embarked','Sex','CabinQuarter','Title']\n",
    "\n",
    "### pipeline\n",
    "trainPipe = pipeline.Pipeline([\n",
    "    ('imputeMedian', train.ContextImputer(nullCol = 'Age', contextCol = 'Parch', strategy = 'median'))     \n",
    "    ,('imputeMode', train.ModeImputer(cols = ['Embarked']))\n",
    "    ,('customBin', train.CustomBinner(customBinDict = customBinDict))\n",
    "    ,('percentileBin', train.PercentileBinner(cols = ['Age','Fare'], percs = [25, 50, 75]))    \n",
    "    ,('encodeOrdinal', train.CustomOrdinalEncoder(encodings = ordCatCols))    \n",
    "    ,('dummyNominal', train.Dummies(cols = nomCatCols, dropFirst = True))\n",
    "    ,('skew', train.SkewTransform(cols = train.featureByDtype_['continuous'], skewMin = 0.75, pctZeroMax = 1.0))    \n",
    "    ])\n",
    "train.X_ = trainPipe.transform(train.X_)\n",
    "\n",
    "# drop features\n",
    "train.featureDropper(cols = ['Name','Cabin'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Prepare-validation-data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T02:44:58.174509Z",
     "start_time": "2019-04-10T02:44:57.947672Z"
    }
   },
   "outputs": [],
   "source": [
    "### import valid data\n",
    "dfValid = pd.read_csv('data/test.csv')\n",
    "valid = mlm.Machine(data = dfValid\n",
    "                  ,removeFeatures = ['PassengerId','Ticket']                      \n",
    "                  ,overrideCat = ['Pclass','SibSp','Parch']\n",
    "                )\n",
    "\n",
    "### feature engineering\n",
    "# Parse titles to learn gender, and identify rare titles which may convey status\n",
    "title = [i.split(',')[1].split('.')[0].strip() for i in valid.X_['Name']]\n",
    "valid.X_['Title'] = pd.Series(title)\n",
    "valid.X_['Title'] = valid.X_['Title'].replace(['Lady','the Countess','Countess','Capt','Col','Don','Dr','Major','Rev','Sir','Jonkheer','Dona']\n",
    "                                            ,'Rare')\n",
    "valid.X_['Title'] = valid.X_['Title'].map({'Master' : 0, 'Miss' : 1, 'Ms' : 1 , 'Mme' : 1, 'Mlle' : 1, 'Mrs' : 1, 'Mr' : 2, 'Rare' : 3})\n",
    "\n",
    "# Distill cabin feature\n",
    "valid.X_['CabinQuarter'] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in valid.X_['Cabin']])\n",
    "\n",
    "# additional features\n",
    "valid.X_['FamilySize'] = valid.X_['SibSp'] + valid.X_['Parch'] + 1\n",
    "\n",
    "### pipeline\n",
    "validPipe = pipeline.Pipeline([\n",
    "    ('imputeMedian', valid.ContextImputer(nullCol = 'Age', contextCol = 'Parch', train = False, trainDf = trainPipe.named_steps['imputeMedian'].fillDf))\n",
    "    ,('imputeMedian2', valid.NumericalImputer(cols = ['Fare','Age'], strategy = 'median'))    \n",
    "    ,('customBin', valid.CustomBinner(customBinDict = customBinDict))\n",
    "    ,('percentileBin', valid.PercentileBinner(train = False, trainDict = trainPipe.named_steps['percentileBin'].trainDict_))   \n",
    "    ,('encodeOrdinal', valid.CustomOrdinalEncoder(encodings = ordCatCols))    \n",
    "    ,('dummyNominal', valid.Dummies(cols = nomCatCols, dropFirst = False))\n",
    "    ,('levels', valid.MissingDummies(trainCols = train.X_.columns))    \n",
    "    ,('skew', valid.SkewTransform(train = False, trainDict = trainPipe.named_steps['skew'].colValueDict_))    \n",
    "    ])\n",
    "valid.X_ = validPipe.transform(valid.X_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'GridSearch'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# set variables\n",
    "scoring = 'accuracy'\n",
    "n_folds = 2\n",
    "n_jobs = 16\n",
    "verbose = 2\n",
    "ITERATION = 0\n",
    "iters = 1\n",
    "\n",
    "\n",
    "# set optimization parameters\n",
    "def objective(space, model = '', X = train.X_, y = train.y_, scoring = scoring, n_folds = n_folds, n_jobs = n_jobs, verbose = verbose, invertLoss = True):\n",
    "    global ITERATION\n",
    "    ITERATION += 1\n",
    "    start = timer()\n",
    "    \n",
    "    # convert select float params to int\n",
    "    for param in ['num_leaves', 'subsample_for_bin', 'min_child_samples']:\n",
    "        if param in space.keys():\n",
    "            space[param] = int(space[param])\n",
    "    \n",
    "    print(space)\n",
    "    \n",
    "    cv = model_selection.cross_val_score(estimator = eval('{0}(**{1})'.format(model, space))\n",
    "                                         ,X = X\n",
    "                                         ,y = y\n",
    "                                         ,verbose = verbose\n",
    "                                         ,n_jobs = n_jobs\n",
    "                                         ,cv = n_folds\n",
    "                                         ,scoring = scoring)\n",
    "    run_time = timer() - start\n",
    "    \n",
    "    # Extract the best score\n",
    "    if invertLoss:\n",
    "        loss = 1 - cv.mean()\n",
    "    else:\n",
    "        loss = cv.mean()\n",
    "    \n",
    "    # export results to CSV\n",
    "    out_file = 'data/{}_{}.csv'.format(rundate, comp)\n",
    "    with open(out_file, 'a', newline = '') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([ITERATION, model, space, loss, cv.min(), cv.mean(), cv.max(), cv.std(), run_time, STATUS_OK]) \n",
    "\n",
    "    return {'iteration' : ITERATION\n",
    "            ,'estimator' : model\n",
    "            ,'params' : space\n",
    "            ,'loss' : loss                   # required\n",
    "            ,'min' : cv.min()\n",
    "            ,'mean' : cv.mean()\n",
    "            ,'max' : cv.max()\n",
    "            ,'std' : cv.std()\n",
    "            ,'train_time' : run_time\n",
    "            ,'status' : STATUS_OK}          # required\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " # parameter space\n",
    "allSpace = {\n",
    "#             'lightgbm.LGBMClassifier' : {\n",
    "#                 'class_weight' : hp.choice('class_weight', [None, 'balanced'])\n",
    "#                 ,'colsample_bytree': hp.quniform('colsample_bytree', 0.5, 1.0, 0.05)\n",
    "#                 ,'boosting_type' : hp.choice('boosting_type', ['gbdt', 'dart', 'goss'])                \n",
    "#                 #,'boosting_type': hp.choice('boosting_type'\n",
    "#                 #                    ,[{'boosting_type': 'gbdt', 'subsample': hp.uniform('gdbt_subsample', 0.5, 1)}\n",
    "#                 #                    ,{'boosting_type': 'dart', 'subsample': hp.uniform('dart_subsample', 0.5, 1)}\n",
    "#                 #                    ,{'boosting_type': 'goss', 'subsample': 1.0}])\n",
    "#                 ,'learning_rate' : hp.quniform('learning_rate', 0.01, 0.2, 0.01)\n",
    "#                 ,'max_depth' : hp.choice('max_depth', np.arange(2, 20, dtype = int))\n",
    "#                 ,'min_child_samples' : hp.quniform('min_child_samples', 20, 500, 5)\n",
    "#                 ,'n_estimators' : hp.choice('n_estimators', np.arange(100, 10000, 10, dtype = int))\n",
    "#                 ,'num_leaves': hp.quniform('num_leaves', 8, 150, 1)\n",
    "#                 ,'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0)\n",
    "#                 ,'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0)\n",
    "#                 ,'subsample_for_bin': hp.quniform('subsample_for_bin', 20000, 300000, 20000)                    \n",
    "#             }\n",
    "#             ,'linear_model.LogisticRegression' : {\n",
    "#                 'C': hp.loguniform('C', np.log(0.001), np.log(0.2))\n",
    "#                 ,'penalty': hp.choice('penalty', ['l1', 'l2'])\n",
    "#                 #,'solver': hp.choice('solver', ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'])\n",
    "#             }\n",
    "#             ,'xgboost.XGBClassifier' : {\n",
    "#                 'colsample_bytree' : hp.quniform('colsample_bytree', 0.5, 1.0, 0.05)\n",
    "#                 ,'gamma' : hp.quniform('gamma', 0.0, 10, 0.05)\n",
    "#                 ,'learning_rate' : hp.quniform('learning_rate', 0.01, 0.2, 0.01)\n",
    "#                 ,'max_depth' : hp.choice('max_depth', np.arange(2, 20, dtype = int))\n",
    "#                 ,'min_child_weight': hp.quniform ('min_child_weight', 1, 20, 1)\n",
    "#                 ,'n_estimators' : hp.choice('n_estimators', np.arange(100, 10000, 10, dtype = int))\n",
    "#                 #,'objective' : hp.choice('objective', ['binary:logistic'])\n",
    "#                 ,'subsample': hp.uniform ('subsample', 0.5, 1)\n",
    "#             }\n",
    "#             ,'ensemble.RandomForestClassifier' : {\n",
    "#                 'bootstrap' : hp.choice('bootstrap', [True, False])\n",
    "#                 ,'max_depth' : hp.choice('max_depth', np.arange(2, 20, dtype = int))\n",
    "#                 ,'n_estimators' : hp.choice('n_estimators', np.arange(100, 10000, 10, dtype = int))\n",
    "#                 ,'max_features' : hp.choice('max_features', ['auto','sqrt'])\n",
    "#                 ,'min_samples_split' : hp.choice('min_samples_split', np.arange(2, 40, dtype = int))\n",
    "#                 ,'min_samples_leaf' : hp.choice('min_samples_leaf', np.arange(2, 40, dtype = int))\n",
    "#             }\n",
    "#             ,'ensemble.GradientBoostingClassifier' : {\n",
    "#                 'n_estimators' : hp.choice('n_estimators', np.arange(100, 10000, 10, dtype = int))\n",
    "#                 ,'max_depth' : hp.choice('max_depth', np.arange(2, 20, dtype = int))\n",
    "#                 ,'max_features' : hp.choice('max_features', ['auto','sqrt'])    \n",
    "#                 ,'learning_rate' : hp.quniform('learning_rate', 0.01, 0.2, 0.01)\n",
    "#                 ,'loss' : hp.choice('loss', ['deviance','exponential'])    \n",
    "#                 ,'min_samples_split' : hp.choice('min_samples_split', np.arange(2, 40, dtype = int))\n",
    "#                 ,'min_samples_leaf' : hp.choice('min_samples_leaf', np.arange(2, 40, dtype = int))\n",
    "#             }\n",
    "            'ensemble.AdaBoostClassifier' : {\n",
    "                'n_estimators' : hp.choice('n_estimators', np.arange(100, 10000, 10, dtype = int))\n",
    "                ,'learning_rate' : hp.quniform('learning_rate', 0.01, 0.2, 0.01)\n",
    "                ,'algorithm' : hp.choice('algorithm', ['SAMME', 'SAMME.R'])                    \n",
    "            }\n",
    "            ,'naive_bayes.BernoulliNB' : {\n",
    "                'alpha' :  hp.uniform('alpha', 0.01, 10)\n",
    "            }\n",
    "            ,'naive_bayes.GaussianNB' : {\n",
    "                'var_smoothing' :  hp.uniform('var_smoothing', 0.01, 1e9)\n",
    "            }\n",
    "            ,'ensemble.BaggingClassifier' : {\n",
    "                'n_estimators' : hp.choice('n_estimators', np.arange(100, 10000, 10, dtype = int))\n",
    "                ,'max_samples' : hp.uniform('max_samples', 0.2, 1)                    \n",
    "            }\n",
    "            ,'ensemble.ExtraTreesClassifier' : {\n",
    "                'n_estimators' : hp.choice('n_estimators', np.arange(100, 10000, 10, dtype = int))\n",
    "                ,'max_depth' : hp.choice('max_depth', np.arange(2, 20, dtype = int))\n",
    "                ,'min_samples_split' : hp.choice('min_samples_split', np.arange(2, 40, dtype = int))\n",
    "                ,'min_samples_leaf' : hp.choice('min_samples_leaf', np.arange(2, 40, dtype = int))\n",
    "                ,'max_features' : hp.choice('max_features', ['auto','sqrt'])\n",
    "                ,'criterion' : hp.choice('criterion', ['gini','entropy'])\n",
    "            }\n",
    "            ,'gaussian_process.GaussianProcessClassifier' : {\n",
    "                'max_iter_predict' : hp.choice('max_iter_predict', np.arange(50, 500, dtype = int))\n",
    "            }\n",
    "            ,'svm.SVC' : {\n",
    "                'C' : hp.quniform('C', 0.0, 10, 0.05)\n",
    "                ,'decision_function_shape' : hp.choice('decision_function_shape', ['ovo','ovr'])\n",
    "                ,'gamma' : hp.quniform('gamma', 0.0, 10, 0.05)                \n",
    "            }\n",
    "            ,'neighbors.KNeighborsClassifier' : {\n",
    "                'algorithm' : hp.choice('algorithm', ['auto','ball_tree','kd_tree','brute'])\n",
    "                ,'n_neighbors' : hp.choice('n_neighbors', np.arange(1, 20, dtype = int))\n",
    "                ,'weights' : hp.choice('weights', ['distance','uniform'])\n",
    "            }\n",
    "}\n",
    "\n",
    "# set variables\n",
    "scoring = 'accuracy'\n",
    "n_folds = 8\n",
    "n_jobs = 8\n",
    "verbose = 0\n",
    "ITERATION = 0\n",
    "iters = 3000\n",
    "\n",
    "# create shell file\n",
    "out_file = 'data/{}_{}.csv'.format(rundate, comp)\n",
    "with open(out_file, 'w', newline = '') as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    writer.writerow(['iteration','estimator','params','loss','min','mean','max','std','train_time','status'])\n",
    "\n",
    "for model in allSpace.keys():\n",
    "    space = allSpace[model]\n",
    "    \n",
    "    # reset objective function defaults\n",
    "    objective.__defaults__ = (model, train.X_, train.y_, scoring, n_folds, n_jobs, verbose, True)\n",
    "    \n",
    "    # Run optimization\n",
    "    print('#'*100)\n",
    "    print('\\nTuning {0}\\n'.format(model))\n",
    "    best = fmin(fn = objective\n",
    "                ,space = space\n",
    "                ,algo = tpe.suggest\n",
    "                ,max_evals = iters\n",
    "                ,trials = Trials()\n",
    "                ,verbose = 1\n",
    "               )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read scores summary table\n",
    "scoreDf = pd.read_csv('data/{}_{}.csv'.format(rundate, comp), index_col = 0, na_values = 'nan')\n",
    "scoreDf[:20]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/WillKoehrsen/hyperparameter-optimization/blob/master/Bayesian%20Hyperparameter%20Optimization%20of%20Gradient%20Boosting%20Machine.ipynb\n",
    "\n",
    "https://github.com/hyperopt/hyperopt/issues/392\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Evaluation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model explanability\n",
    "\n",
    "https://www.kaggle.com/learn/machine-learning-explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Feature-importance'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Permutation importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Permutation-importance'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T13:55:21.322458Z",
     "start_time": "2019-04-02T13:55:21.259Z"
    }
   },
   "outputs": [],
   "source": [
    "# permutation importance\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "perm = PermutationImportance(my_model, random_state=1).fit(val_X, val_y)\n",
    "eli5.show_weights(perm, feature_names = val_X.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partial plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Partial-plots'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T13:55:21.326597Z",
     "start_time": "2019-04-02T13:55:21.267Z"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "from matplotlib import pyplot as plt\n",
    "from pdpbox import pdp, get_dataset, info_plots\n",
    "\n",
    "# Create the data that we will plot\n",
    "pdp_goals = pdp.pdp_isolate(model=tree_model, dataset=val_X, model_features=feature_names, feature='Goal Scored')\n",
    "\n",
    "# plot it\n",
    "pdp.pdp_plot(pdp_goals, 'Goal Scored')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T13:55:21.334512Z",
     "start_time": "2019-04-02T13:55:21.275Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_to_plot = 'Distance Covered (Kms)'\n",
    "pdp_dist = pdp.pdp_isolate(model=tree_model, dataset=val_X, model_features=feature_names, feature=feature_to_plot)\n",
    "\n",
    "pdp.pdp_plot(pdp_dist, feature_to_plot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T13:55:21.336354Z",
     "start_time": "2019-04-02T13:55:21.281Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build Random Forest model\n",
    "rf_model = RandomForestClassifier(random_state=0).fit(train_X, train_y)\n",
    "\n",
    "pdp_dist = pdp.pdp_isolate(model=rf_model, dataset=val_X, model_features=feature_names, feature=feature_to_plot)\n",
    "\n",
    "pdp.pdp_plot(pdp_dist, feature_to_plot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T13:55:21.454582Z",
     "start_time": "2019-04-02T13:55:21.438137Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2D plots\n",
    "# Similar to previous PDP plot except we use pdp_interact instead of pdp_isolate and pdp_interact_plot instead of pdp_isolate_plot\n",
    "features_to_plot = ['Goal Scored', 'Distance Covered (Kms)']\n",
    "inter1  =  pdp.pdp_interact(model=tree_model, dataset=val_X, model_features=feature_names, features=features_to_plot)\n",
    "\n",
    "pdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=features_to_plot, plot_type='contour')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SHAP values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'SHAP-values'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T13:55:21.457225Z",
     "start_time": "2019-04-02T13:55:21.302Z"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "row_to_show = 5\n",
    "data_for_prediction = val_X.iloc[row_to_show]  # use 1 row of data here. Could use multiple rows if desired\n",
    "data_for_prediction_array = data_for_prediction.values.reshape(1, -1)\n",
    "\n",
    "\n",
    "my_model.predict_proba(data_for_prediction_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T13:55:21.458460Z",
     "start_time": "2019-04-02T13:55:21.309Z"
    }
   },
   "outputs": [],
   "source": [
    "import shap  # package used to calculate Shap values\n",
    "\n",
    "# Create object that can calculate shap values\n",
    "explainer = shap.TreeExplainer(my_model)\n",
    "\n",
    "# Calculate Shap values\n",
    "shap_values = explainer.shap_values(data_for_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T13:55:21.459772Z",
     "start_time": "2019-04-02T13:55:21.315Z"
    }
   },
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T13:55:21.461568Z",
     "start_time": "2019-04-02T13:55:21.322Z"
    }
   },
   "outputs": [],
   "source": [
    "# use Kernel SHAP to explain test set predictions\n",
    "k_explainer = shap.KernelExplainer(my_model.predict_proba, train_X)\n",
    "k_shap_values = k_explainer.shap_values(data_for_prediction)\n",
    "shap.force_plot(k_explainer.expected_value[1], k_shap_values[1], data_for_prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T13:55:21.463070Z",
     "start_time": "2019-04-02T13:55:21.329Z"
    }
   },
   "outputs": [],
   "source": [
    "shap.DeepExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T13:55:21.464257Z",
     "start_time": "2019-04-02T13:55:21.335Z"
    }
   },
   "outputs": [],
   "source": [
    "import shap  # package used to calculate Shap values\n",
    "\n",
    "# Create object that can calculate shap values\n",
    "explainer = shap.TreeExplainer(my_model)\n",
    "\n",
    "# calculate shap values. This is what we will plot.\n",
    "# Calculate shap_values for all of val_X rather than a single row, to have more data for plot.\n",
    "shap_values = explainer.shap_values(val_X)\n",
    "\n",
    "# Make plot. Index of [1] is explained in text below.\n",
    "shap.summary_plot(shap_values[1], val_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T13:55:21.465491Z",
     "start_time": "2019-04-02T13:55:21.341Z"
    }
   },
   "outputs": [],
   "source": [
    "import shap  # package used to calculate Shap values\n",
    "\n",
    "# Create object that can calculate shap values\n",
    "explainer = shap.TreeExplainer(my_model)\n",
    "\n",
    "# calculate shap values. This is what we will plot.\n",
    "shap_values = explainer.shap_values(X)\n",
    "\n",
    "# make plot.\n",
    "shap.dependence_plot('Ball Possession %', shap_values[1], X, interaction_index=\"Goal Scored\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Stacking'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primary models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Primary-models'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T03:10:24.616842Z",
     "start_time": "2019-04-10T03:10:23.808455Z"
    }
   },
   "outputs": [],
   "source": [
    "scoreDf = pd.read_csv('data/cvresults_20190407.csv', index_col = 0)\n",
    "scoreDf[:50]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T03:29:08.434252Z",
     "start_time": "2019-04-10T03:29:08.366980Z"
    }
   },
   "outputs": [],
   "source": [
    "top10s = scoreDf[scoreDf['estimator'] == 'XGBoost'].index[:10].tolist() +\\\n",
    "scoreDf[scoreDf['estimator'] == 'LGBM'].index[:10].tolist() +\\\n",
    "scoreDf[scoreDf['estimator'] == 'RandomForestClassifier'].index[:10].tolist() +\\\n",
    "scoreDf[scoreDf['estimator'] == 'GradientBoostingClassifier'].index[:10].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T03:32:45.259794Z",
     "start_time": "2019-04-10T03:29:09.248558Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "del oofValid; del oofTrain\n",
    "# modelIxs = [109552, 194576, 25588, 60775]\n",
    "modelIxs = top10s\n",
    "columns = []\n",
    "\n",
    "for ix in modelIxs:\n",
    "    estimator, params = train.modelParamBuilder(scoreDf = scoreDf, modelIx = ix)\n",
    "    \n",
    "    columns.append(estimator + '_' + str(ix))\n",
    "    \n",
    "    if estimator == 'XGBoost':\n",
    "        model = train.SklearnHelper(clf = xgboost.XGBClassifier, params = params)\n",
    "    elif estimator == 'LGBM':\n",
    "        model = train.SklearnHelper(clf = lightgbm.LGBMClassifier, params = params)\n",
    "    elif estimator == 'GradientBoostingClassifier':\n",
    "        model = train.SklearnHelper(clf = ensemble.GradientBoostingClassifier, params = params)\n",
    "    elif estimator == 'RandomForestClassifier':\n",
    "        model = train.SklearnHelper(clf = ensemble.RandomForestClassifier, params = params)\n",
    "    #elif estimator == '':\n",
    "    #     model = train.SklearnHelper(clf = , params = params)\n",
    "    \n",
    "    oofTrainModel, oofValidModel = train.oofGenerator(clf = model, x_train = train.X_.values, y_train = train.y_, x_valid = valid.X_.values)\n",
    "    \n",
    "    try:\n",
    "        oofTrain = np.hstack((oofTrain, oofTrainModel))\n",
    "        oofValid = np.hstack((oofValid, oofValidModel))        \n",
    "    except NameError:\n",
    "        oofTrain = oofTrainModel\n",
    "        oofValid = oofValidModel\n",
    "        print(\"not defined\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T03:33:10.326911Z",
     "start_time": "2019-04-10T03:33:09.112684Z"
    }
   },
   "outputs": [],
   "source": [
    "# view correlations of predictions\n",
    "sns.set_style('whitegrid')\n",
    "p = qp.plotter.QuickPlot(fig = plt.figure(), chartProp = 15)\n",
    "ax = p.makeCanvas(position = 111)\n",
    "p.qpCorrHeatmap(df = pd.DataFrame(oofTrain, columns = columns)\n",
    "                ,annot = False\n",
    "                ,ax = ax\n",
    "               )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T13:55:21.470721Z",
     "start_time": "2019-04-02T13:55:21.373Z"
    }
   },
   "outputs": [],
   "source": [
    "cols = train.X_.columns.values.tolist()\n",
    "df = pd.DataFrame({'features' : cols\n",
    "                    ,'XGBoost' : topXGBoost.feature_importances(train.X_.values, train.y_)\n",
    "                    ,'GBC' : topGBR.feature_importances(train.X_.values, train.y_)\n",
    "                    ,'LGBM' : topLGBM.feature_importances(train.X_.values, train.y_)\n",
    "#                     ,'LogReg' : topLogReg.feature_importances(train.X_.values, train.y_)                   \n",
    "                  })\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Meta-model'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T04:22:40.476582Z",
     "start_time": "2019-04-10T03:34:43.112456Z"
    }
   },
   "outputs": [],
   "source": [
    "# model dictionary\n",
    "models = {\n",
    "    'XGBoost' : xgboost.XGBClassifier()\n",
    "}\n",
    "\n",
    "# parameter grids\n",
    "params = {\n",
    "        'XGBoost' : {\n",
    "            'colsample_bytree' : np.arange(0.1, 1.0, 0.1)\n",
    "            ,'gamma' : np.arange(0.01, .1, 0.01)\n",
    "            ,'learning_rate' : np.arange(0.01, .1, 0.01)\n",
    "            ,'max_depth' : np.arange(2, 6)\n",
    "            ,'min_child_weight' : np.arange(0.5, 2.5, 0.25)\n",
    "            ,'n_estimators' : np.arange(800, 2000, 100) \n",
    "            ,'reg_alpha' : np.arange(0.1, 1.0, 0.1)\n",
    "            ,'reg_lambda' : np.arange(0.1, 1.0, 0.1)\n",
    "            ,'subsample' : np.arange(0.1, 1.0, 0.1)\n",
    "        }\n",
    "}\n",
    "\n",
    "# execute cross-validation grid search for model / parameter grid pairs\n",
    "helper = train.PowerGridSearcher(models, params)\n",
    "gridSearch = helper.fitRgs(oofTrain\n",
    "                ,train.y_\n",
    "                ,n_iter = 1000\n",
    "                ,cv = 8\n",
    "                ,n_jobs = 2\n",
    "                ,verbose = 0\n",
    "                ,scoring = 'accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T12:40:11.615603Z",
     "start_time": "2019-04-10T12:40:11.099825Z"
    }
   },
   "outputs": [],
   "source": [
    "# review summary\n",
    "secondLevelScoreDf = helper.scoreSummary()\n",
    "secondLevelScoreDf[:10]\n",
    "# scores.fillna('')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Submission'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Standard'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T02:40:30.310100Z",
     "start_time": "2019-04-09T02:40:29.488774Z"
    }
   },
   "outputs": [],
   "source": [
    "# standard model fit and predict\n",
    "params = train.modelParamBuilder(scoreDf = scoreDf, modelIx = 126913)\n",
    "\n",
    "# model = linear_model.LogisticRegression(penalty = 'l1', **params)\n",
    "# model = linear_model.LogisticRegression(penalty = 'l2', **params)\n",
    "\n",
    "model = xgboost.XGBClassifier(**params)\n",
    "# model = lightgbm.LGBMClassifier(**params)\n",
    "# model = ensemble.GradientBoostingClassifier(**params)\n",
    "# model = ensemble.RandomForestClassifier(**params)\n",
    "\n",
    "\n",
    "model.fit(train.X_, train.y_)\n",
    "yPred = model.predict(valid.X_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T02:40:33.504975Z",
     "start_time": "2019-04-09T02:40:33.468336Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate prediction submission file\n",
    "my_submission = pd.DataFrame({'PassengerId': dfValid.PassengerId, 'Survived': yPred})\n",
    "my_submission.to_csv('data/submission.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Stack'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T12:41:50.469334Z",
     "start_time": "2019-04-10T12:41:50.146429Z"
    }
   },
   "outputs": [],
   "source": [
    "# best second level learning model\n",
    "estimator, params = train.modelParamBuilder(scoreDf = secondLevelScoreDf, modelIx = 279)\n",
    "model = xgboost.XGBClassifier(**params)\n",
    "\n",
    "model.fit(oofTrain, train.y_)\n",
    "yPred = model.predict(oofValid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T12:41:52.388467Z",
     "start_time": "2019-04-10T12:41:52.361208Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate prediction submission file\n",
    "my_submission = pd.DataFrame({'PassengerId': dfValid.PassengerId, 'Survived': yPred})\n",
    "my_submission.to_csv('data/submission.csv', index = False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
