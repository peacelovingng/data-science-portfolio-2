{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Kaggle competition - house prices__\n",
    "\n",
    "1. [Kaggle competition - house prices](#Kaggle-competition-house-prices)\n",
    "1. [Import](#Import)\n",
    "    1. [Tools](#Tools)\n",
    "    1. [Data](#Data)    \n",
    "1. [Initial EDA](#Initial-EDA)\n",
    "    1. [Categorical feature EDA](#Categorical-feature-EDA)\n",
    "        1. [Univariate & feature vs. target](#Univariate-&-feature-vs.-target)\n",
    "    1. [Continuous feature EDA](#Continuous-feature-EDA)\n",
    "        1. [Univariate & feature vs. target](#Univariate-&-feature-vs.-target2)\n",
    "        1. [Correlation](#Correlation)\n",
    "            1. [Correlation (all samples)](#Correlation-all-samples)\n",
    "            1. [Correlation (top vs. target)](#Correlation-top-vs-target)\n",
    "        1. [Pair plot](#Pair-plot)\n",
    "    1. [Faceting](#Faceting)\n",
    "    1. [Target variable evaluation](#Target-variable-evaluation)    \n",
    "1. [Data cleaning](#Data-cleaning)\n",
    "    1. [Outliers (preliminary)](#Outliers-preliminary)\n",
    "        1. [Training](#Training5)\n",
    "        1. [Validation](#Validation5)\n",
    "    1. [Missing data](#Missing-data)\n",
    "        1. [Evaluate](#Evaluate1)\n",
    "        1. [Training](#Training1)\n",
    "        1. [Validation](#Validation1)\n",
    "    1. [Engineering](#Engineering)\n",
    "        1. [Evaluate](#Evaluate3)\n",
    "        1. [Training](#Training3)\n",
    "        1. [Validation](#Validation3)\n",
    "    1. [Encoding](#Encoding)\n",
    "        1. [Evaluate](#Evaluate2)\n",
    "        1. [Training](#Training2)\n",
    "        1. [Validation](#Validation2)\n",
    "    1. [Transformation](#Transformation)\n",
    "        1. [Evaluate](#Evaluate4)\n",
    "        1. [Training](#Training4)\n",
    "        1. [Validation](#Validation4)\n",
    "    1. [Outliers (final)](#Outliers-final)\n",
    "        1. [Training](#Training6)\n",
    "1. [Data evaluation](#Data-evaluation)\n",
    "    1. [Feature importance](#Feature-importance)    \n",
    "    1. [Rationality](#Rationality)\n",
    "    1. [Value override](#Value-override)\n",
    "    1. [Continuous feature EDA](#Continuous-feature-EDA3)\n",
    "        1. [Univariate & feature vs. target](#Univariate-&-feature-vs.-target3)\n",
    "        1. [Correlation](#Correlation3)\n",
    "            1. [Correlation (top vs. target)](#Correlation-top-vs-target3)\n",
    "1. [Modeling](#Modeling)\n",
    "    1. [Prepare training data](#Prepare-training-data)\n",
    "    1. [Prepare validation data](#Prepare-validation-data)\n",
    "    1. [GridSearch](#GridSearch)\n",
    "        1. [Evaluation](#Evaluation)\n",
    "        1. [Model explanability](#Model-explanability)\n",
    "            1. [Permutation importance](#Permutation-importance)\n",
    "            1. [Partial plots](#Partial-plots)\n",
    "            1. [SHAP values](#SHAP-values)\n",
    "    1. [Stacking](#Stacking)\n",
    "        1. [Primary models](#Primary-models)\n",
    "        1. [Meta model](#Meta-model)                \n",
    "1. [Submission](#Submission)\n",
    "    1. [Stack](#Stack)\n",
    "    1. [Standard](#Standard)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle competition - Titanic\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Kaggle-competition-house-prices'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Import'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Tools'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T04:22:23.532795Z",
     "start_time": "2019-04-21T04:22:22.020754Z"
    }
   },
   "outputs": [],
   "source": [
    "# Standard libary and settings\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "import itertools\n",
    "import csv\n",
    "import ast\n",
    "from timeit import default_timer as timer\n",
    "global ITERATION\n",
    "import time\n",
    "rundate = time.strftime('%Y%m%d')\n",
    "comp = 'titanic'\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "from IPython.core.display import display, HTML; display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "# Data extensions and settings\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold = np.inf, suppress = True)\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.options.display.float_format = '{:,.6f}'.format\n",
    "\n",
    "# Modeling extensions\n",
    "import sklearn.base as base\n",
    "import sklearn.cluster as cluster\n",
    "import sklearn.datasets as datasets\n",
    "import sklearn.decomposition as decomposition\n",
    "import sklearn.discriminant_analysis as discriminant_analysis\n",
    "import sklearn.ensemble as ensemble\n",
    "import sklearn.feature_extraction as feature_extraction\n",
    "import sklearn.feature_selection as feature_selection\n",
    "import sklearn.gaussian_process as gaussian_process\n",
    "import sklearn.linear_model as linear_model\n",
    "import sklearn.kernel_ridge as kernel_ridge\n",
    "import sklearn.metrics as metrics\n",
    "import sklearn.model_selection as model_selection\n",
    "import sklearn.naive_bayes as naive_bayes\n",
    "import sklearn.neighbors as neighbors\n",
    "import sklearn.pipeline as pipeline\n",
    "import sklearn.preprocessing as preprocessing\n",
    "import sklearn.svm as svm\n",
    "import sklearn.tree as tree\n",
    "import sklearn.utils as utils\n",
    "\n",
    "from scipy import stats, special\n",
    "import xgboost\n",
    "import lightgbm\n",
    "import catboost\n",
    "\n",
    "from hyperopt import hp, tpe, Trials, fmin, STATUS_OK\n",
    "from hyperopt.pyll.stochastic import sample\n",
    "\n",
    "# Visualization extensions and settings\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Custom extensions and settings\n",
    "sys.path.append('/main') if '/main' not in sys.path else None\n",
    "# sys.path.append('C:/Users/petersont/Atheneum/dev') if 'C:/Users/petersont/Atheneum/dev' not in sys.path else None\n",
    "sys.path.append('U:\\\\') if 'U:\\\\' not in sys.path else None\n",
    "\n",
    "import mlmachine as mlm\n",
    "import quickplot as qp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T04:22:23.607027Z",
     "start_time": "2019-04-21T04:22:23.537993Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load data and print dimensions\n",
    "dfTrain = pd.read_csv('data/train.csv')\n",
    "dfValid = pd.read_csv('data/test.csv')\n",
    "\n",
    "print('Training data dimensions: {}'.format(dfTrain.shape))\n",
    "print('Validation data dimensions: {}'.format(dfValid.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T04:22:23.702586Z",
     "start_time": "2019-04-21T04:22:23.610699Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display info and first 5 rows\n",
    "\n",
    "dfTrain.info()\n",
    "display(dfTrain[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T04:22:23.713100Z",
     "start_time": "2019-04-21T04:22:23.705808Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# counts of columns types\n",
    "dfTrain.dtypes.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T04:22:23.790858Z",
     "start_time": "2019-04-21T04:22:23.716242Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load training data into ML machine\n",
    "importlib.reload(mlm)\n",
    "train = mlm.Machine(data = dfTrain\n",
    "                  ,target = ['Survived']\n",
    "                  ,removeFeatures = ['PassengerId','Ticket']                      \n",
    "                  ,overrideCat = ['Pclass','SibSp','Parch']\n",
    "                  ,targetType = 'categorical'\n",
    "                )\n",
    "print(train.X_.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T04:22:23.805251Z",
     "start_time": "2019-04-21T04:22:23.793079Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load training data into ML machine\n",
    "valid = mlm.Machine(data = dfValid\n",
    "                  ,removeFeatures = ['PassengerId','Ticket']                      \n",
    "                  ,overrideCat = ['Pclass','SibSp','Parch']\n",
    "                )\n",
    "print(valid.X_.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Initial EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Initial-EDA'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Categorical feature EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Categorical-feature-EDA'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Univariate & feature vs. target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Univariate-&-feature-vs.-target'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T04:22:25.672089Z",
     "start_time": "2019-04-21T04:22:23.806885Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Categorical features\n",
    "train.edaCatTargetCatFeat(skipCols = ['Name','Cabin'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Continuous feature EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Continuous-feature-EDA'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Univariate & feature vs. target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Univariate-&-feature-vs.-target2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T04:22:27.364417Z",
     "start_time": "2019-04-21T04:22:25.674434Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Continuous features\n",
    "train.edaCatTargetNumFeat()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Correlation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Correlation (all samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Correlation-all-samples'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T04:22:27.762170Z",
     "start_time": "2019-04-21T04:22:27.367854Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# correlation heat map \n",
    "p = qp.plotter.QuickPlot(fig = plt.figure(), chartProp = 15)\n",
    "ax = p.makeCanvas()\n",
    "p.qpCorrHeatmap(df = train.X_\n",
    "                ,target = train.y_\n",
    "                ,targetLabel = train.target[0]\n",
    "                ,cols = None\n",
    "                ,annot = True\n",
    "                ,ax = ax\n",
    "               )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Correlation (top vs. target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Correlation-top-vs-target'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T04:22:27.994296Z",
     "start_time": "2019-04-21T04:22:27.764702Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# correlation heat map with most highly correlated features relative to the target\n",
    "p = qp.plotter.QuickPlot(fig = plt.figure(), chartProp = 10)\n",
    "ax = p.makeCanvas()\n",
    "p.qpCorrHeatmapRefine(df = train.X_\n",
    "                      ,target = train.y_\n",
    "                      ,targetLabel = train.target[0]\n",
    "                      ,cols = None\n",
    "                      ,annot = True\n",
    "                      ,thresh = 0.2\n",
    "                      ,ax = ax\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Remarks - There are three pairs of highly correlated features:\n",
    "    - 'GarageArea' and 'GarageCars'\n",
    "    - 'TotRmsAbvGrd' and 'GrLivArea'\n",
    "    - '1stFlrSF' and 'TotalBsmtSF\n",
    "This makes sense, given what each feature represents and how each pair items relate to each other. We likely only need one feature from each pair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Pair plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Pair-plot'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T04:22:31.073954Z",
     "start_time": "2019-04-21T04:22:27.996402Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Pair plot\n",
    "p = qp.plotter.QuickPlot(fig = plt.figure(), chartProp = 10)\n",
    "p.qpPairPlot(df = train.X_\n",
    "             ,diag_kind = 'auto')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Faceting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Faceting'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T04:22:31.305392Z",
     "start_time": "2019-04-21T04:22:31.078176Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "p = qp.plotter.QuickPlot(fig = plt.figure(), chartProp = 10)\n",
    "ax = p.makeCanvas(title = '', xLabel = '', yLabel = ''\n",
    "                  ,yShift = 0.8, position = 111)\n",
    "\n",
    "p.qpTwoCatBar(df = train.X_\n",
    "               ,x = 'Pclass'\n",
    "               ,hue = 'Embarked'\n",
    "               ,target = train.y_\n",
    "               ,targetLabel = train.target[0]\n",
    "               ,yUnits = 'p'\n",
    "               ,ax = ax)\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T04:22:32.852454Z",
     "start_time": "2019-04-21T04:22:31.307328Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "p = qp.plotter.QuickPlot(fig = plt.figure(), chartProp = 15)\n",
    "# ax = p.makeCanvas(title = '', xLabel = '', yLabel = ''\n",
    "#                   ,yShift = 0.8, position = 111)\n",
    "\n",
    "p.qpCatNumHistFacet(df = train.X_\n",
    "           ,target = train.y_\n",
    "           ,targetLabel = train.target[0]\n",
    "           ,catRow = 'Sex'\n",
    "           ,catCol = 'Embarked'\n",
    "           ,numCol = 'Age'\n",
    "           ,height = 3\n",
    "           ,aspect = 2\n",
    "           )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T04:22:34.414217Z",
     "start_time": "2019-04-21T04:22:32.855368Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "p = qp.plotter.QuickPlot(fig = plt.figure(), chartProp = 15)\n",
    "# ax = p.makeCanvas(title = '', xLabel = '', yLabel = ''\n",
    "#                   ,yShift = 0.8, position = 111)\n",
    "\n",
    "p.qpCatNumHistFacet(df = train.X_\n",
    "           ,target = train.y_\n",
    "           ,targetLabel = train.target[0]\n",
    "           ,catRow = 'Sex'\n",
    "           ,catCol = 'Pclass'\n",
    "           ,numCol = 'Age'\n",
    "           ,height = 3\n",
    "           ,aspect = 2\n",
    "           )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T04:22:35.394644Z",
     "start_time": "2019-04-21T04:22:34.417124Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p = qp.plotter.QuickPlot(fig = plt.figure(), chartProp = 15)\n",
    "\n",
    "p.qpTwoCatPointFacet(df = train.X_\n",
    "           ,target = train.y_\n",
    "           ,targetLabel = train.target[0]\n",
    "           ,catLine = 'Pclass'\n",
    "           ,catPoint = 'Sex'\n",
    "           ,catGrid = 'Embarked'\n",
    "           ,order = ['female','male'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T04:22:36.281792Z",
     "start_time": "2019-04-21T04:22:35.401944Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p = qp.plotter.QuickPlot(fig = plt.figure(), chartProp = 15)\n",
    "\n",
    "p.qpTwoCatPointFacet(df = train.X_\n",
    "           ,target = train.y_\n",
    "           ,targetLabel = train.target[0]\n",
    "           ,catLine = 'Sex'\n",
    "           ,catPoint = 'Pclass'\n",
    "           ,catGrid = 'Embarked'\n",
    "           ,order = ['female','male'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Target variable evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Target-variable-evaluation'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T04:22:36.303301Z",
     "start_time": "2019-04-21T04:22:36.285595Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# null score\n",
    "pd.Series(train.y_).value_counts(normalize = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Data-cleaning'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers (preliminary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Outliers-preliminary'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Training5'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T04:22:36.365838Z",
     "start_time": "2019-04-21T04:22:36.333382Z"
    }
   },
   "outputs": [],
   "source": [
    "nonNull = train.X_.columns[train.X_.isnull().sum() == 0].values.tolist()\n",
    "nonNullNumCol = list(set(nonNull).intersection(train.featureByDtype_['continuous']))\n",
    "print(nonNull)\n",
    "print(nonNullNumCol)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T04:22:36.374265Z",
     "start_time": "2019-04-21T04:22:36.370309Z"
    }
   },
   "outputs": [],
   "source": [
    "trainPipe = pipeline.Pipeline([\n",
    "    ('outlier', train.OutlierIQR(outlierCount = 2, iqrStep = 1.5, features = ['Age','SibSp','Parch','Fare'], dropOutliers = False))     \n",
    "    ])\n",
    "train.X_ = trainPipe.transform(train.X_)\n",
    "\n",
    "iqrOutliers = np.array(sorted(trainPipe.named_steps['outlier'].outliers_))\n",
    "# train.y_ = np.delete(train.y_, trainPipe.named_steps['outlier'].outliers_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T04:22:36.382591Z",
     "start_time": "2019-04-21T04:22:36.378239Z"
    }
   },
   "outputs": [],
   "source": [
    "clf = ensemble.IsolationForest(behaviour = 'new'\n",
    "                        ,max_samples = train.X_.shape[0]\n",
    "                        ,random_state = 0\n",
    "                        ,contamination = 0.02\n",
    "                        )\n",
    "clf.fit(train.X_[['SibSp','Parch','Fare']])\n",
    "preds = clf.predict(train.X_[['SibSp','Parch','Fare']])\n",
    "# np.unique(preds, return_counts = True)\n",
    "\n",
    "mask = np.isin(preds, -1)  # np.in1d if np.isin is not available\n",
    "ifOutliers = np.where(mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T04:22:36.391625Z",
     "start_time": "2019-04-21T04:22:36.387105Z"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "import eif as iso\n",
    "if_eif = iso.iForest(train.X_[['SibSp','Parch','Fare']].values\n",
    "                 ,ntrees = 100\n",
    "                 ,sample_size = 256\n",
    "                 ,ExtensionLevel = 1\n",
    "                )\n",
    "\n",
    "# calculate anomaly scores\n",
    "anomalies_ratio = 0.02\n",
    "anomaly_scores = if_eif.compute_paths(X_in = train.X_[['SibSp','Parch','Fare']].values)\n",
    "anomaly_scores_sorted = np.argsort(anomaly_scores)\n",
    "eifOutliers = np.array(anomaly_scores_sorted[-int(np.ceil(anomalies_ratio * train.X_.shape[0])):])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "from functools import reduce\n",
    "reduce(np.intersect1d, (iqrOutliers, ifOutliers, eifOutliers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "outliers = np.array([27,  88, 258, 311, 341, 438, 679, 737, 742])\n",
    "train.X_ = train.X_.drop(train.X_.index[outliers])\n",
    "train.y_ = np.delete(train.y_, outliers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Validation5'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Missing data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Missing-data'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Evaluate1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T04:22:36.606152Z",
     "start_time": "2019-04-21T04:22:36.395810Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate missing data\n",
    "train.edaMissingSummary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T04:22:36.834800Z",
     "start_time": "2019-04-21T04:22:36.608958Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate missing data\n",
    "valid.edaMissingSummary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T04:22:36.841187Z",
     "start_time": "2019-04-21T04:22:36.837396Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# missingdata_df = merged_df.columns[merged_df.isnull().any()].tolist()\n",
    "# msno.matrix(merged_df[missingdata_df])\n",
    "\n",
    "# msno.bar(merged_df[missingdata_df], color=\"blue\", log=True, figsize=(30,18))\n",
    "\n",
    "# # \n",
    "# msno.heatmap(merged_df[missingdata_df], figsize=(20,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T04:22:36.879549Z",
     "start_time": "2019-04-21T04:22:36.864418Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# compare feature with missing data\n",
    "train.missingColCompare(train.X_, valid.X_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Training1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T04:22:37.101760Z",
     "start_time": "2019-04-21T04:22:36.882031Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# impute training data\n",
    "trainPipe = pipeline.Pipeline([\n",
    "    ('imputeMedian', train.ContextImputer(nullCol = 'Age', contextCol = 'Parch', strategy = 'median'))     \n",
    "    ,('imputeMode', train.ModeImputer(cols = ['Embarked']))\n",
    "    ])\n",
    "train.X_ = trainPipe.transform(train.X_)\n",
    "train.edaMissingSummary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Validation1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T04:22:37.336682Z",
     "start_time": "2019-04-21T04:22:37.104941Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# impute validation data\n",
    "validPipe = pipeline.Pipeline([\n",
    "    ('imputeMedian', valid.ContextImputer(nullCol = 'Age', contextCol = 'Parch', train = False, trainDf = trainPipe.named_steps['imputeMedian'].fillDf))\n",
    "    ,('imputeMedian2', valid.NumericalImputer(cols = ['Fare','Age'], strategy = 'median'))    \n",
    "    ])\n",
    "valid.X_ = validPipe.transform(valid.X_)\n",
    "valid.edaMissingSummary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Engineering'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Evaluate3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Training3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T04:22:37.518681Z",
     "start_time": "2019-04-21T04:22:37.338914Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Parse titles to learn gender, and identify rare titles which may convey status\n",
    "title = [i.split(',')[1].split('.')[0].strip() for i in train.X_['Name']]\n",
    "train.X_['Title'] = pd.Series(title)\n",
    "train.X_['Title'] = train.X_['Title'].replace(['Lady','the Countess','Countess','Capt','Col','Don','Dr','Major','Rev','Sir','Jonkheer','Dona']\n",
    "                                            ,'Rare')\n",
    "train.X_['Title'] = train.X_['Title'].map({'Master' : 0, 'Miss' : 1, 'Ms' : 1 , 'Mme' : 1, 'Mlle' : 1, 'Mrs' : 1, 'Mr' : 2, 'Rare' : 3})\n",
    "\n",
    "# Distill cabin feature\n",
    "train.X_['CabinQuarter'] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in train.X_['Cabin']])\n",
    "\n",
    "# Family size features and binning\n",
    "train.X_['FamilySize'] = train.X_['SibSp'] + train.X_['Parch'] + 1\n",
    "\n",
    "customBinDict = {'Age' : [16, 32, 48, 64]\n",
    "                 ,'FamilySize' : [1, 2, 4]\n",
    "          }\n",
    "\n",
    "trainPipe = pipeline.Pipeline([\n",
    "    ('customBin', train.CustomBinner(customBinDict = customBinDict))\n",
    "    ,('percentileBin', train.PercentileBinner(cols = ['Age','Fare'], percs = [25, 50, 75]))    \n",
    "    ])\n",
    "train.X_ = trainPipe.transform(train.X_)\n",
    "\n",
    "# drop features\n",
    "train.featureDropper(cols = ['Name','Cabin'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T04:22:37.537771Z",
     "start_time": "2019-04-21T04:22:37.520665Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print new columns\n",
    "for col in train.X_.columns:\n",
    "    if col not in train.featureByDtype_['categorical'] and col not in train.featureByDtype_['continuous']:\n",
    "        print(col)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T04:22:37.552174Z",
     "start_time": "2019-04-21T04:22:37.543082Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# append new continuous features\n",
    "for col in ['FamilySize']:\n",
    "    train.featureByDtype_['continuous'].append(col)\n",
    "\n",
    "# append new categorical features\n",
    "for col in ['AgeCustomBin','AgePercBin','FarePercBin','FamilySize','FamilySizeCustomBin','Title','CabinQuarter']:\n",
    "    train.featureByDtype_['categorical'].append(col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T04:22:42.335338Z",
     "start_time": "2019-04-21T04:22:37.556806Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate additional features\n",
    "train.edaCatTargetCatFeat()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Validation3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T04:22:42.480378Z",
     "start_time": "2019-04-21T04:22:42.337971Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Parse titles to learn gender, and identify rare titles which may convey status\n",
    "title = [i.split(',')[1].split('.')[0].strip() for i in valid.X_['Name']]\n",
    "valid.X_['Title'] = pd.Series(title)\n",
    "valid.X_['Title'] = valid.X_['Title'].replace(['Lady','the Countess','Countess','Capt','Col','Don','Dr','Major','Rev','Sir','Jonkheer','Dona']\n",
    "                                            ,'Rare')\n",
    "valid.X_['Title'] = valid.X_['Title'].map({'Master' : 0, 'Miss' : 1, 'Ms' : 1 , 'Mme' : 1, 'Mlle' : 1, 'Mrs' : 1, 'Mr' : 2, 'Rare' : 3})\n",
    "\n",
    "# Distill cabin feature\n",
    "valid.X_['CabinQuarter'] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in valid.X_['Cabin']])\n",
    "\n",
    "# additional features\n",
    "valid.X_['FamilySize'] = valid.X_['SibSp'] + valid.X_['Parch'] + 1\n",
    "\n",
    "validPipe = pipeline.Pipeline([\n",
    "    ('customBin', valid.CustomBinner(customBinDict = customBinDict))\n",
    "    ,('percentileBin', valid.PercentileBinner(train = False, trainDict = trainPipe.named_steps['percentileBin'].trainDict_))    \n",
    "    ])\n",
    "valid.X_ = validPipe.transform(valid.X_)\n",
    "\n",
    "# drop features\n",
    "valid.featureDropper(cols = ['Name','Cabin'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T04:22:42.489863Z",
     "start_time": "2019-04-21T04:22:42.482890Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print new columns\n",
    "for col in valid.X_.columns:\n",
    "    if col not in valid.featureByDtype_['categorical'] and col not in valid.featureByDtype_['continuous']:\n",
    "        print(col)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T04:22:42.501519Z",
     "start_time": "2019-04-21T04:22:42.494558Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# append new continuous features\n",
    "for col in ['FamilySize']:\n",
    "    valid.featureByDtype_['continuous'].append(col)\n",
    "\n",
    "# append new categorical features\n",
    "for col in ['AgeCustomBin','AgePercBin','FarePercBin','FamilySize','FamilySizeCustomBin','Title','CabinQuarter']:\n",
    "    valid.featureByDtype_['categorical'].append(col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Encoding'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Evaluate2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T04:22:42.532189Z",
     "start_time": "2019-04-21T04:22:42.503650Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# counts of unique values in training data string columns\n",
    "train.X_[train.featureByDtype_['categorical']].apply(pd.Series.nunique, axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T04:22:42.554199Z",
     "start_time": "2019-04-21T04:22:42.535398Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print unique values in each categorical columns\n",
    "for col in train.X_[train.featureByDtype_['categorical']]:\n",
    "    try:\n",
    "        print(col, np.unique(train.X_[col]))\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T04:22:42.571233Z",
     "start_time": "2019-04-21T04:22:42.556494Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# counts of unique values in validation data string columns\n",
    "valid.X_[valid.featureByDtype_['categorical']].apply(pd.Series.nunique, axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T04:22:42.619560Z",
     "start_time": "2019-04-21T04:22:42.573251Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print unique values in each categorical columns\n",
    "for col in valid.X_[valid.featureByDtype_['categorical']]:\n",
    "    if col not in ['Name','Cabin']:\n",
    "        print(col, np.unique(valid.X_[col]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T04:22:42.638953Z",
     "start_time": "2019-04-21T04:22:42.623117Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# \n",
    "for col in train.featureByDtype_['categorical']:\n",
    "    if col not in ['Name','Cabin']:\n",
    "        trainValues = train.X_[col].unique()\n",
    "        validValues = valid.X_[col].unique()\n",
    "\n",
    "        trainDiff = set(trainValues) - set(validValues)\n",
    "        validDiff = set(validValues) - set(trainValues)\n",
    "\n",
    "        if len(trainDiff) > 0 or len(validDiff) > 0:\n",
    "            print('\\n\\n*** ' + col)\n",
    "            print('Value present in training data, not in validation data')\n",
    "            print(trainDiff)\n",
    "            print('Value present in validation data, not in training data')\n",
    "            print(validDiff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Pclass [1 2 3] - ordinal\n",
    "\n",
    "Sex ['female' 'male'] - nominal\n",
    "\n",
    "Embarked ['C' 'Q' 'S'] - nominal\n",
    "\n",
    "HasCabin [0 1] - nominal\n",
    "\n",
    "Title [0 1 2 3] - nominal\n",
    "\n",
    "CabinQuarter ['A' 'B' 'C' 'D' 'E' 'F' 'G' 'X'] - nominal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Training2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T04:22:42.702100Z",
     "start_time": "2019-04-21T04:22:42.641505Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### ordinal columns\n",
    "ordCatCols = {\n",
    "    'Pclass' : {1 : 1, 2 : 2, 3 : 3}\n",
    "    }\n",
    "\n",
    "# encode categorical columns\n",
    "nomCatCols = ['Embarked','Sex','CabinQuarter','Title']\n",
    "trainPipe = pipeline.Pipeline([\n",
    "    ('encodeOrdinal', train.CustomOrdinalEncoder(encodings = ordCatCols))    \n",
    "    ,('dummyNominal', train.Dummies(cols = nomCatCols, dropFirst = True))\n",
    "    ])\n",
    "train.X_ = trainPipe.transform(train.X_)\n",
    "train.X_[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Validation2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T04:22:42.752219Z",
     "start_time": "2019-04-21T04:22:42.705204Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# encode categorical columns\n",
    "validPipe = pipeline.Pipeline([\n",
    "    ('encodeOrdinal', valid.CustomOrdinalEncoder(encodings = ordCatCols))    \n",
    "    ,('dummyNominal', valid.Dummies(cols = nomCatCols, dropFirst = False))\n",
    "    ,('levels', valid.MissingDummies(trainCols = train.X_.columns))    \n",
    "    ])\n",
    "valid.X_ = validPipe.transform(valid.X_)\n",
    "valid.X_[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Transformation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Evaluate4'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T04:22:42.817130Z",
     "start_time": "2019-04-21T04:22:42.778090Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate skew of continuous features - Train\n",
    "train.skewSummary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T04:22:42.861850Z",
     "start_time": "2019-04-21T04:22:42.819778Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate skew of continuous features - Validation\n",
    "valid.skewSummary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Training4'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T04:22:43.435265Z",
     "start_time": "2019-04-21T04:22:42.864385Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "trainPipe = pipeline.Pipeline([\n",
    "    ('skew', train.SkewTransform(cols = train.featureByDtype_['continuous'], skewMin = 0.75, pctZeroMax = 1.0))\n",
    "    ])\n",
    "train.X_ = trainPipe.transform(train.X_)\n",
    "train.skewSummary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Validation4'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T04:22:43.551291Z",
     "start_time": "2019-04-21T04:22:43.437466Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "validPipe = pipeline.Pipeline([\n",
    "    ('skew', valid.SkewTransform(train = False, trainDict = trainPipe.named_steps['skew'].colValueDict_))\n",
    "    ])\n",
    "valid.X_ = validPipe.transform(valid.X_)\n",
    "valid.skewSummary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Outliers (final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Outliers-final'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Training6'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T04:22:43.569181Z",
     "start_time": "2019-04-21T04:22:43.565995Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainPipe = pipeline.Pipeline([\n",
    "    ('outlier', train.OutlierIQR(outlierCount = 5, iqrStep = 1.5, features = train.X_.columns, dropOutliers = False))     \n",
    "    ])\n",
    "train.X_ = trainPipe.transform(train.X_)\n",
    "\n",
    "iqrOutliers = np.array(sorted(trainPipe.named_steps['outlier'].outliers_))\n",
    "# train.y_ = np.delete(train.y_, trainPipe.named_steps['outlier'].outliers_)\n",
    "iqrOutliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T04:22:43.576514Z",
     "start_time": "2019-04-21T04:22:43.572803Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "clf = ensemble.IsolationForest(behaviour = 'new'\n",
    "                        ,max_samples = train.X_.shape[0]\n",
    "                        ,random_state = 0\n",
    "                        ,contamination = 0.01\n",
    "                        )\n",
    "clf.fit(train.X_[train.X_.columns])\n",
    "preds = clf.predict(train.X_[train.X_.columns])\n",
    "# np.unique(preds, return_counts = True)\n",
    "\n",
    "mask = np.isin(preds, -1)  # np.in1d if np.isin is not available\n",
    "ifOutliers = np.where(mask)\n",
    "ifOutliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T04:22:43.604243Z",
     "start_time": "2019-04-21T04:22:43.579376Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import eif as iso\n",
    "if_eif = iso.iForest(train.X_.values\n",
    "                 ,ntrees = 100\n",
    "                 ,sample_size = 256\n",
    "                 ,ExtensionLevel = 1\n",
    "                )\n",
    "\n",
    "# calculate anomaly scores\n",
    "anomalies_ratio = 0.01\n",
    "anomaly_scores = if_eif.compute_paths(X_in = train.X_.values)\n",
    "anomaly_scores_sorted = np.argsort(anomaly_scores)\n",
    "eifOutliers = np.array(anomaly_scores_sorted[-int(np.ceil(anomalies_ratio * train.X_.shape[0])):])\n",
    "eifOutliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "from functools import reduce\n",
    "# reduce(np.intersect1d, (iqrOutliers, ifOutliers, eifOutliers))\n",
    "reduce(np.intersect1d, (ifOutliers, eifOutliers))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Data evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Data evaluation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Feature-importance'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T04:22:43.642810Z",
     "start_time": "2019-04-21T04:22:43.607052Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# feature importance summary table\n",
    "featureImp = train.featureImportanceSummary()\n",
    "featureImp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Rationality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Rationality'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T04:22:44.118834Z",
     "start_time": "2019-04-21T04:22:43.645510Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# percent difference summary\n",
    "dfDiff = abs((((valid.X_.describe() + 1) - (train.X_.describe() + 1)) / (train.X_.describe() + 1)) * 100)\n",
    "dfDiff = dfDiff[dfDiff.columns].replace({0 : np.nan})\n",
    "dfDiff[dfDiff < 0] = np.nan\n",
    "dfDiff = dfDiff.fillna('')\n",
    "display(dfDiff)\n",
    "display(train.X_.describe())\n",
    "display(valid.X_.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Value override"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Value override'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T04:22:44.123975Z",
     "start_time": "2019-04-21T04:22:44.120951Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# change clearly erroneous value to what it probably was\n",
    "# exploreValid.X_['GarageYrBlt'].replace({2207 : 2007}, inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Continuous feature EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Continuous-feature-EDA3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Univariate & feature vs. target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Univariate-&-feature-vs.-target3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T04:22:46.398075Z",
     "start_time": "2019-04-21T04:22:44.126522Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Continuous features\n",
    "train.edaCatTargetNumFeat()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Correlation3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Correlation (top vs. target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Correlation-top-vs-target3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T04:22:46.929314Z",
     "start_time": "2019-04-21T04:22:46.402183Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# correlation heat map with most highly correlated features relative to the target\n",
    "p = qp.plotter.QuickPlot(fig = plt.figure(), chartProp = 15)\n",
    "ax = p.makeCanvas()\n",
    "p.qpCorrHeatmapRefine(df = train.X_\n",
    "                      ,target = train.y_\n",
    "                      ,targetLabel = train.target[0]\n",
    "                      ,cols = None\n",
    "                      ,annot = True\n",
    "                      ,thresh = 0.25\n",
    "                      ,ax = ax\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Modeling'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Prepare-training-data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T04:34:47.470321Z",
     "start_time": "2019-04-21T04:34:46.704576Z"
    }
   },
   "outputs": [],
   "source": [
    "importlib.reload(mlm.model.tune.bayesianOptimSearch)\n",
    "importlib.reload(mlm.model.tune.stack)\n",
    "importlib.reload(mlm)\n",
    "\n",
    "### import training data\n",
    "dfTrain = pd.read_csv('data/train.csv')\n",
    "train = mlm.Machine(data = dfTrain\n",
    "                  ,target = ['Survived']\n",
    "                  ,removeFeatures = ['PassengerId','Ticket']                      \n",
    "                  ,overrideCat = ['Pclass','SibSp','Parch']\n",
    "                  ,targetType = 'categorical'\n",
    "                )\n",
    "\n",
    "### feature engineering\n",
    "# Parse titles to learn gender, and identify rare titles which may convey status\n",
    "title = [i.split(',')[1].split('.')[0].strip() for i in train.X_['Name']]\n",
    "train.X_['Title'] = pd.Series(title)\n",
    "train.X_['Title'] = train.X_['Title'].replace(['Lady','the Countess','Countess','Capt','Col','Don','Dr','Major','Rev','Sir','Jonkheer','Dona']\n",
    "                                            ,'Rare')\n",
    "train.X_['Title'] = train.X_['Title'].map({'Master' : 0, 'Miss' : 1, 'Ms' : 1 , 'Mme' : 1, 'Mlle' : 1, 'Mrs' : 1, 'Mr' : 2, 'Rare' : 3})\n",
    "\n",
    "# Distill cabin feature\n",
    "train.X_['CabinQuarter'] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in train.X_['Cabin']])\n",
    "\n",
    "# Family size features\n",
    "train.X_['FamilySize'] = train.X_['SibSp'] + train.X_['Parch'] + 1\n",
    "\n",
    "# custom bin specifications\n",
    "customBinDict = {'Age' : [16, 32, 48, 64]\n",
    "                 ,'FamilySize' : [1, 2, 4]\n",
    "          }\n",
    "# categorical column specifications\n",
    "ordCatCols = {\n",
    "    'Pclass' : {1 : 1, 2 : 2, 3 : 3}\n",
    "    }\n",
    "nomCatCols = ['Embarked','Sex','CabinQuarter','Title']\n",
    "\n",
    "# remove outliers\n",
    "outliers = np.array([27, 88, 258, 311, 341, 438, 679, 737, 742])\n",
    "train.X_ = train.X_.drop(train.X_.index[outliers])\n",
    "train.y_ = np.delete(train.y_, outliers)\n",
    "\n",
    "### pipeline\n",
    "trainPipe = pipeline.Pipeline([\n",
    "    ('imputeMedian', train.ContextImputer(nullCol = 'Age', contextCol = 'Parch', strategy = 'median'))     \n",
    "    ,('imputeMode', train.ModeImputer(cols = ['Embarked']))\n",
    "    ,('customBin', train.CustomBinner(customBinDict = customBinDict))\n",
    "    ,('percentileBin', train.PercentileBinner(cols = ['Age','Fare'], percs = [25, 50, 75]))    \n",
    "    ,('encodeOrdinal', train.CustomOrdinalEncoder(encodings = ordCatCols))    \n",
    "    ,('dummyNominal', train.Dummies(cols = nomCatCols, dropFirst = True))\n",
    "    ,('skew', train.SkewTransform(cols = train.featureByDtype_['continuous'], skewMin = 0.75, pctZeroMax = 1.0))    \n",
    "    ])\n",
    "train.X_ = trainPipe.transform(train.X_)\n",
    "\n",
    "# drop features\n",
    "train.featureDropper(cols = ['Name','Cabin'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Prepare validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Prepare-validation-data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T04:22:47.649554Z",
     "start_time": "2019-04-21T04:22:47.423277Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### import valid data\n",
    "dfValid = pd.read_csv('data/test.csv')\n",
    "valid = mlm.Machine(data = dfValid\n",
    "                  ,removeFeatures = ['PassengerId','Ticket']                      \n",
    "                  ,overrideCat = ['Pclass','SibSp','Parch']\n",
    "                )\n",
    "\n",
    "### feature engineering\n",
    "# Parse titles to learn gender, and identify rare titles which may convey status\n",
    "title = [i.split(',')[1].split('.')[0].strip() for i in valid.X_['Name']]\n",
    "valid.X_['Title'] = pd.Series(title)\n",
    "valid.X_['Title'] = valid.X_['Title'].replace(['Lady','the Countess','Countess','Capt','Col','Don','Dr','Major','Rev','Sir','Jonkheer','Dona']\n",
    "                                            ,'Rare')\n",
    "valid.X_['Title'] = valid.X_['Title'].map({'Master' : 0, 'Miss' : 1, 'Ms' : 1 , 'Mme' : 1, 'Mlle' : 1, 'Mrs' : 1, 'Mr' : 2, 'Rare' : 3})\n",
    "\n",
    "# Distill cabin feature\n",
    "valid.X_['CabinQuarter'] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in valid.X_['Cabin']])\n",
    "\n",
    "# additional features\n",
    "valid.X_['FamilySize'] = valid.X_['SibSp'] + valid.X_['Parch'] + 1\n",
    "\n",
    "### pipeline\n",
    "validPipe = pipeline.Pipeline([\n",
    "    ('imputeMedian', valid.ContextImputer(nullCol = 'Age', contextCol = 'Parch', train = False, trainDf = trainPipe.named_steps['imputeMedian'].fillDf))\n",
    "    ,('imputeMedian2', valid.NumericalImputer(cols = ['Fare','Age'], strategy = 'median'))    \n",
    "    ,('customBin', valid.CustomBinner(customBinDict = customBinDict))\n",
    "    ,('percentileBin', valid.PercentileBinner(train = False, trainDict = trainPipe.named_steps['percentileBin'].trainDict_))   \n",
    "    ,('encodeOrdinal', valid.CustomOrdinalEncoder(encodings = ordCatCols))    \n",
    "    ,('dummyNominal', valid.Dummies(cols = nomCatCols, dropFirst = False))\n",
    "    ,('levels', valid.MissingDummies(trainCols = train.X_.columns))    \n",
    "    ,('skew', valid.SkewTransform(train = False, trainDict = trainPipe.named_steps['skew'].colValueDict_))    \n",
    "    ])\n",
    "valid.X_ = validPipe.transform(valid.X_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'GridSearch'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T04:27:03.319083Z",
     "start_time": "2019-04-21T04:27:03.187953Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    " # parameter space\n",
    "allSpace = {\n",
    "            'lightgbm.LGBMClassifier' : {\n",
    "                'class_weight' : hp.choice('class_weight', [None])\n",
    "                ,'colsample_bytree': hp.uniform('colsample_bytree', 0.4, 0.7)\n",
    "                ,'boosting_type' : hp.choice('boosting_type', ['dart'])                \n",
    "                ,'subsample': hp.uniform('subsample', 0.5, 1)\n",
    "                ,'learning_rate' : hp.uniform('learning_rate', 0.15, 0.25)\n",
    "                ,'max_depth' : hp.choice('max_depth', np.arange(4, 20, dtype = int))\n",
    "                ,'min_child_samples' : hp.quniform('min_child_samples', 50, 150, 5)\n",
    "                ,'n_estimators' : hp.choice('n_estimators', np.arange(100, 4000, 10, dtype = int))\n",
    "                ,'num_leaves': hp.quniform('num_leaves', 30, 70, 1)\n",
    "                ,'reg_alpha': hp.uniform('reg_alpha', 0.75, 1.25)\n",
    "                ,'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0)\n",
    "                ,'subsample_for_bin': hp.quniform('subsample_for_bin', 100000, 350000, 20000)\n",
    "            }\n",
    "            ,'linear_model.LogisticRegression' : {\n",
    "                'C': hp.uniform('C', 0.04, 0.1)\n",
    "                ,'penalty': hp.choice('penalty', ['l1'])\n",
    "            }\n",
    "            ,'xgboost.XGBClassifier' : {\n",
    "                'colsample_bytree' : hp.uniform('colsample_bytree', 0.4, 0.7)\n",
    "                ,'gamma' : hp.quniform('gamma', 0.0, 10, 0.05)\n",
    "                ,'learning_rate' : hp.quniform('learning_rate', 0.01, 0.2, 0.01)\n",
    "                ,'max_depth' : hp.choice('max_depth', np.arange(2, 15, dtype = int))\n",
    "                ,'min_child_weight': hp.quniform ('min_child_weight', 2.5, 7.5, 1)\n",
    "                ,'n_estimators' : hp.choice('n_estimators', np.arange(100, 4000, 10, dtype = int))\n",
    "                ,'subsample': hp.uniform ('subsample', 0.4, 0.7)\n",
    "            }\n",
    "            ,'ensemble.RandomForestClassifier' : {\n",
    "                'bootstrap' : hp.choice('bootstrap', [True, False])\n",
    "                ,'max_depth' : hp.choice('max_depth', np.arange(2, 10, dtype = int))\n",
    "                ,'n_estimators' : hp.choice('n_estimators', np.arange(100, 8000, 10, dtype = int))\n",
    "                ,'max_features' : hp.choice('max_features', ['sqrt'])\n",
    "                ,'min_samples_split' : hp.choice('min_samples_split', np.arange(15, 25, dtype = int))\n",
    "                ,'min_samples_leaf' : hp.choice('min_samples_leaf', np.arange(2, 20, dtype = int))\n",
    "            }\n",
    "            ,'ensemble.GradientBoostingClassifier' : {\n",
    "                'n_estimators' : hp.choice('n_estimators', np.arange(100, 4000, 10, dtype = int))\n",
    "                ,'max_depth' : hp.choice('max_depth', np.arange(2, 11, dtype = int))\n",
    "                ,'max_features' : hp.choice('max_features', ['sqrt'])    \n",
    "                ,'learning_rate' : hp.quniform('learning_rate', 0.01, 0.09, 0.01)\n",
    "                ,'loss' : hp.choice('loss', ['deviance','exponential'])    \n",
    "                ,'min_samples_split' : hp.choice('min_samples_split', np.arange(2, 40, dtype = int))\n",
    "                ,'min_samples_leaf' : hp.choice('min_samples_leaf', np.arange(2, 40, dtype = int))\n",
    "            }\n",
    "            ,'ensemble.AdaBoostClassifier' : {\n",
    "                'n_estimators' : hp.choice('n_estimators', np.arange(100, 4000, 10, dtype = int))\n",
    "                ,'learning_rate' : hp.quniform('learning_rate', 0.1, 0.25, 0.01)\n",
    "                ,'algorithm' : hp.choice('algorithm', ['SAMME'])                    \n",
    "            }\n",
    "            ,'naive_bayes.BernoulliNB' : {\n",
    "                'alpha' :  hp.uniform('alpha', 0.01, 2)\n",
    "            }\n",
    "            ,'ensemble.BaggingClassifier' : {\n",
    "                'n_estimators' : hp.choice('n_estimators', np.arange(100, 4000, 10, dtype = int))\n",
    "                ,'max_samples' : hp.uniform('max_samples', 0.01, 0.3)                    \n",
    "            }\n",
    "            ,'ensemble.ExtraTreesClassifier' : {\n",
    "                'n_estimators' : hp.choice('n_estimators', np.arange(100, 4000, 10, dtype = int))\n",
    "                ,'max_depth' : hp.choice('max_depth', np.arange(2, 15, dtype = int))\n",
    "                ,'min_samples_split' : hp.choice('min_samples_split', np.arange(4, 30, dtype = int))\n",
    "                ,'min_samples_leaf' : hp.choice('min_samples_leaf', np.arange(2, 20, dtype = int))\n",
    "                ,'max_features' : hp.choice('max_features', ['auto'])\n",
    "                ,'criterion' : hp.choice('criterion', ['entropy'])\n",
    "            }\n",
    "            ,'svm.SVC' : {\n",
    "                'C' : hp.uniform('C', 4, 15)\n",
    "                ,'decision_function_shape' : hp.choice('decision_function_shape', ['ovr'])\n",
    "                ,'gamma' : hp.uniform('gamma', 0.00000001, 1.5)\n",
    "            }\n",
    "            ,'neighbors.KNeighborsClassifier' : {\n",
    "                'algorithm' : hp.choice('algorithm', ['ball_tree','brute'])\n",
    "                ,'n_neighbors' : hp.choice('n_neighbors', np.arange(1, 15, dtype = int))\n",
    "                ,'weights' : hp.choice('weights', ['uniform'])\n",
    "            }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T04:35:54.027572Z",
     "start_time": "2019-04-21T04:34:49.115652Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# execute bayesian optimization grid search\n",
    "analysis = 'titanic'\n",
    "train.execBayesOptimSearch(allSpace = allSpace\n",
    "                           ,resultsDir = 'data/{}_hyperopt_{}.csv'.format(rundate, analysis)\n",
    "#                            ,model = ''\n",
    "                           ,X = train.X_\n",
    "                           ,y = train.y_\n",
    "                           ,scoring = 'accuracy'\n",
    "                           ,n_folds = 8\n",
    "                           ,n_jobs = 16\n",
    "                           ,iters = 1000\n",
    "                           ,verbose = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T21:40:36.984359Z",
     "start_time": "2019-04-21T21:40:22.684177Z"
    }
   },
   "outputs": [],
   "source": [
    "# read scores summary table\n",
    "resultsDf = pd.read_csv('data/20190423_hyperopt_titanic.csv', na_values = 'nan')\n",
    "results = train.unpackParams(resultsDf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T02:36:41.656988Z",
     "start_time": "2019-04-21T02:36:36.436713Z"
    }
   },
   "outputs": [],
   "source": [
    "# loss plot\n",
    "train.lossPlot(resultsDf = results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T03:06:35.854272Z",
     "start_time": "2019-04-21T03:05:05.987237Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# estimator parameter plots\n",
    "train.paramPlot(results = results, allSpace = allSpace, nIter = 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T02:56:09.932551Z",
     "start_time": "2019-04-21T02:56:08.917473Z"
    }
   },
   "outputs": [],
   "source": [
    "sampleSpace = {\n",
    "#             'param': hp.uniform('param', np.log(0.4), np.log(0.6))\n",
    "            '' : 0.000001 + hp.uniform('gamma', 0.000001, 10)\n",
    "#             'param2': hp.loguniform('param2', np.log(0.001), np.log(0.01))\n",
    "        }\n",
    "\n",
    "train.samplePlot(sampleSpace, 1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Evaluation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Model explanability\n",
    "\n",
    "https://www.kaggle.com/learn/machine-learning-explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Feature-importance'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Permutation importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Permutation-importance'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T13:55:21.322458Z",
     "start_time": "2019-04-02T13:55:21.259Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# permutation importance\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "perm = PermutationImportance(my_model, random_state=1).fit(val_X, val_y)\n",
    "eli5.show_weights(perm, feature_names = val_X.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Partial plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Partial-plots'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T13:55:21.326597Z",
     "start_time": "2019-04-02T13:55:21.267Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "from matplotlib import pyplot as plt\n",
    "from pdpbox import pdp, get_dataset, info_plots\n",
    "\n",
    "# Create the data that we will plot\n",
    "pdp_goals = pdp.pdp_isolate(model=tree_model, dataset=val_X, model_features=feature_names, feature='Goal Scored')\n",
    "\n",
    "# plot it\n",
    "pdp.pdp_plot(pdp_goals, 'Goal Scored')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T13:55:21.334512Z",
     "start_time": "2019-04-02T13:55:21.275Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "feature_to_plot = 'Distance Covered (Kms)'\n",
    "pdp_dist = pdp.pdp_isolate(model=tree_model, dataset=val_X, model_features=feature_names, feature=feature_to_plot)\n",
    "\n",
    "pdp.pdp_plot(pdp_dist, feature_to_plot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T13:55:21.336354Z",
     "start_time": "2019-04-02T13:55:21.281Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Build Random Forest model\n",
    "rf_model = RandomForestClassifier(random_state=0).fit(train_X, train_y)\n",
    "\n",
    "pdp_dist = pdp.pdp_isolate(model=rf_model, dataset=val_X, model_features=feature_names, feature=feature_to_plot)\n",
    "\n",
    "pdp.pdp_plot(pdp_dist, feature_to_plot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T13:55:21.454582Z",
     "start_time": "2019-04-02T13:55:21.438137Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 2D plots\n",
    "# Similar to previous PDP plot except we use pdp_interact instead of pdp_isolate and pdp_interact_plot instead of pdp_isolate_plot\n",
    "features_to_plot = ['Goal Scored', 'Distance Covered (Kms)']\n",
    "inter1  =  pdp.pdp_interact(model=tree_model, dataset=val_X, model_features=feature_names, features=features_to_plot)\n",
    "\n",
    "pdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=features_to_plot, plot_type='contour')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### SHAP values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'SHAP-values'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T13:55:21.457225Z",
     "start_time": "2019-04-02T13:55:21.302Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "row_to_show = 5\n",
    "data_for_prediction = val_X.iloc[row_to_show]  # use 1 row of data here. Could use multiple rows if desired\n",
    "data_for_prediction_array = data_for_prediction.values.reshape(1, -1)\n",
    "\n",
    "\n",
    "my_model.predict_proba(data_for_prediction_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T13:55:21.458460Z",
     "start_time": "2019-04-02T13:55:21.309Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import shap  # package used to calculate Shap values\n",
    "\n",
    "# Create object that can calculate shap values\n",
    "explainer = shap.TreeExplainer(my_model)\n",
    "\n",
    "# Calculate Shap values\n",
    "shap_values = explainer.shap_values(data_for_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T13:55:21.459772Z",
     "start_time": "2019-04-02T13:55:21.315Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T13:55:21.461568Z",
     "start_time": "2019-04-02T13:55:21.322Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# use Kernel SHAP to explain test set predictions\n",
    "k_explainer = shap.KernelExplainer(my_model.predict_proba, train_X)\n",
    "k_shap_values = k_explainer.shap_values(data_for_prediction)\n",
    "shap.force_plot(k_explainer.expected_value[1], k_shap_values[1], data_for_prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T13:55:21.463070Z",
     "start_time": "2019-04-02T13:55:21.329Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "shap.DeepExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T13:55:21.464257Z",
     "start_time": "2019-04-02T13:55:21.335Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import shap  # package used to calculate Shap values\n",
    "\n",
    "# Create object that can calculate shap values\n",
    "explainer = shap.TreeExplainer(my_model)\n",
    "\n",
    "# calculate shap values. This is what we will plot.\n",
    "# Calculate shap_values for all of val_X rather than a single row, to have more data for plot.\n",
    "shap_values = explainer.shap_values(val_X)\n",
    "\n",
    "# Make plot. Index of [1] is explained in text below.\n",
    "shap.summary_plot(shap_values[1], val_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T13:55:21.465491Z",
     "start_time": "2019-04-02T13:55:21.341Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import shap  # package used to calculate Shap values\n",
    "\n",
    "# Create object that can calculate shap values\n",
    "explainer = shap.TreeExplainer(my_model)\n",
    "\n",
    "# calculate shap values. This is what we will plot.\n",
    "shap_values = explainer.shap_values(X)\n",
    "\n",
    "# make plot.\n",
    "shap.dependence_plot('Ball Possession %', shap_values[1], X, interaction_index=\"Goal Scored\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Stacking'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primary models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Primary-models'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T03:31:27.585294Z",
     "start_time": "2019-04-21T03:31:27.340020Z"
    }
   },
   "outputs": [],
   "source": [
    "resultsDf[resultsDf['estimator'] == 'xgboost.XGBClassifier'].sort_values(['mean'], ascending = [False])[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T21:45:35.037218Z",
     "start_time": "2019-04-21T21:45:34.984437Z"
    }
   },
   "outputs": [],
   "source": [
    "def topParamSelector(resultsDf, num):\n",
    "    models = {}\n",
    "    for estimator in resultsDf['estimator'].unique():\n",
    "        estDf = resultsDf[resultsDf['estimator'] == estimator].sort_values(['mean'], ascending = [False])['iteration'][:num]\n",
    "        models[estimator] = estDf.values.tolist()\n",
    "    return models\n",
    "models = topParamSelector(resultsDf = resultsDf, num = 1)\n",
    "models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T21:57:57.217321Z",
     "start_time": "2019-04-21T21:45:50.491883Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "oofTrain, oofValid, columns = train.modelStacker(models = models\n",
    "                                                 ,resultsDf = resultsDf\n",
    "                                                 ,XTrain = train.X_.values\n",
    "                                                 ,yTrain = train.y_\n",
    "                                                 ,XValid = valid.X_.values\n",
    "                                                 ,nFolds = 2\n",
    "                                                 ,nJobs = 16)\n",
    "                                                             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T04:40:47.625741Z",
     "start_time": "2019-04-21T04:40:47.030519Z"
    }
   },
   "outputs": [],
   "source": [
    "# view correlations of predictions\n",
    "sns.set_style('whitegrid')\n",
    "p = qp.plotter.QuickPlot(fig = plt.figure(), chartProp = 15)\n",
    "ax = p.makeCanvas(position = 111)\n",
    "p.qpCorrHeatmap(df = pd.DataFrame(oofTrain, columns = columns)\n",
    "                ,annot = True\n",
    "                ,ax = ax\n",
    "                ,vmin = 0\n",
    "               )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Meta-model'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T04:40:47.901475Z",
     "start_time": "2019-04-21T04:40:47.840529Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "importlib.reload(mlm.model.tune.bayesianOptimSearch)\n",
    "importlib.reload(mlm.model.tune.stack)\n",
    "importlib.reload(mlm)\n",
    "# parameter space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    " # parameter space\n",
    "allSpace = {\n",
    "            'lightgbm.LGBMClassifier' : {\n",
    "                'class_weight' : hp.choice('class_weight', [None])\n",
    "                ,'colsample_bytree': hp.uniform('colsample_bytree', 0.4, 0.7)\n",
    "                ,'boosting_type' : hp.choice('boosting_type', ['dart'])                \n",
    "                ,'subsample': hp.uniform('subsample', 0.5, 1)\n",
    "                ,'learning_rate' : hp.uniform('learning_rate', 0.15, 0.25)\n",
    "                ,'max_depth' : hp.choice('max_depth', np.arange(4, 20, dtype = int))\n",
    "                ,'min_child_samples' : hp.quniform('min_child_samples', 50, 150, 5)\n",
    "                ,'n_estimators' : hp.choice('n_estimators', np.arange(100, 4000, 10, dtype = int))\n",
    "                ,'num_leaves': hp.quniform('num_leaves', 30, 70, 1)\n",
    "                ,'reg_alpha': hp.uniform('reg_alpha', 0.75, 1.25)\n",
    "                ,'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0)\n",
    "                ,'subsample_for_bin': hp.quniform('subsample_for_bin', 100000, 350000, 20000)\n",
    "            }\n",
    "            ,'linear_model.LogisticRegression' : {\n",
    "                'C': hp.uniform('C', 0.04, 0.1)\n",
    "                ,'penalty': hp.choice('penalty', ['l1'])\n",
    "            }\n",
    "            ,'xgboost.XGBClassifier' : {\n",
    "                'colsample_bytree' : hp.uniform('colsample_bytree', 0.4, 0.7)\n",
    "                ,'gamma' : hp.quniform('gamma', 0.0, 10, 0.05)\n",
    "                ,'learning_rate' : hp.quniform('learning_rate', 0.01, 0.2, 0.01)\n",
    "                ,'max_depth' : hp.choice('max_depth', np.arange(2, 15, dtype = int))\n",
    "                ,'min_child_weight': hp.quniform ('min_child_weight', 2.5, 7.5, 1)\n",
    "                ,'n_estimators' : hp.choice('n_estimators', np.arange(100, 4000, 10, dtype = int))\n",
    "                ,'subsample': hp.uniform ('subsample', 0.4, 0.7)\n",
    "            }\n",
    "            ,'ensemble.RandomForestClassifier' : {\n",
    "                'bootstrap' : hp.choice('bootstrap', [True, False])\n",
    "                ,'max_depth' : hp.choice('max_depth', np.arange(2, 10, dtype = int))\n",
    "                ,'n_estimators' : hp.choice('n_estimators', np.arange(100, 8000, 10, dtype = int))\n",
    "                ,'max_features' : hp.choice('max_features', ['sqrt'])\n",
    "                ,'min_samples_split' : hp.choice('min_samples_split', np.arange(15, 25, dtype = int))\n",
    "                ,'min_samples_leaf' : hp.choice('min_samples_leaf', np.arange(2, 20, dtype = int))\n",
    "            }\n",
    "            ,'ensemble.GradientBoostingClassifier' : {\n",
    "                'n_estimators' : hp.choice('n_estimators', np.arange(100, 4000, 10, dtype = int))\n",
    "                ,'max_depth' : hp.choice('max_depth', np.arange(2, 11, dtype = int))\n",
    "                ,'max_features' : hp.choice('max_features', ['sqrt'])    \n",
    "                ,'learning_rate' : hp.quniform('learning_rate', 0.01, 0.09, 0.01)\n",
    "                ,'loss' : hp.choice('loss', ['deviance','exponential'])    \n",
    "                ,'min_samples_split' : hp.choice('min_samples_split', np.arange(2, 40, dtype = int))\n",
    "                ,'min_samples_leaf' : hp.choice('min_samples_leaf', np.arange(2, 40, dtype = int))\n",
    "            }\n",
    "            ,'ensemble.AdaBoostClassifier' : {\n",
    "                'n_estimators' : hp.choice('n_estimators', np.arange(100, 4000, 10, dtype = int))\n",
    "                ,'learning_rate' : hp.quniform('learning_rate', 0.1, 0.25, 0.01)\n",
    "                ,'algorithm' : hp.choice('algorithm', ['SAMME'])                    \n",
    "            }\n",
    "            ,'naive_bayes.BernoulliNB' : {\n",
    "                'alpha' :  hp.uniform('alpha', 0.01, 2)\n",
    "            }\n",
    "            ,'ensemble.BaggingClassifier' : {\n",
    "                'n_estimators' : hp.choice('n_estimators', np.arange(100, 4000, 10, dtype = int))\n",
    "                ,'max_samples' : hp.uniform('max_samples', 0.01, 0.3)                    \n",
    "            }\n",
    "            ,'ensemble.ExtraTreesClassifier' : {\n",
    "                'n_estimators' : hp.choice('n_estimators', np.arange(100, 4000, 10, dtype = int))\n",
    "                ,'max_depth' : hp.choice('max_depth', np.arange(2, 15, dtype = int))\n",
    "                ,'min_samples_split' : hp.choice('min_samples_split', np.arange(4, 30, dtype = int))\n",
    "                ,'min_samples_leaf' : hp.choice('min_samples_leaf', np.arange(2, 20, dtype = int))\n",
    "                ,'max_features' : hp.choice('max_features', ['auto'])\n",
    "                ,'criterion' : hp.choice('criterion', ['entropy'])\n",
    "            }\n",
    "            'svm.SVC' : {\n",
    "                'C' : hp.uniform('C', 0.00000001, 15)\n",
    "                ,'decision_function_shape' : hp.choice('decision_function_shape', ['ovr','ovo'])\n",
    "                ,'gamma' : hp.uniform('gamma', 0.00000001, 1.5)\n",
    "            }\n",
    "            ,'neighbors.KNeighborsClassifier' : {\n",
    "                'algorithm' : hp.choice('algorithm', ['ball_tree','brute'])\n",
    "                ,'n_neighbors' : hp.choice('n_neighbors', np.arange(1, 15, dtype = int))\n",
    "                ,'weights' : hp.choice('weights', ['uniform'])\n",
    "            }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T04:43:29.736147Z",
     "start_time": "2019-04-21T04:41:22.027092Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# execute bayesian optimization grid search\n",
    "train.execBayesOptimSearch(allSpace = allSpace\n",
    "                           ,resultsDir = 'data/{}_hyperopt_meta_{}_2.csv'.format(rundate, analysis)\n",
    "                           ,X = oofTrain\n",
    "                           ,y = train.y_\n",
    "                           ,scoring = 'accuracy'\n",
    "                           ,n_folds = 8\n",
    "                           ,n_jobs = 8\n",
    "                           ,iters = 3000\n",
    "                           ,verbose = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T05:22:07.374995Z",
     "start_time": "2019-04-21T05:22:06.541075Z"
    }
   },
   "outputs": [],
   "source": [
    "# read scores summary table\n",
    "resultsMetaDf = pd.read_csv('data/20190423_hyperopt_meta_titanic_2.csv', na_values = 'nan')\n",
    "resultsMeta = train.unpackParams(resultsMetaDf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss plot\n",
    "train.lossPlot(resultsDf = resultsMeta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimator parameter plots\n",
    "train.paramPlot(results = resultsMeta, allSpace = allSpace, nIter = 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Submission'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Standard'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T05:28:25.009926Z",
     "start_time": "2019-04-21T05:28:24.956970Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## standard model fit and predict\n",
    "\n",
    "# select estimator and iteration\n",
    "# estimator = 'ensemble.RandomForestClassifier'\n",
    "# iteration = 1955\n",
    "# estimator = 'xgboost.XGBClassifier'\n",
    "# iteration = 2097\n",
    "estimator = 'lightgbm.LGBMClassifier'\n",
    "iteration = 2264\n",
    "\n",
    "# extract params and instantiate model\n",
    "params = train.paramExtractor(resultsDf = resultsDf, estimator = estimator, iteration = iteration)\n",
    "model = eval('{0}(**{1})'.format(estimator, params))\n",
    "\n",
    "# fit model and make predictions\n",
    "model.fit(train.X_, train.y_)\n",
    "yPred = model.predict(valid.X_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T04:14:43.023400Z",
     "start_time": "2019-04-21T04:14:42.757009Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# generate prediction submission file\n",
    "my_submission = pd.DataFrame({'PassengerId': dfValid.PassengerId, 'Survived': yPred})\n",
    "my_submission.to_csv('data/submission.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Stack'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resultsMetaDf.sort_values(['mean'], ascending = [False])[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T05:29:20.333669Z",
     "start_time": "2019-04-21T05:29:20.304882Z"
    }
   },
   "outputs": [],
   "source": [
    "# best second level learning model\n",
    "# estimator = 'xgboost.XGBClassifier'\n",
    "# estimator = 'ensemble.RandomForestClassifier'\n",
    "# estimator = 'ensemble.GradientBoostingClassifier'\n",
    "estimator = 'svm.SVC'\n",
    "\n",
    "iteration = 2436\n",
    "\n",
    "# extract params and instantiate model\n",
    "params = train.paramExtractor(resultsDf = resultsMetaDf, estimator = estimator, iteration = iteration)\n",
    "model = eval('{0}(**{1})'.format(estimator, params))\n",
    "\n",
    "model.fit(oofTrain, train.y_)\n",
    "yPred = model.predict(oofValid)\n",
    "print(sum(yPred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-21T05:29:22.737078Z",
     "start_time": "2019-04-21T05:29:22.642015Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate prediction submission file\n",
    "my_submission = pd.DataFrame({'PassengerId': dfValid.PassengerId, 'Survived': yPred})\n",
    "my_submission.to_csv('data/submission.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'https://medium.com/@rrfd/boosting-bagging-and-stacking-ensemble-methods-with-sklearn-and-mlens-a455c0c982de'\n",
    "\n",
    "from itertools import combinations\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "names = ['Random Forest', 'Extra Trees', 'KNeighbors', 'SVC', 'Ridge Classifier']\n",
    "def zip_stacked_classifiers(*args):\n",
    "    to_zip = []\n",
    "    for arg in args:\n",
    "        combined_items = sum([map(list, combinations(arg, i)) for i in range(len(arg) + 1)], [])\n",
    "        combined_items = filter(lambda x: len(x) > 0, combined_items)\n",
    "        to_zip.append(combined_items)\n",
    "    \n",
    "    return zip(to_zip[0], to_zip[1])\n",
    "stacked_clf_list = zip_stacked_classifiers(clf_array, names)\n",
    "best_combination = [0.00, \"\"]\n",
    "for clf in stacked_clf_list:\n",
    "    \n",
    "    ensemble = SuperLearner(scorer = accuracy_score, \n",
    "                            random_state = seed, \n",
    "                            folds = 10)\n",
    "    ensemble.add(clf[0])\n",
    "    ensemble.add_meta(lr)\n",
    "    ensemble.fit(X_train, y_train)\n",
    "    preds = ensemble.predict(X_test)\n",
    "    accuracy = accuracy_score(preds, y_test)\n",
    "    \n",
    "    if accuracy > best_combination[0]:\n",
    "        best_combination[0] = accuracy\n",
    "        best_combination[1] = clf[1]\n",
    "    \n",
    "    print(\"Accuracy score: {:.3f} {}\").format(accuracy, clf[1])\n",
    "print(\"\\nBest stacking model is {} with accuracy of: {:.3f}\").format(best_combination[1], best_combination[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
