{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Kaggle competition - house prices__\n",
    "\n",
    "1. [Kaggle competition - house prices](#Kaggle-competition-house-prices)\n",
    "1. [Import](#Import)\n",
    "    1. [Tools](#Tools)\n",
    "    1. [Data](#Data)    \n",
    "1. [Initial EDA](#Initial-EDA)\n",
    "    1. [Categorical feature EDA](#Categorical-feature-EDA)\n",
    "        1. [Univariate & feature vs. target](#Univariate-&-feature-vs.-target)\n",
    "    1. [Continuous feature EDA](#Continuous-feature-EDA)\n",
    "        1. [Univariate & feature vs. target](#Univariate-&-feature-vs.-target2)\n",
    "        1. [Correlation](#Correlation)\n",
    "            1. [Correlation (all samples)](#Correlation-all-samples)\n",
    "            1. [Correlation (top vs. target)](#Correlation-top-vs-target)\n",
    "        1. [Pair plot](#Pair-plot)\n",
    "    1. [Target variable evaluation](#Target-variable-evaluation)    \n",
    "1. [Data cleaning](#Data-cleaning)\n",
    "    1. [Outliers (preliminary)](#Outliers-preliminary)\n",
    "        1. [Training](#Training5)\n",
    "        1. [Validation](#Validation5)\n",
    "    1. [Missing data](#Missing-data)\n",
    "        1. [Evaluate](#Evaluate1)\n",
    "        1. [Training](#Training1)\n",
    "        1. [Validation](#Validation1)\n",
    "    1. [Engineering](#Engineering)\n",
    "        1. [Evaluate](#Evaluate3)\n",
    "        1. [Training](#Training3)\n",
    "        1. [Validation](#Validation3)\n",
    "    1. [Encoding](#Encoding)\n",
    "        1. [Evaluate](#Evaluate2)\n",
    "        1. [Training](#Training2)\n",
    "        1. [Validation](#Validation2)\n",
    "    1. [Transformation](#Transformation)\n",
    "        1. [Evaluate](#Evaluate4)\n",
    "        1. [Training](#Training4)\n",
    "        1. [Validation](#Validation4)\n",
    "    1. [Outliers (final)](#Outliers-final)\n",
    "        1. [Training](#Training6)\n",
    "1. [Data evaluation](#Data-evaluation)\n",
    "    1. [Feature importance](#Feature-importance)\n",
    "    1. [Rationality](#Rationality)\n",
    "    1. [Value override](#Value-override)\n",
    "    1. [Continuous feature EDA](#Continuous-feature-EDA3)\n",
    "        1. [Univariate & feature vs. target](#Univariate-&-feature-vs.-target3)\n",
    "        1. [Correlation](#Correlation3)\n",
    "            1. [Correlation (top vs. target)](#Correlation-top-vs-target3)\n",
    "1. [Modeling](#Modeling)\n",
    "    1. [Prepare training data](#Prepare-training-data)\n",
    "    1. [Prepare validation data](#Prepare-validation-data)\n",
    "    1. [GridSearch](#GridSearch)\n",
    "        1. [Evaluation](#Evaluation)\n",
    "        1. [Model explanability](#Model-explanability)\n",
    "            1. [Permutation importance](#Permutation-importance)\n",
    "            1. [Partial plots](#Partial-plots)\n",
    "            1. [SHAP values](#SHAP-values)\n",
    "    1. [Stacking](#Stacking)\n",
    "        1. [Primary models](#Primary-models)\n",
    "        1. [Meta model](#Meta-model)        \n",
    "1. [Submission](#Submission)\n",
    "    1. [Stack](#Stack)\n",
    "    1. [Standard](#Standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle competition - house prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Kaggle-competition-house-prices'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Import'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Tools'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T04:10:56.631284Z",
     "start_time": "2019-04-01T04:10:55.175775Z"
    }
   },
   "outputs": [],
   "source": [
    "# Standard libary and settings\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "import itertools\n",
    "import csv\n",
    "import ast\n",
    "from timeit import default_timer as timer\n",
    "global ITERATION\n",
    "import time\n",
    "rundate = time.strftime('%Y%m%d')\n",
    "comp = 'housing'\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "from IPython.core.display import display, HTML; display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "# Data extensions and settings\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold = np.inf, suppress = True)\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.options.display.float_format = '{:,.6f}'.format\n",
    "\n",
    "# Modeling extensions\n",
    "import sklearn.base as base\n",
    "import sklearn.cluster as cluster\n",
    "import sklearn.datasets as datasets\n",
    "import sklearn.decomposition as decomposition\n",
    "import sklearn.discriminant_analysis as discriminant_analysis\n",
    "import sklearn.ensemble as ensemble\n",
    "import sklearn.feature_extraction as feature_extraction\n",
    "import sklearn.feature_selection as feature_selection\n",
    "import sklearn.gaussian_process as gaussian_process\n",
    "import sklearn.linear_model as linear_model\n",
    "import sklearn.kernel_ridge as kernel_ridge\n",
    "import sklearn.metrics as metrics\n",
    "import sklearn.model_selection as model_selection\n",
    "import sklearn.naive_bayes as naive_bayes\n",
    "import sklearn.neighbors as neighbors\n",
    "import sklearn.pipeline as pipeline\n",
    "import sklearn.preprocessing as preprocessing\n",
    "import sklearn.svm as svm\n",
    "import sklearn.tree as tree\n",
    "import sklearn.utils as utils\n",
    "\n",
    "from scipy import stats, special\n",
    "import xgboost\n",
    "import lightgbm\n",
    "\n",
    "from hyperopt import hp, tpe, Trials, fmin, STATUS_OK\n",
    "from hyperopt.pyll.stochastic import sample\n",
    "\n",
    "# Visualization extensions and settings\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Custom extensions and settings\n",
    "sys.path.append('/main') if '/main' not in sys.path else None\n",
    "# sys.path.append('C:/Users/petersont/Atheneum/dev') if 'C:/Users/petersont/Atheneum/dev' not in sys.path else None\n",
    "sys.path.append('U:\\\\') if 'U:\\\\' not in sys.path else None\n",
    "\n",
    "import mlmachine as mlm\n",
    "import quickplot as qp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T04:10:56.789361Z",
     "start_time": "2019-04-01T04:10:56.634376Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load data and print dimensions\n",
    "dfTrain = pd.read_csv('data/train.csv')\n",
    "dfValid = pd.read_csv('data/test.csv')\n",
    "\n",
    "print('Training data dimensions: {}'.format(dfTrain.shape))\n",
    "print('Validation data dimensions: {}'.format(dfValid.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T04:10:56.892862Z",
     "start_time": "2019-04-01T04:10:56.791724Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display info and first 5 rows\n",
    "\n",
    "dfTrain.info()\n",
    "display(dfTrain[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-01T04:10:56.914214Z",
     "start_time": "2019-04-01T04:10:56.896199Z"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "dfTrain.dtypes.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T00:05:24.821543Z",
     "start_time": "2019-03-24T00:05:24.797085Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load training data into ML machine\n",
    "train = mlm.Machine(data = dfTrain\n",
    "                  ,target = ['SalePrice']\n",
    "                  ,removeFeatures = ['Id']                      \n",
    "                  ,overrideCat = ['MSSubClass','OverallQual','OverallCond','YearBuilt','YearRemodAdd','MoSold','YrSold']\n",
    "                  ,targetType = 'continuous'\n",
    "                )\n",
    "print(train.X_.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T00:05:24.845828Z",
     "start_time": "2019-03-24T00:05:24.825199Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load training data into ML machine\n",
    "valid = mlm.Machine(data = dfValid\n",
    "                  ,removeFeatures = ['Id']                      \n",
    "                  ,overrideCat = ['MSSubClass','OverallQual','OverallCond','YearBuilt','YearRemodAdd','MoSold','YrSold']\n",
    "                )\n",
    "print(valid.X_.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Initial-EDA'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical feature EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Categorical-feature-EDA'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate & feature vs. target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Univariate-&-feature-vs.-target'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T21:16:20.377428Z",
     "start_time": "2019-03-23T21:15:52.254252Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Categorical features\n",
    "train.edaNumTargetCatFeat()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous feature EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Continuous-feature-EDA'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate & feature vs. target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Univariate-&-feature-vs.-target2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T21:16:42.053849Z",
     "start_time": "2019-03-23T21:16:20.384806Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Continuous features\n",
    "train.edaNumTargetNumFeat()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Correlation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation (all samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Correlation-all-samples'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T21:16:43.132273Z",
     "start_time": "2019-03-23T21:16:42.063115Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# correlation heat map \n",
    "p = qp.plotter.QuickPlot(fig = plt.figure(), chartProp = 25)\n",
    "ax = p.makeCanvas()\n",
    "p.qpCorrHeatmap(df = train.X_\n",
    "                ,target = train.y_\n",
    "                ,targetLabel = train.target[0]\n",
    "                ,cols = None\n",
    "                ,annot = False\n",
    "                ,ax = ax\n",
    "               )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation (top vs. target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Correlation-top-vs-target'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T21:16:43.488796Z",
     "start_time": "2019-03-23T21:16:43.135022Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# correlation heat map with most highly correlated features relative to the target\n",
    "p = qp.plotter.QuickPlot(fig = plt.figure(), chartProp = 15)\n",
    "ax = p.makeCanvas()\n",
    "p.qpCorrHeatmapRefine(df = train.X_\n",
    "                      ,target = train.y_\n",
    "                      ,targetLabel = train.target[0]\n",
    "                      ,cols = None\n",
    "                      ,annot = False\n",
    "                      ,thresh = 0.6\n",
    "                      ,ax = ax\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Remarks - There are three pairs of highly correlated features:\n",
    "    - 'GarageArea' and 'GarageCars'\n",
    "    - 'TotRmsAbvGrd' and 'GrLivArea'\n",
    "    - '1stFlrSF' and 'TotalBsmtSF\n",
    "This makes sense, given what each feature represents and how each pair items relate to each other. We likely only need one feature from each pair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pair plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Pair-plot'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T21:17:11.737104Z",
     "start_time": "2019-03-23T21:16:43.491417Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Pair plot\n",
    "p = qp.plotter.QuickPlot(fig = plt.figure(), chartProp = 10)\n",
    "p.qpPairPlot(df = train.X_\n",
    "             ,cols = ['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF',\n",
    "                     'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GrLivArea', 'TotRmsAbvGrd', 'GarageYrBlt',\n",
    "                     'GarageArea', 'WoodDeckSF', 'OpenPorchSF'] \n",
    "             ,diag_kind = 'auto')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target variable evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Target-variable-evaluation'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T21:17:12.397415Z",
     "start_time": "2019-03-23T21:17:11.739385Z"
    }
   },
   "outputs": [],
   "source": [
    "# evaluate distribution of target variable\n",
    "train.edaTransformInitial(data = train.y_, name = train.target[0])\n",
    "train.edaTransformLog1(data = train.y_, name = train.target[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T21:17:12.407738Z",
     "start_time": "2019-03-23T21:17:12.400389Z"
    }
   },
   "outputs": [],
   "source": [
    "# log + 1 transform target\n",
    "train.y_ = np.log1p(train.y_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Data-cleaning'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers (preliminary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Outliers-preliminary'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Training5'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify outliers\n",
    "nonNull = train.X_.columns[train.X_.isnull().sum() == 0].values.tolist()\n",
    "nonNullNumCol = list(set(nonNull).intersection(train.featureByDtype_['continuous']))\n",
    "    \n",
    "trainPipe = pipeline.Pipeline([\n",
    "    ('outlier', train.OutlierIQR(outlierCount = 4, iqrStep = 2.0, features = nonNullNumCol, dropOutliers = False))     \n",
    "    ])\n",
    "train.X_ = trainPipe.transform(train.X_)\n",
    "\n",
    "np.array(sorted(trainPipe.named_steps['outlier'].outliers_))\n",
    "# train.y_ = np.delete(train.y_, trainPipe.named_steps['outlier'].outliers_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print outlier rows\n",
    "train.X_.loc[[185,197,523,635,691,1230,1298,1386]][nonNullNumCol]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display descriptive stats\n",
    "train.X_[nonNullNumCol].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "clf = ensemble.IsolationForest(behaviour = 'new'\n",
    "                        ,max_samples = train.X_.shape[0]\n",
    "                        ,random_state = 0\n",
    "                        ,contamination = 0.02\n",
    "                        )\n",
    "clf.fit(train.X_[nonNullNumCol])\n",
    "preds = clf.predict(train.X_[nonNullNumCol])\n",
    "# # np.unique(preds, return_counts = True)\n",
    "\n",
    "mask = np.isin(preds, -1)  # np.in1d if np.isin is not available\n",
    "idx = np.where(mask)\n",
    "idx[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eif as iso\n",
    "if_eif = iso.iForest(train.X_[nonNullNumCol].values\n",
    "                 ,ntrees = 100\n",
    "                 ,sample_size = 256\n",
    "                 ,ExtensionLevel = 1\n",
    "                )\n",
    "\n",
    "# calculate anomaly scores\n",
    "anomalies_ratio = 0.009\n",
    "anomaly_scores = if_eif.compute_paths(X_in = train.X_[nonNullNumCol].values)\n",
    "anomaly_scores_sorted = np.argsort(anomaly_scores)\n",
    "indices_with_preds = anomaly_scores_sorted[-int(np.ceil(anomalies_ratio * train.X_.shape[0])):]\n",
    "sorted(indices_with_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removeIx = train.X_[train.X_['LotArea'] > 60000].index.values.tolist() + \\\n",
    "            train.X_[train.X_['LotFrontage'] > 300].index.values.tolist() + \\\n",
    "            train.X_[train.X_['GrLivArea'] > 4000].index.values.tolist()\n",
    "sorted(removeIx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Validation5'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify outliers\n",
    "nonNull = valid.X_.columns[valid.X_.isnull().sum() == 0].values.tolist()\n",
    "nonNullNumCol = list(set(nonNull).intersection(valid.featureByDtype_['continuous']))\n",
    "    \n",
    "validPipe = pipeline.Pipeline([\n",
    "    ('outlier', valid.OutlierIQR(outlierCount = 4, iqrStep = 2.0, features = nonNullNumCol, dropOutliers = False))     \n",
    "    ])\n",
    "valid.X_ = validPipe.transform(valid.X_)\n",
    "\n",
    "np.array(sorted(validPipe.named_steps['outlier'].outliers_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print outlier rows\n",
    "valid.X_.loc[[1089, 1097]][nonNullNumCol]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display descriptive stats\n",
    "valid.X_[nonNullNumCol].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "clf = ensemble.IsolationForest(behaviour = 'new'\n",
    "                        ,max_samples = valid.X_.shape[0]\n",
    "                        ,random_state = 0\n",
    "                        ,contamination = 0.02\n",
    "                        )\n",
    "clf.fit(valid.X_[nonNullNumCol])\n",
    "preds = clf.predict(valid.X_[nonNullNumCol])\n",
    "# # np.unique(preds, return_counts = True)\n",
    "\n",
    "mask = np.isin(preds, -1)  # np.in1d if np.isin is not available\n",
    "idx = np.where(mask)\n",
    "idx[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eif as iso\n",
    "if_eif = iso.iForest(valid.X_[nonNullNumCol].values\n",
    "                 ,ntrees = 100\n",
    "                 ,sample_size = 256\n",
    "                 ,ExtensionLevel = 1\n",
    "                )\n",
    "\n",
    "# calculate anomaly scores\n",
    "anomalies_ratio = 0.009\n",
    "anomaly_scores = if_eif.compute_paths(X_in = valid.X_[nonNullNumCol].values)\n",
    "anomaly_scores_sorted = np.argsort(anomaly_scores)\n",
    "indices_with_preds = anomaly_scores_sorted[-int(np.ceil(anomalies_ratio * valid.X_.shape[0])):]\n",
    "sorted(indices_with_preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing data\n",
    "\n",
    "-__MCAR__ - Completely unsystematic missingness, completely unralted to any of the other variables. simple imputation of mean, median or mode is most acceptable for this type of missingness.\n",
    "\n",
    "-__MAR__ - The nature of the missing data is related to observed data in other variables, not the missing data. The missing data is conditional on some other variable.  For example, men are more likely to tell you their weight than woemn. The missingness of weight has to do with gender.\n",
    "\n",
    "-__MNAR__ - There is a relationship between the propensity of a value to be missing and its values. For example, the wealthiest people choosing not to state their income.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Missing-data'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Evaluate1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T21:17:12.813934Z",
     "start_time": "2019-03-23T21:17:12.412215Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate missing data\n",
    "train.edaMissingSummary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T21:17:13.238442Z",
     "start_time": "2019-03-23T21:17:12.817453Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate missing data\n",
    "valid.edaMissingSummary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T21:17:13.273030Z",
     "start_time": "2019-03-23T21:17:13.254418Z"
    }
   },
   "outputs": [],
   "source": [
    "# compare feature with missing data\n",
    "train.missingColCompare(train.X_, valid.X_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T21:17:13.249562Z",
     "start_time": "2019-03-23T21:17:13.243069Z"
    }
   },
   "outputs": [],
   "source": [
    "# missingdata_df = merged_df.columns[merged_df.isnull().any()].tolist()\n",
    "# msno.matrix(merged_df[missingdata_df])\n",
    "\n",
    "# msno.bar(merged_df[missingdata_df], color=\"blue\", log=True, figsize=(30,18))\n",
    "\n",
    "# # \n",
    "# msno.heatmap(merged_df[missingdata_df], figsize=(20,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Training1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T21:17:13.366430Z",
     "start_time": "2019-03-23T21:17:13.275386Z"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "trainPipe = pipeline.Pipeline([\n",
    "    ('imputeConstantCat', train.ConstantImputer(cols = ['PoolQC','Alley','Fence','FireplaceQu','GarageType','GarageFinish','GarageQual','MiscFeature'\n",
    "                                              ,'GarageCond','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','MasVnrType']\n",
    "                                      ,fill = 'Nonexistent'))    \n",
    "    ,('imputeConstantNum', train.ConstantImputer(cols = ['GarageYrBlt','MasVnrArea'], fill = 0)) \n",
    "    ,('imputeMode', train.ModeImputer(cols = ['Electrical']))\n",
    "    ,('imputeContext', train.ContextImputer(nullCol = 'LotFrontage', contextCol = 'Neighborhood', strategy = 'mean'))\n",
    "    ])\n",
    "train.X_ = trainPipe.transform(train.X_)\n",
    "train.edaMissingSummary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Validation1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T21:17:13.671696Z",
     "start_time": "2019-03-23T21:17:13.577524Z"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "validPipe = pipeline.Pipeline([\n",
    "    ('imputeConstantCat', valid.ConstantImputer(cols = ['PoolQC','Alley','Fence','FireplaceQu','GarageType','GarageFinish','GarageQual','MiscFeature'\n",
    "                                                      ,'GarageCond','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','MasVnrType']\n",
    "                                              ,fill = 'Nonexistent'))    \n",
    "    ,('imputeConstantNum', valid.ConstantImputer(cols = ['GarageYrBlt','MasVnrArea','BsmtUnfSF','GarageArea','BsmtFinSF1','TotalBsmtSF','BsmtFinSF2']\n",
    "                                                ,fill = 0)) \n",
    "    ,('imputeModeCat', valid.ModeImputer(cols = ['Functional','SaleType','Exterior1st','MSZoning','Exterior2nd','KitchenQual','Utilities']))\n",
    "    ,('imputeModeNum', valid.NumericalImputer(cols = ['BsmtHalfBath','GarageCars','BsmtFullBath']\n",
    "                                             ,strategy = 'most_frequent'))\n",
    "    ,('imputeContext', valid.ContextImputer(nullCol = 'LotFrontage', contextCol = 'Neighborhood', strategy = 'mean'\n",
    "                                          ,train = False, trainDf = trainPipe.named_steps['imputeContext'].fillDf))\n",
    "    ])\n",
    "valid.X_ = validPipe.transform(valid.X_)\n",
    "valid.edaMissingSummary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Engineering'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Evaluate3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Training3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T21:17:16.007737Z",
     "start_time": "2019-03-23T21:17:15.989331Z"
    }
   },
   "outputs": [],
   "source": [
    "# additional features\n",
    "train.X_['BsmtFinSF'] = train.X_['BsmtFinSF1'] + train.X_['BsmtFinSF2']\n",
    "train.X_['TotalSF'] = train.X_['TotalBsmtSF'] + train.X_['1stFlrSF'] + train.X_['2ndFlrSF']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Validation3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional features\n",
    "valid.X_['BsmtFinSF'] = valid.X_['BsmtFinSF1'] + valid.X_['BsmtFinSF2']\n",
    "valid.X_['TotalSF'] = valid.X_['TotalBsmtSF'] + valid.X_['1stFlrSF'] + valid.X_['2ndFlrSF']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Encoding'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Evaluate2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# counts of unique values in training data string columns\n",
    "train.X_[train.featureByDtype_['categorical']].apply(pd.Series.nunique, axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print unique values in each categorical columns\n",
    "for col in train.X_[train.featureByDtype_['categorical']]:\n",
    "    print(col, np.unique(train.X_[col]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# counts of unique values in validation data string columns\n",
    "valid.X_[valid.featureByDtype_['categorical']].apply(pd.Series.nunique, axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print unique values in each categorical columns\n",
    "for col in valid.X_[valid.featureByDtype_['categorical']]:\n",
    "    if col not in ['Name','Cabin']:\n",
    "        print(col, np.unique(valid.X_[col]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T21:17:13.968858Z",
     "start_time": "2019-03-23T21:17:13.896211Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "for col in train.featureByDtype_['categorical']:\n",
    "    trainValues = train.X_[col].unique()\n",
    "    validValues = valid.X_[col].unique()\n",
    "    \n",
    "    trainDiff = set(trainValues) - set(validValues)\n",
    "    validDiff = set(validValues) - set(trainValues)\n",
    "    \n",
    "    if len(trainDiff) > 0 or len(validDiff) > 0:\n",
    "        print('\\n\\n*** ' + col)\n",
    "        print('Value present in training data, not in validation data')\n",
    "        print(trainDiff)\n",
    "        print('Value present in validation data, not in training data')\n",
    "        print(validDiff)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Training2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### ordinal columns\n",
    "ordinalEncodings = {\n",
    "    'Street' : {'Grvl' : 0, 'Pave' : 1}\n",
    "    ,'Alley' : {'Nonexistent' : 0, 'Grvl' : 1, 'Pave' : 2}\n",
    "    ,'LotShape' : {'IR3' : 0, 'IR2' : 1, 'IR1' : 2, 'Reg' : 3}\n",
    "    ,'Utilities' : {'ELO' : 0, 'NoSeWa' : 1, 'NoSewr' : 2, 'AllPub' : 3}\n",
    "    ,'LotConfig' : {'FR3' : 0, 'FR2' : 1, 'Corner' : 2, 'Inside' : 3, 'CulDSac' : 4}\n",
    "    ,'LandSlope' : {'Sev' : 0, 'Mod' : 1, 'Gtl' : 2}\n",
    "    ,'ExterQual' : {'Po' : 0, 'Fa' : 1, 'TA' : 2, 'Gd' : 3, 'Ex' : 4}\n",
    "    ,'ExterCond' : {'Po' : 0, 'Fa' : 1, 'TA' : 2, 'Gd' : 3, 'Ex' : 4}\n",
    "    ,'BsmtQual' : {'Nonexistent' : 0, 'Po' : 1, 'Fa' : 2, 'TA' : 3, 'Gd' : 4, 'Ex' : 5}\n",
    "    ,'BsmtCond' : {'Nonexistent' : 0, 'Po' : 1, 'Fa' : 2, 'TA' : 3, 'Gd' : 4, 'Ex' : 5}\n",
    "    ,'BsmtExposure' : {'Nonexistent' : 0, 'No' : 1, 'Mn' : 2, 'Av' : 3, 'Gd' : 4}    \n",
    "    ,'BsmtFinType1' : {'Nonexistent' : 0, 'Unf' : 1, 'LwQ' : 2, 'BLQ' : 3, 'Rec' : 4, 'ALQ' : 5, 'GLQ' : 6} #split?\n",
    "    ,'BsmtFinType2' : {'Nonexistent' : 0, 'Unf' : 1, 'LwQ' : 2, 'BLQ' : 3, 'Rec' : 4, 'ALQ' : 5, 'GLQ' : 6} #split?\n",
    "    ,'HeatingQC' : {'Po' : 0, 'Fa' : 1, 'TA' : 2, 'Gd' : 3, 'Ex' : 4}    \n",
    "    ,'CentralAir' : {'N' : 0, 'Y' : 1}\n",
    "    ,'Electrical' : {'FuseP' : 0, 'FuseF' : 1, 'FuseA' : 2, 'Mix' : 3, 'SBrkr' : 4}\n",
    "    ,'KitchenQual' : {'Po' : 0, 'Fa' : 1, 'TA' : 2, 'Gd' : 3, 'Ex' : 4}\n",
    "    ,'Functional' : {'Sal' : 0, 'Sev' : 1, 'Maj2' : 2, 'Maj1' : 3, 'Mod' : 4, 'Min2' : 5, 'Min1' : 6, 'Typ' : 7}\n",
    "    ,'FireplaceQu' : {'Nonexistent' : 0, 'Po' : 1, 'Fa' : 2, 'TA' : 3, 'Gd' : 4, 'Ex' : 5}\n",
    "    ,'GarageFinish' : {'Nonexistent' : 0, 'Unf' : 1, 'RFn' : 2, 'Fin' : 3}\n",
    "    ,'GarageQual' : {'Nonexistent' : 0, 'Po' : 1, 'Fa' : 2, 'TA' : 3, 'Gd' : 4, 'Ex' : 5}\n",
    "    ,'GarageCond' : {'Nonexistent' : 0, 'Po' : 1, 'Fa' : 2, 'TA' : 3, 'Gd' : 4, 'Ex' : 5}\n",
    "    ,'PavedDrive' : {'N' : 0, 'P' : 1, 'Y' : 2}\n",
    "    ,'PoolQC' : {'Nonexistent' : 0, 'Fa' : 1, 'TA' : 2, 'Gd' : 3, 'Ex' : 4}\n",
    "}\n",
    "\n",
    "### nominal columns\n",
    "nomCatCols = ['MSSubClass','MSZoning','LandContour','Neighborhood','Condition1','Condition2','BldgType','HouseStyle','RoofStyle','RoofMatl','Exterior1st'\n",
    "             ,'Exterior2nd','MasVnrType','Foundation','Heating','GarageType','Fence','SaleType','SaleCondition','MiscFeature']\n",
    "\n",
    "trainPipe = pipeline.Pipeline([\n",
    "    ('encodeOrdinal', train.CustomOrdinalEncoder(encodings = ordinalEncodings))    \n",
    "    ,('dummyNominal', train.Dummies(cols = nomCatCols, dropFirst = False))\n",
    "    ])\n",
    "train.X_ = trainPipe.transform(train.X_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Validation2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### pre-processing pipeline\n",
    "validPipe = pipeline.Pipeline([\n",
    "    ('encodeOrdinal', valid.CustomOrdinalEncoder(encodings = ordinalEncodings))    \n",
    "    ,('dummyNominal', valid.Dummies(cols = nomCatCols, dropFirst = False))\n",
    "    ,('levels', valid.MissingDummies(trainCols = train.X_.columns))\n",
    "    ])\n",
    "valid.X_ = validPipe.transform(valid.X_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Transformation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Evaluate4'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T21:17:16.158627Z",
     "start_time": "2019-03-23T21:17:16.009720Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate skew of continuous features\n",
    "train.skewSummary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T21:17:16.158627Z",
     "start_time": "2019-03-23T21:17:16.009720Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate skew of continuous features\n",
    "valid.skewSummary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Training4'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T21:17:16.834716Z",
     "start_time": "2019-03-23T21:17:16.160834Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "trainPipe = pipeline.Pipeline([\n",
    "    ('skew', train.SkewTransform(cols = train.featureByDtype_['continuous'], skewMin = 0.75, pctZeroMax = 1.0))\n",
    "    ])\n",
    "train.X_ = trainPipe.transform(train.X_)\n",
    "train.skewSummary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Validation4'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "validPipe = pipeline.Pipeline([\n",
    "    ('skew', valid.SkewTransform(train = False, trainDict = trainPipe.named_steps['skew'].colValueDict_))\n",
    "    ])\n",
    "valid.X_ = validPipe.transform(valid.X_)\n",
    "valid.skewSummary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers (final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Outliers-final'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Training6'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T21:17:15.033363Z",
     "start_time": "2019-03-23T21:17:14.698318Z"
    }
   },
   "outputs": [],
   "source": [
    "# review scatter plot identified in pair plot\n",
    "p = qp.plotter.QuickPlot(fig = plt.figure(), chartProp = 10)\n",
    "ax = p.makeCanvas(title = '', xLabel = '', yLabel = ''\n",
    "                  ,yShift = 0.8, position = 111)\n",
    "p.qp2dScatter(x = 'LotArea' \n",
    "              ,y = 'LotFrontage' \n",
    "              ,df = train.X_\n",
    "              ,ax = ax\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T21:17:15.395533Z",
     "start_time": "2019-03-23T21:17:15.038679Z"
    }
   },
   "outputs": [],
   "source": [
    "# review scatter plot identified in pair plot\n",
    "p = qp.plotter.QuickPlot(fig = plt.figure(), chartProp = 10)\n",
    "ax = p.makeCanvas(title = '', xLabel = '', yLabel = ''\n",
    "                  ,yShift = 0.8, position = 111)\n",
    "p.qp2dScatter(x = 'GrLivArea'\n",
    "              ,y = 'LotFrontage'\n",
    "              ,df = train.X_\n",
    "              ,ax = ax\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T21:17:15.441448Z",
     "start_time": "2019-03-23T21:17:15.398289Z"
    }
   },
   "outputs": [],
   "source": [
    "# save indexes\n",
    "removeIx = train.X_[train.X_['LotArea'] > 60000].index.values.tolist() + \\\n",
    "            train.X_[train.X_['LotFrontage'] > 300].index.values.tolist() + \\\n",
    "            train.X_[train.X_['GrLivArea'] > 4000].index.values.tolist()\n",
    "\n",
    "# remove entire observation \n",
    "train.X_.drop(train.X_[train.X_['GrLivArea'] > 4000].index.values, inplace = True)\n",
    "train.X_.drop(train.X_[train.X_['LotFrontage'] > 300].index.values, inplace = True)\n",
    "train.X_.drop(train.X_[train.X_['LotArea'] > 60000].index.values, inplace = True)\n",
    "\n",
    "# remove corresponding labels\n",
    "removeIx = np.array(list(set(removeIx)))\n",
    "train.y_ = np.delete(train.y_, removeIx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T21:17:15.706380Z",
     "start_time": "2019-03-23T21:17:15.443489Z"
    }
   },
   "outputs": [],
   "source": [
    "# review scatter plot post-removal\n",
    "p = qp.plotter.QuickPlot(fig = plt.figure(), chartProp = 10)\n",
    "ax = p.makeCanvas(title = '', xLabel = '', yLabel = ''\n",
    "                  ,yShift = 0.8, position = 111)\n",
    "p.qp2dScatter(x = 'LotArea'\n",
    "              ,y = 'LotFrontage'\n",
    "              ,df = train.X_\n",
    "              ,ax = ax\n",
    "             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T21:17:15.985983Z",
     "start_time": "2019-03-23T21:17:15.714757Z"
    }
   },
   "outputs": [],
   "source": [
    "# review scatter plot post-removal\n",
    "p = qp.plotter.QuickPlot(fig = plt.figure(), chartProp = 10)\n",
    "ax = p.makeCanvas(title = '', xLabel = '', yLabel = ''\n",
    "                  ,yShift = 0.8, position = 111)\n",
    "p.qp2dScatter(x = 'GrLivArea'\n",
    "              ,y = 'LotFrontage'\n",
    "              ,df = train.X_\n",
    "              ,ax = ax\n",
    "             )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Data evaluation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Feature-importance'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T21:17:41.625912Z",
     "start_time": "2019-03-23T21:17:41.437432Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# feature importance summary table\n",
    "featureImp = train.featureImportanceSummary()\n",
    "featureImp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rationality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Rationality'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T21:17:14.684705Z",
     "start_time": "2019-03-23T21:17:13.970768Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# percent difference summary\n",
    "dfDiff = abs((((valid.X_.describe() + 1) - (train.X_.describe() + 1)) / (train.X_.describe() + 1)) * 100)\n",
    "dfDiff = dfDiff[dfDiff.columns].replace({0 : np.nan})\n",
    "dfDiff[dfDiff < 0] = np.nan\n",
    "dfDiff = dfDiff.fillna('')\n",
    "display(dfDiff)\n",
    "display(train.X_.describe())\n",
    "display(valid.X_.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value override"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Value override'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T21:17:14.694855Z",
     "start_time": "2019-03-23T21:17:14.687948Z"
    }
   },
   "outputs": [],
   "source": [
    "# change clearly erroneous value to what it probably was\n",
    "valid.X_['GarageYrBlt'].replace({2207 : 2007}, inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous feature EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Continuous-feature-EDA3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate & feature vs. target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Univariate-&-feature-vs.-target3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T21:17:39.962078Z",
     "start_time": "2019-03-23T21:17:16.837033Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Continuous features\n",
    "train.edaNumTargetNumFeat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T21:17:39.962078Z",
     "start_time": "2019-03-23T21:17:16.837033Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Continuous features\n",
    "train.edaNumTargetNumFeat()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Correlation3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation (top vs. target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Correlation-top-vs-target3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T21:17:41.435071Z",
     "start_time": "2019-03-23T21:17:41.113719Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# correlation heat map with most highly correlated features relative to the target\n",
    "p = qp.plotter.QuickPlot(fig = plt.figure(), chartProp = 15)\n",
    "ax = p.makeCanvas()\n",
    "p.qpCorrHeatmapRefine(df = train.X_\n",
    "                      ,target = train.y_\n",
    "                      ,targetLabel = train.target[0]\n",
    "                      ,cols = None\n",
    "                      ,annot = False\n",
    "                      ,thresh = 0.4\n",
    "                      ,ax = ax\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Remarks - There are three pairs of highly correlated features:\n",
    "    - 'GarageArea' and 'GarageCars'\n",
    "    - 'TotRmsAbvGrd' and 'GrLivArea'\n",
    "    - '1stFlrSF' and 'TotalBsmtSF\n",
    "This makes sense, given what each feature represents and how each pair items relate to each other. We likely only need one feature from each pair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Modeling'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Prepare-training-data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T00:05:55.327051Z",
     "start_time": "2019-03-24T00:05:50.485821Z"
    }
   },
   "outputs": [],
   "source": [
    "# import training data\n",
    "dfTrain = pd.read_csv('data/train.csv')\n",
    "train = mlm.Machine(data = dfTrain\n",
    "                  ,target = ['SalePrice']\n",
    "                  ,removeFeatures = ['Id','MiscVal']\n",
    "                  ,overrideCat = ['MSSubClass','OverallQual','OverallCond','YearBuilt','YearRemodAdd','MoSold','YrSold']\n",
    "                  ,targetType = 'continuous'\n",
    "                )\n",
    "\n",
    "### training data transformation pipeline\n",
    "### ordinal columns\n",
    "ordinalEncodings = {\n",
    "    'Street' : {'Grvl' : 0, 'Pave' : 1}\n",
    "    ,'Alley' : {'Nonexistent' : 0, 'Grvl' : 1, 'Pave' : 2}\n",
    "    ,'LotShape' : {'IR3' : 0, 'IR2' : 1, 'IR1' : 2, 'Reg' : 3}\n",
    "    ,'Utilities' : {'ELO' : 0, 'NoSeWa' : 1, 'NoSewr' : 2, 'AllPub' : 3}\n",
    "    ,'LotConfig' : {'FR3' : 0, 'FR2' : 1, 'Corner' : 2, 'Inside' : 3, 'CulDSac' : 4}\n",
    "    ,'LandSlope' : {'Sev' : 0, 'Mod' : 1, 'Gtl' : 2}\n",
    "    ,'ExterQual' : {'Po' : 0, 'Fa' : 1, 'TA' : 2, 'Gd' : 3, 'Ex' : 4}\n",
    "    ,'ExterCond' : {'Po' : 0, 'Fa' : 1, 'TA' : 2, 'Gd' : 3, 'Ex' : 4}\n",
    "    ,'BsmtQual' : {'Nonexistent' : 0, 'Po' : 1, 'Fa' : 2, 'TA' : 3, 'Gd' : 4, 'Ex' : 5}\n",
    "    ,'BsmtCond' : {'Nonexistent' : 0, 'Po' : 1, 'Fa' : 2, 'TA' : 3, 'Gd' : 4, 'Ex' : 5}\n",
    "    ,'BsmtExposure' : {'Nonexistent' : 0, 'No' : 1, 'Mn' : 2, 'Av' : 3, 'Gd' : 4}    \n",
    "    ,'BsmtFinType1' : {'Nonexistent' : 0, 'Unf' : 1, 'LwQ' : 2, 'BLQ' : 3, 'Rec' : 4, 'ALQ' : 5, 'GLQ' : 6} #split?\n",
    "    ,'BsmtFinType2' : {'Nonexistent' : 0, 'Unf' : 1, 'LwQ' : 2, 'BLQ' : 3, 'Rec' : 4, 'ALQ' : 5, 'GLQ' : 6} #split?\n",
    "    ,'HeatingQC' : {'Po' : 0, 'Fa' : 1, 'TA' : 2, 'Gd' : 3, 'Ex' : 4}    \n",
    "    ,'CentralAir' : {'N' : 0, 'Y' : 1}\n",
    "    ,'Electrical' : {'FuseP' : 0, 'FuseF' : 1, 'FuseA' : 2, 'Mix' : 3, 'SBrkr' : 4}\n",
    "    ,'KitchenQual' : {'Po' : 0, 'Fa' : 1, 'TA' : 2, 'Gd' : 3, 'Ex' : 4}\n",
    "    ,'Functional' : {'Sal' : 0, 'Sev' : 1, 'Maj2' : 2, 'Maj1' : 3, 'Mod' : 4, 'Min2' : 5, 'Min1' : 6, 'Typ' : 7}\n",
    "    ,'FireplaceQu' : {'Nonexistent' : 0, 'Po' : 1, 'Fa' : 2, 'TA' : 3, 'Gd' : 4, 'Ex' : 5}\n",
    "    ,'GarageFinish' : {'Nonexistent' : 0, 'Unf' : 1, 'RFn' : 2, 'Fin' : 3}\n",
    "    ,'GarageQual' : {'Nonexistent' : 0, 'Po' : 1, 'Fa' : 2, 'TA' : 3, 'Gd' : 4, 'Ex' : 5}\n",
    "    ,'GarageCond' : {'Nonexistent' : 0, 'Po' : 1, 'Fa' : 2, 'TA' : 3, 'Gd' : 4, 'Ex' : 5}\n",
    "    ,'PavedDrive' : {'N' : 0, 'P' : 1, 'Y' : 2}\n",
    "    ,'PoolQC' : {'Nonexistent' : 0, 'Fa' : 1, 'TA' : 2, 'Gd' : 3, 'Ex' : 4}\n",
    "}\n",
    "\n",
    "### nominal columns\n",
    "nomCatCols = ['MSSubClass','MSZoning','LandContour','Neighborhood','Condition1','Condition2','BldgType','HouseStyle','RoofStyle','RoofMatl','Exterior1st'\n",
    "             ,'Exterior2nd','MasVnrType','Foundation','Heating','GarageType','Fence','SaleType','SaleCondition','MiscFeature']\n",
    "\n",
    "### additional features\n",
    "train.X_['BsmtFinSF'] = train.X_['BsmtFinSF1'] + train.X_['BsmtFinSF2']\n",
    "train.X_['TotalSF'] = train.X_['TotalBsmtSF'] + train.X_['1stFlrSF'] + train.X_['2ndFlrSF']\n",
    "\n",
    "### observation removal\n",
    "# save indexes\n",
    "removeIx = train.X_[train.X_['LotArea'] > 60000].index.values.tolist() + \\\n",
    "            train.X_[train.X_['LotFrontage'] > 300].index.values.tolist() + \\\n",
    "            train.X_[train.X_['GrLivArea'] > 4000].index.values.tolist()\n",
    "\n",
    "# remove entire observation \n",
    "train.X_.drop(train.X_[train.X_['GrLivArea'] > 4000].index.values, inplace = True)\n",
    "train.X_.drop(train.X_[train.X_['LotFrontage'] > 300].index.values, inplace = True)\n",
    "train.X_.drop(train.X_[train.X_['LotArea'] > 60000].index.values, inplace = True)\n",
    "\n",
    "# remove corresponding labels\n",
    "removeIx = np.array(list(set(removeIx)))\n",
    "train.y_ = np.delete(train.y_, removeIx)\n",
    "\n",
    "### pre-processing pipeline\n",
    "trainPipe = pipeline.Pipeline([\n",
    "    ('imputeConstantCat', train.ConstantImputer(cols = ['PoolQC','Alley','Fence','FireplaceQu','GarageType','GarageFinish','GarageQual','MiscFeature'\n",
    "                                              ,'GarageCond','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','MasVnrType']\n",
    "                                      ,fill = 'Nonexistent'))    \n",
    "    ,('imputeConstantNum', train.ConstantImputer(cols = ['GarageYrBlt','MasVnrArea'], fill = 0)) \n",
    "    ,('imputeMode', train.ModeImputer(cols = ['Electrical']))\n",
    "    ,('imputeContext', train.ContextImputer(nullCol = 'LotFrontage', contextCol = 'Neighborhood', strategy = 'mean'))\n",
    "    ,('encodeOrdinal', train.CustomOrdinalEncoder(encodings = ordinalEncodings))    \n",
    "    ,('dummyNominal', train.Dummies(cols = nomCatCols, dropFirst = False))\n",
    "    ,('skew', train.SkewTransform(cols = train.featureByDtype_['continuous'], skewMin = 0.75, pctZeroMax = 1.0))\n",
    "    ,('scale', train.Robust(cols = 'non-binary'))\n",
    "    ])\n",
    "train.X_ = trainPipe.transform(train.X_)\n",
    "\n",
    "train.y_ = np.log1p(train.y_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Prepare-validation-data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T00:05:55.400797Z",
     "start_time": "2019-03-24T00:05:55.329686Z"
    }
   },
   "outputs": [],
   "source": [
    "# import valid data\n",
    "dfValid = pd.read_csv('data/test.csv')\n",
    "valid = mlm.Machine(data = dfValid\n",
    "                  ,removeFeatures = ['Id','MiscVal']\n",
    "                  ,overrideCat = ['MSSubClass','OverallQual','OverallCond','YearBuilt','YearRemodAdd','MoSold','YrSold']\n",
    "                  ,targetType = 'continuous'\n",
    "                )\n",
    "\n",
    "### additional features\n",
    "valid.X_['BsmtFinSF'] = valid.X_['BsmtFinSF1'] + valid.X_['BsmtFinSF2']\n",
    "valid.X_['TotalSF'] = valid.X_['TotalBsmtSF'] + valid.X_['1stFlrSF'] + valid.X_['2ndFlrSF']\n",
    "valid.X_.loc[valid.X_['TotalSF'].isnull(), 'TotalSF'] = valid.X_['1stFlrSF'] + valid.X_['2ndFlrSF']\n",
    "\n",
    "### pre-processing pipeline\n",
    "validPipe = pipeline.Pipeline([\n",
    "    ('imputeConstantCat', valid.ConstantImputer(cols = ['PoolQC','Alley','Fence','FireplaceQu','GarageType','GarageFinish','GarageQual','MiscFeature'\n",
    "                                                      ,'GarageCond','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','MasVnrType']\n",
    "                                              ,fill = 'Nonexistent'))    \n",
    "    ,('imputeConstantNum', valid.ConstantImputer(cols = ['GarageYrBlt','MasVnrArea','BsmtUnfSF','GarageArea','BsmtFinSF1','TotalBsmtSF','BsmtFinSF2']\n",
    "                                                ,fill = 0)) \n",
    "    ,('imputeModeCat', valid.ModeImputer(cols = ['Functional','SaleType','Exterior1st','MSZoning','Exterior2nd','KitchenQual','Utilities']))\n",
    "    ,('imputeModeNum', valid.NumericalImputer(cols = ['BsmtHalfBath','GarageCars','BsmtFullBath']\n",
    "                                             ,strategy = 'most_frequent'))\n",
    "    ,('imputeContext', valid.ContextImputer(nullCol = 'LotFrontage', contextCol = 'Neighborhood', strategy = 'mean'\n",
    "                                          ,train = False, trainDf = trainPipe.named_steps['imputeContext'].fillDf))    \n",
    "    ,('encodeOrdinal', valid.CustomOrdinalEncoder(encodings = ordinalEncodings))    \n",
    "    ,('dummyNominal', valid.Dummies(cols = nomCatCols, dropFirst = False))\n",
    "    ,('skew', valid.SkewTransform(cols = valid.featureByDtype_['continuous'], train = False\n",
    "                                 ,trainDict = trainPipe.named_steps['skew'].colValueDict_))\n",
    "    ,('scale', valid.Robust(cols = 'non-binary', train = False\n",
    "                                 ,trainDict = trainPipe.named_steps['scale'].colValueDict_))    \n",
    "    ,('levels', valid.MissingDummies(trainCols = train.X_.columns))    \n",
    "    ])\n",
    "valid.X_ = validPipe.transform(valid.X_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'GridSearch'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T00:05:55.681240Z",
     "start_time": "2019-03-24T00:05:55.664460Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# set variables\n",
    "scoring = 'neg_mean_squared_error'\n",
    "n_folds = 2\n",
    "n_jobs = 1\n",
    "verbose = 2\n",
    "ITERATION = 0\n",
    "iters = 1\n",
    "\n",
    "\n",
    "# set optimization parameters\n",
    "def objective(space, model = '', X = train.X_, y = train.y_, scoring = scoring, n_folds = n_folds, n_jobs = n_jobs, verbose = verbose, invertLoss = False):\n",
    "    global ITERATION\n",
    "    ITERATION += 1\n",
    "    start = timer()\n",
    "    \n",
    "    # convert select float params to int\n",
    "    for param in ['num_leaves', 'subsample_for_bin', 'min_child_samples']:\n",
    "        if param in space.keys():\n",
    "            space[param] = int(space[param])\n",
    "    \n",
    "    print(space)\n",
    "    \n",
    "    cv = model_selection.cross_val_score(estimator = eval('{0}(**{1})'.format(model, space))\n",
    "                                         ,X = X\n",
    "                                         ,y = y\n",
    "                                         ,verbose = verbose\n",
    "                                         ,n_jobs = n_jobs\n",
    "                                         ,cv = n_folds\n",
    "                                         ,scoring = scoring)\n",
    "    run_time = timer() - start\n",
    "    \n",
    "    # Extract the best score\n",
    "    if invertLoss:\n",
    "        loss = 1 - cv.mean()\n",
    "    else:\n",
    "        loss = cv.mean()\n",
    "    \n",
    "    # export results to CSV\n",
    "    out_file = 'data/{}_{}.csv'.format(rundate, comp)\n",
    "    with open(out_file, 'a', newline = '') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([ITERATION, model, space, loss, cv.min(), cv.mean(), cv.max(), cv.std(), run_time, STATUS_OK]) \n",
    "\n",
    "    return {'iteration' : ITERATION\n",
    "            ,'estimator' : model\n",
    "            ,'params' : space\n",
    "            ,'loss' : loss                   # required\n",
    "            ,'min' : cv.min()\n",
    "            ,'mean' : cv.mean()\n",
    "            ,'max' : cv.max()\n",
    "            ,'std' : cv.std()\n",
    "            ,'train_time' : run_time\n",
    "            ,'status' : STATUS_OK}          # required\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T08:30:54.433713Z",
     "start_time": "2019-03-24T00:05:55.686470Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " # parameter space\n",
    "allSpace = {\n",
    "            'linear_model.Lasso' :{\n",
    "                'alpha' :  hp.uniform('alpha', 0.0001, 100)\n",
    "            }\n",
    "            ,'linear_model.Ridge' :{\n",
    "                'alpha' :  hp.uniform('alpha', 0.0001, 100)\n",
    "            }\n",
    "            ,'linear_model.ElasticNet' :{\n",
    "                'alpha' :  hp.uniform('alpha', 0.0001, 100)\n",
    "                ,'l1_ratio' :  hp.uniform('l1_ratio', 0.0, 1.0)\n",
    "            }\n",
    "            ,'kernel_ridge.KernelRidge' :{\n",
    "                'alpha' :  hp.uniform('alpha', 0.0001, 100)\n",
    "                ,'kernel' : hp.choice('kernel', ['linear', 'polynomial', 'rbf'])\n",
    "                ,'degree' : hp.choice('degree', [2,3])\n",
    "                ,'gamma' : hp.quniform('gamma', 0.0, 10, 0.05)\n",
    "            }\n",
    "            ,'lightgbm.LGBMRegressor' : {\n",
    "                'colsample_bytree': hp.quniform('colsample_bytree', 0.5, 1.0, 0.05)\n",
    "                ,'boosting_type' : hp.choice('boosting_type', ['gbdt', 'dart', 'goss'])\n",
    "                #,'boosting_type': hp.choice('boosting_type'\n",
    "                #                    ,[{'boosting_type': 'gbdt', 'subsample': hp.uniform('gdbt_subsample', 0.5, 1)}\n",
    "                #                    ,{'boosting_type': 'dart', 'subsample': hp.uniform('dart_subsample', 0.5, 1)}\n",
    "                #                    ,{'boosting_type': 'goss', 'subsample': 1.0}])\n",
    "                ,'learning_rate' : hp.quniform('learning_rate', 0.01, 0.2, 0.01)\n",
    "                ,'max_depth' : hp.choice('max_depth', np.arange(2, 20, dtype = int))\n",
    "                ,'min_child_samples' : hp.quniform('min_child_samples', 20, 500, 5)\n",
    "                ,'n_estimators' : hp.choice('n_estimators', np.arange(100, 10000, 10, dtype = int))\n",
    "                ,'num_leaves': hp.quniform('num_leaves', 8, 150, 1)\n",
    "                ,'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0)\n",
    "                ,'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0)\n",
    "                ,'subsample_for_bin': hp.quniform('subsample_for_bin', 20000, 300000, 20000)                    \n",
    "            }\n",
    "            ,'xgboost.XGBRegressor' : {\n",
    "                'colsample_bytree' : hp.quniform('colsample_bytree', 0.5, 1.0, 0.05)\n",
    "                ,'gamma' : hp.quniform('gamma', 0.0, 10, 0.05)\n",
    "                ,'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0)\n",
    "                ,'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0)\n",
    "                ,'learning_rate' : hp.quniform('learning_rate', 0.01, 0.2, 0.01)\n",
    "                ,'max_depth' : hp.choice('max_depth', np.arange(2, 20, dtype = int))\n",
    "                ,'min_child_weight': hp.quniform ('min_child_weight', 1, 20, 1)\n",
    "                ,'n_estimators' : hp.choice('n_estimators', np.arange(100, 10000, 10, dtype = int))\n",
    "                #,'objective' : hp.choice('objective', ['binary:logistic'])\n",
    "                ,'subsample': hp.uniform ('subsample', 0.5, 1)\n",
    "            }\n",
    "            ,'ensemble.RandomForestRegressor' : {\n",
    "                'bootstrap' : hp.choice('bootstrap', [True, False])\n",
    "                ,'max_depth' : hp.choice('max_depth', np.arange(2, 20, dtype = int))\n",
    "                ,'n_estimators' : hp.choice('n_estimators', np.arange(100, 10000, 10, dtype = int))\n",
    "                ,'max_features' : hp.choice('max_features', ['auto','sqrt'])\n",
    "                ,'min_samples_split' : hp.choice('min_samples_split', np.arange(2, 40, dtype = int))\n",
    "                ,'min_samples_leaf' : hp.choice('min_samples_leaf', np.arange(2, 40, dtype = int))\n",
    "            }\n",
    "            ,'ensemble.GradientBoostingRegressor' : {\n",
    "                'n_estimators' : hp.choice('n_estimators', np.arange(100, 10000, 10, dtype = int))\n",
    "                ,'max_depth' : hp.choice('max_depth', np.arange(2, 20, dtype = int))\n",
    "                ,'max_features' : hp.choice('max_features', ['auto','sqrt'])    \n",
    "                ,'learning_rate' : hp.quniform('learning_rate', 0.01, 0.2, 0.01)\n",
    "                ,'loss' : hp.choice('loss', ['ls','lad','huber','quantile'])    \n",
    "                ,'min_samples_split' : hp.choice('min_samples_split', np.arange(2, 40, dtype = int))\n",
    "                ,'min_samples_leaf' : hp.choice('min_samples_leaf', np.arange(2, 40, dtype = int))\n",
    "            }\n",
    "            \n",
    "            ,'ensemble.AdaBoostRegressor' : {\n",
    "                'n_estimators' : hp.choice('n_estimators', np.arange(100, 10000, 10, dtype = int))\n",
    "                ,'learning_rate' : hp.quniform('learning_rate', 0.01, 0.2, 0.01)\n",
    "                ,'loss' : hp.choice('loss', ['linear', 'square','exponential'])                    \n",
    "            }\n",
    "            ,'ensemble.ExtraTreesRegressor' : {\n",
    "                'n_estimators' : hp.choice('n_estimators', np.arange(100, 10000, 10, dtype = int))\n",
    "                ,'max_depth' : hp.choice('max_depth', np.arange(2, 20, dtype = int))\n",
    "                ,'min_samples_split' : hp.choice('min_samples_split', np.arange(2, 40, dtype = int))\n",
    "                ,'min_samples_leaf' : hp.choice('min_samples_leaf', np.arange(2, 40, dtype = int))\n",
    "                ,'max_features' : hp.choice('max_features', ['auto','sqrt'])\n",
    "            }\n",
    "            ,'svm.SVR' : {\n",
    "                'C' : hp.quniform('C', 0.0, 10, 0.05)\n",
    "                ,'kernel' : hp.choice('kernel', ['linear','poly','rbf','sigmoid'])\n",
    "                ,'degree' : hp.choice('degree', [2,3])\n",
    "                ,'gamma' : hp.quniform('gamma', 0.0, 10, 0.05)\n",
    "                ,'epsilon' : hp.quniform('epsilon', 0.001, 5, 0.05)\n",
    "            }\n",
    "            ,'neighbors.KNeighborsRegressor' : {\n",
    "                'algorithm' : hp.choice('algorithm', ['auto','ball_tree','kd_tree','brute'])\n",
    "                ,'n_neighbors' : hp.choice('n_neighbors', np.arange(1, 20, dtype = int))\n",
    "                ,'weights' : hp.choice('weights', ['distance','uniform'])\n",
    "                ,'p' : hp.choice('p', [1,2])\n",
    "            }\n",
    "}\n",
    "\n",
    "# set variables\n",
    "scoring = 'neg_mean_squared_error'\n",
    "n_folds = 2\n",
    "n_jobs = 1\n",
    "verbose = 0\n",
    "ITERATION = 0\n",
    "iters = 1\n",
    "\n",
    "# create shell file\n",
    "out_file = 'data/{}_{}.csv'.format(rundate, comp)\n",
    "with open(out_file, 'w', newline = '') as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    writer.writerow(['iteration','estimator','params','loss','min','mean','max','std','train_time','status'])\n",
    "\n",
    "for model in allSpace.keys():\n",
    "    space = allSpace[model]\n",
    "    \n",
    "    # reset objective function defaults\n",
    "    objective.__defaults__ = (model, train.X_, train.y_, scoring, n_folds, n_jobs, verbose, False)\n",
    "    \n",
    "    # Run optimization\n",
    "    print('#'*100)\n",
    "    print('\\nTuning {0}\\n'.format(model))\n",
    "    best = fmin(fn = objective\n",
    "                ,space = space\n",
    "                ,algo = tpe.suggest\n",
    "                ,max_evals = iters\n",
    "                ,trials = Trials()\n",
    "                ,verbose = 1\n",
    "               )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Evaluation'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T08:30:55.777454Z",
     "start_time": "2019-03-24T08:30:54.435579Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model explanability\n",
    "\n",
    "https://www.kaggle.com/learn/machine-learning-explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Feature-importance'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Permutation importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Permutation-importance'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T08:30:55.873743Z",
     "start_time": "2019-03-24T08:30:55.870060Z"
    }
   },
   "outputs": [],
   "source": [
    "# permutation importance\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "perm = PermutationImportance(my_model, random_state=1).fit(val_X, val_y)\n",
    "eli5.show_weights(perm, feature_names = val_X.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partial plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Partial-plots'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "from matplotlib import pyplot as plt\n",
    "from pdpbox import pdp, get_dataset, info_plots\n",
    "\n",
    "# Create the data that we will plot\n",
    "pdp_goals = pdp.pdp_isolate(model=tree_model, dataset=val_X, model_features=feature_names, feature='Goal Scored')\n",
    "\n",
    "# plot it\n",
    "pdp.pdp_plot(pdp_goals, 'Goal Scored')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_to_plot = 'Distance Covered (Kms)'\n",
    "pdp_dist = pdp.pdp_isolate(model=tree_model, dataset=val_X, model_features=feature_names, feature=feature_to_plot)\n",
    "\n",
    "pdp.pdp_plot(pdp_dist, feature_to_plot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Random Forest model\n",
    "rf_model = RandomForestClassifier(random_state=0).fit(train_X, train_y)\n",
    "\n",
    "pdp_dist = pdp.pdp_isolate(model=rf_model, dataset=val_X, model_features=feature_names, feature=feature_to_plot)\n",
    "\n",
    "pdp.pdp_plot(pdp_dist, feature_to_plot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D plots\n",
    "# Similar to previous PDP plot except we use pdp_interact instead of pdp_isolate and pdp_interact_plot instead of pdp_isolate_plot\n",
    "features_to_plot = ['Goal Scored', 'Distance Covered (Kms)']\n",
    "inter1  =  pdp.pdp_interact(model=tree_model, dataset=val_X, model_features=feature_names, features=features_to_plot)\n",
    "\n",
    "pdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=features_to_plot, plot_type='contour')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SHAP values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'SHAP-values'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "row_to_show = 5\n",
    "data_for_prediction = val_X.iloc[row_to_show]  # use 1 row of data here. Could use multiple rows if desired\n",
    "data_for_prediction_array = data_for_prediction.values.reshape(1, -1)\n",
    "\n",
    "\n",
    "my_model.predict_proba(data_for_prediction_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap  # package used to calculate Shap values\n",
    "\n",
    "# Create object that can calculate shap values\n",
    "explainer = shap.TreeExplainer(my_model)\n",
    "\n",
    "# Calculate Shap values\n",
    "shap_values = explainer.shap_values(data_for_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Kernel SHAP to explain test set predictions\n",
    "k_explainer = shap.KernelExplainer(my_model.predict_proba, train_X)\n",
    "k_shap_values = k_explainer.shap_values(data_for_prediction)\n",
    "shap.force_plot(k_explainer.expected_value[1], k_shap_values[1], data_for_prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.DeepExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap  # package used to calculate Shap values\n",
    "\n",
    "# Create object that can calculate shap values\n",
    "explainer = shap.TreeExplainer(my_model)\n",
    "\n",
    "# calculate shap values. This is what we will plot.\n",
    "# Calculate shap_values for all of val_X rather than a single row, to have more data for plot.\n",
    "shap_values = explainer.shap_values(val_X)\n",
    "\n",
    "# Make plot. Index of [1] is explained in text below.\n",
    "shap.summary_plot(shap_values[1], val_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap  # package used to calculate Shap values\n",
    "\n",
    "# Create object that can calculate shap values\n",
    "explainer = shap.TreeExplainer(my_model)\n",
    "\n",
    "# calculate shap values. This is what we will plot.\n",
    "shap_values = explainer.shap_values(X)\n",
    "\n",
    "# make plot.\n",
    "shap.dependence_plot('Ball Possession %', shap_values[1], X, interaction_index=\"Goal Scored\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Stacking'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primary models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Primary-models'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T14:05:17.965746Z",
     "start_time": "2019-03-24T14:05:17.669308Z"
    }
   },
   "outputs": [],
   "source": [
    "# read scores summary table\n",
    "scoreDf = pd.read_csv('data/cvresults_20190324.csv', index_col = 0, na_values = 'nan')\n",
    "scoreDf[:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T14:11:12.912222Z",
     "start_time": "2019-03-24T14:09:44.583800Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T14:11:26.441649Z",
     "start_time": "2019-03-24T14:11:26.032435Z"
    }
   },
   "outputs": [],
   "source": [
    "# view correlations of predictions\n",
    "p = qp.plotter.QuickPlot(fig = plt.figure(), chartProp = 15)\n",
    "ax = p.makeCanvas(position = 111)\n",
    "p.qpCorrHeatmap(df = pd.DataFrame(x_valid, columns = ['a','b','c','d','e','f'])\n",
    "                ,annot = True\n",
    "                ,ax = ax\n",
    "               )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Meta-model'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T14:16:24.776352Z",
     "start_time": "2019-03-24T14:12:03.005790Z"
    }
   },
   "outputs": [],
   "source": [
    "# model dictionary\n",
    "models = {\n",
    "    'XGBoost' : xgboost.XGBRegressor()\n",
    "}\n",
    "\n",
    "# parameter grids\n",
    "params = {\n",
    "        'XGBoost' : {\n",
    "            'colsample_bytree' : np.arange(0.1, 1.0, 0.1)\n",
    "            ,'gamma' : np.arange(0.01, .1, 0.01)\n",
    "            ,'learning_rate' : np.arange(0.01, .1, 0.01)\n",
    "            ,'max_depth' : np.arange(2, 6)\n",
    "            ,'min_child_weight' : np.arange(0.5, 2.5, 0.25)\n",
    "            ,'n_estimators' : np.arange(800, 2000, 100) \n",
    "            ,'reg_alpha' : np.arange(0.1, 1.0, 0.1)\n",
    "            ,'reg_lambda' : np.arange(0.1, 1.0, 0.1)\n",
    "            ,'subsample' : np.arange(0.1, 1.0, 0.1)\n",
    "        }\n",
    "}\n",
    "\n",
    "# execute cross-validation grid search for model / parameter grid pairs\n",
    "helper = train.PowerGridSearcher(models, params)\n",
    "gridSearch = helper.fitRgs(x_train\n",
    "                ,train.y_\n",
    "                ,n_iter = 100\n",
    "                ,cv = 5\n",
    "                ,verbose = 0\n",
    "                ,scoring = 'neg_mean_squared_error')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T14:16:24.983826Z",
     "start_time": "2019-03-24T14:16:24.779451Z"
    }
   },
   "outputs": [],
   "source": [
    "# review summary\n",
    "secondLevelScoreDf = helper.scoreSummary()\n",
    "secondLevelScoreDf[['min_score', 'mean_score', 'max_score']] = secondLevelScoreDf[['min_score', 'mean_score', 'max_score']].applymap(lambda x: np.round(np.sqrt(-x), 6))\n",
    "secondLevelScoreDf[:10]\n",
    "# scores.fillna('')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Submission'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Standard'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T13:46:12.681655Z",
     "start_time": "2019-03-24T13:46:06.563199Z"
    }
   },
   "outputs": [],
   "source": [
    "# standard model fit and predict\n",
    "params = train.modelParamBuilder(scoreDf = scoreDf, modelIx = 1663)\n",
    "\n",
    "model = xgboost.XGBRegressor(**params)\n",
    "# model = ensemble.GradientBoostingRegressor(**params)\n",
    "\n",
    "model.fit(train.X_, train.y_)\n",
    "yPred = model.predict(valid.X_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T13:46:14.138741Z",
     "start_time": "2019-03-24T13:46:14.106125Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate prediction submission file\n",
    "my_submission = pd.DataFrame({'Id': dfTest.Id, 'SalePrice': np.expm1(yPred)})\n",
    "my_submission.to_csv('data/submission.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Stack'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T14:17:12.332889Z",
     "start_time": "2019-03-24T14:17:11.732365Z"
    }
   },
   "outputs": [],
   "source": [
    "# best second level learning model\n",
    "params = train.modelParamBuilder(scoreDf = secondLevelScoreDf, modelIx = 28)\n",
    "model = xgboost.XGBRegressor(**params)\n",
    "\n",
    "model.fit(x_train, train.y_)\n",
    "yPred = model.predict(x_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T14:17:12.422440Z",
     "start_time": "2019-03-24T14:17:12.335453Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate prediction submission file\n",
    "my_submission = pd.DataFrame({'Id': dfTest.Id, 'SalePrice': np.expm1(yPred)})\n",
    "my_submission.to_csv('data/submission.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# misc code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Filling missing value of Age \n",
    "\n",
    "## Fill Age with the median age of similar rows according to Pclass, Parch and SibSp\n",
    "# Index of NaN age rows\n",
    "index_NaN_age = list(dataset[\"Age\"][dataset[\"Age\"].isnull()].index)\n",
    "\n",
    "for i in index_NaN_age :\n",
    "    age_med = dataset[\"Age\"].median()\n",
    "    age_pred = dataset[\"Age\"][((dataset['SibSp'] == dataset.iloc[i][\"SibSp\"]) & (dataset['Parch'] == dataset.iloc[i][\"Parch\"]) & (dataset['Pclass'] == dataset.iloc[i][\"Pclass\"]))].median()\n",
    "    if not np.isnan(age_pred) :\n",
    "        dataset['Age'].iloc[i] = age_pred\n",
    "    else :\n",
    "        dataset['Age'].iloc[i] = age_med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# libs\n",
    "\n",
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-13T03:47:25.794949Z",
     "start_time": "2019-03-13T03:47:25.789160Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n_folds = 10\n",
    "\n",
    "def rmsle_cv(model):\n",
    "    kf = model_selection.KFold(n_folds, shuffle = True, random_state = 42).get_n_splits(train.X_.values)\n",
    "    rmse= np.sqrt(-model_selection.cross_val_score(model, train.X_.values, train.y_, scoring=\"neg_mean_squared_error\", cv = kf))\n",
    "    return(rmse)\n",
    "\n",
    "\n",
    "averaged_models = AveragingModels(models = (topXGBoost, topGBR, topLGBM))\n",
    "avg_scores = rmsle_cv(averaged_models)\n",
    "\n",
    "\n",
    "\n",
    "cv = train. rmsleCV(estimator = topLGBM, X = train.X_, y = train.y_, scoring = 'neg_mean_squared_error', cv = 10, modelDesc = 'lightgbm')\n",
    "\n",
    "\n",
    "#\n",
    "\n",
    "z = list(zip(rfrFinal.feature_importances_, np.append(numCols,catCols)))\n",
    "z = sorted(z, key = lambda tup: tup[0], reverse = True)[:20]\n",
    "\n",
    "# plot horizontal bar by feature importance\n",
    "z.sort(reverse = False)\n",
    "values, labels = zip(*z)\n",
    "plt.figure(figsize = (15,8))\n",
    "plt.subplot(121)\n",
    "plt.barh(labels, values)\n",
    "plt.xlabel('Percent Contribution to Random Forest Model')\n",
    "plt.ylabel('Feature Names')\n",
    "plt.title('Comparison of Features by Importance')\n",
    "\n",
    "# reverse sorting (for a more intuitive aesthetic) and plot the cumulative value of features\n",
    "plt.subplot(122)\n",
    "z.sort(reverse = True)\n",
    "values, labels = zip(*z)\n",
    "plt.plot(np.cumsum(values))\n",
    "plt.ylabel('Contribution to the Random Forest Model')\n",
    "plt.xlabel('Number of Features')\n",
    "plt.title('Cumulative Value of Features by Importance');\n",
    "\n",
    "plt.subplots_adjust(wspace = .75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-13T03:44:30.184949Z",
     "start_time": "2019-03-13T03:44:30.177683Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class AveragingModels(base.BaseEstimator, base.RegressorMixin, base.TransformerMixin):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "        \n",
    "    # we define clones of the original models to fit the data in\n",
    "    def fit(self, X, y):\n",
    "        self.models_ = [base.clone(x) for x in self.models]\n",
    "        \n",
    "        # Train cloned base models\n",
    "        for model in self.models_:\n",
    "            model.fit(X, y)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    #Now we do the predictions for cloned models and average them\n",
    "    def predict(self, X):\n",
    "        predictions = np.column_stack([\n",
    "            model.predict(X) for model in self.models_\n",
    "        ])\n",
    "        return np.mean(predictions, axis = 1) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "class ClaimAggregater(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y = None):\n",
    "        agg_op_dt_claim = {\n",
    "            'PayDelay': {\n",
    "                'max_PayDelay': 'max',\n",
    "                'min_PayDelay': 'min',\n",
    "                'avg_PayDelay': 'mean'\n",
    "            },\n",
    "            'LengthOfStay': {\n",
    "                'max_LOS': 'max',\n",
    "                'min_LOS': 'min',\n",
    "                'avg_LOS': 'mean'\n",
    "            },\n",
    "            'DSFS': {\n",
    "                'max_dsfs': 'max',\n",
    "                'min_dsfs': 'min',\n",
    "                'avg_dsfs': 'mean'\n",
    "            },\n",
    "            'CharlsonIndex': {\n",
    "                'max_CharlsonIndex': 'max',\n",
    "                'min_CharlsonIndex': 'min',\n",
    "                'avg_CharlsonIndex': 'mean'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # add binary categorical columns to agg_op_dt_claim for groupby\n",
    "        for i in X.columns[np.array(X.dtypes == 'uint8')]:\n",
    "            agg_op_dt_claim['{0}'.format(i)] = {'Sum_{0}'.format(i) : 'sum'}\n",
    "        \n",
    "        result = X.groupby(['Year', 'MemberID']).agg(agg_op_dt_claim)\n",
    "        result.columns = result.columns.droplevel()\n",
    "        result = result.reset_index(level = ['Year', 'MemberID'])\n",
    "        result['range_dsfs'] = result['max_dsfs'] - result['min_dsfs']\n",
    "        result['range_CharlsonIndex'] = result['max_CharlsonIndex'] - result['min_CharlsonIndex']\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# preprocess via pipeline\n",
    "\n",
    "class DrugAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):  \n",
    "        dsfs_dt = {'0- 1 month': 15, '1- 2 months': 45, '2- 3 months': 75, '3- 4 months': 105, \n",
    "                   '4- 5 months': 135, '5- 6 months': 165, '6- 7 months': 195, '7- 8 months': 225, \n",
    "                   '8- 9 months': 255, '9-10 months': 285, '10-11 months': 315, '11-12 months': 345}\n",
    "        X['DSFS'] = X['DSFS'].apply(lambda x: dsfs_dt[x])\n",
    "        X['DrugCount'] = X['DrugCount'].apply(lambda x: 7 if x == '7+' else int(x))\n",
    "        return X\n",
    "\n",
    "class DrugAggregater(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y = None):\n",
    "        agg_op_dt_drug = {\n",
    "            'DrugCount': {\n",
    "                'max_DrugCount': 'max',\n",
    "                'min_DrugCount': 'min',\n",
    "                'avg_DrugCount': 'mean',\n",
    "                'months_DrugCount': 'count'\n",
    "            }\n",
    "        }\n",
    "        result = X.groupby(['Year', 'MemberID']).agg(agg_op_dt_drug)\n",
    "        result.columns = result.columns.droplevel()\n",
    "        result = result.reset_index(level=['Year', 'MemberID'])\n",
    "        return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# convert 'AgeAtFirstClaim' to numerical approximation\n",
    "\n",
    "class MemberAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None): \n",
    "        age_dt = {'40-49': 45, '70-79': 75, '50-59': 55, '60-69': 65, '30-39': 35,\n",
    "          '10-19': 15, '0-9': 5, '20-29': 25, '80+': 85}\n",
    "        X['AgeAtFirstClaim'] = X['AgeAtFirstClaim'].apply(lambda x: None if pd.isnull(x) else age_dt[x])\n",
    "        return X\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def splitPrep(dataset):\n",
    "    dfTrain, dfTest = train_test_split(dataset, test_size = 0.3, random_state = 42)\n",
    "    \n",
    "    yTrain = dfTrain['label']\n",
    "    yTest = dfTest['label']\n",
    "    \n",
    "    xTrain = dfTrain.drop(['label'], axis = 1)\n",
    "    xTest = dfTest.drop(['label'], axis = 1)\n",
    "    \n",
    "    allCols = xTrain.columns.values\n",
    "    catCols = ['ClaimsTruncated','F','M']\n",
    "    index = [np.argwhere(allCols == i)[0][0] for i in catCols]\n",
    "    numCols = np.delete(allCols, index)\n",
    "\n",
    "    numPipeline = Pipeline([\n",
    "        ('selector', DataFrameSelector(numCols)),\n",
    "        ('imputer', Imputer(strategy = 'median')),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "    catPipeline = Pipeline([\n",
    "        ('selector', DataFrameSelector(catCols)),\n",
    "    ])\n",
    "\n",
    "    fullPipeline = FeatureUnion(transformer_list = [\n",
    "        ('numPipeline', numPipeline),\n",
    "        ('catPipeline', catPipeline),\n",
    "    ])    \n",
    "    \n",
    "    xTrain = fullPipeline.fit_transform(xTrain)\n",
    "    xTest = fullPipeline.transform(xTest)\n",
    "    \n",
    "    return xTrain, xTest, yTrain, yTest, numCols, catCols\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "LandContour: Flatness of the property\n",
    "\n",
    "       Lvl\tNear Flat/Level\t\n",
    "       Bnk\tBanked - Quick and significant rise from street grade to building\n",
    "       HLS\tHillside - Significant slope from side to side\n",
    "       Low\tDepression\n",
    "\n",
    "MiscFeature: Miscellaneous feature not covered in other categories\n",
    "\t\t\n",
    "       Elev\tElevator\n",
    "       Gar2\t2nd Garage (if not described in garage section)\n",
    "       Othr\tOther\n",
    "       Shed\tShed (over 100 SF)\n",
    "       TenC\tTennis Court\n",
    "       NA\tNone\n",
    "       \n",
    "     MiscVal: $Value of miscellaneous feature\n",
    "\n",
    "\n",
    "Condition1: Proximity to various conditions\n",
    "\t\n",
    "       Artery\tAdjacent to arterial street\n",
    "       Feedr\tAdjacent to feeder street\t\n",
    "       Norm\tNormal\t\n",
    "       RRNn\tWithin 200' of North-South Railroad\n",
    "       RRAn\tAdjacent to North-South Railroad\n",
    "       PosN\tNear positive off-site feature--park, greenbelt, etc.\n",
    "       PosA\tAdjacent to postive off-site feature\n",
    "       RRNe\tWithin 200' of East-West Railroad\n",
    "       RRAe\tAdjacent to East-West Railroad\n",
    "\t\n",
    "Condition2: Proximity to various conditions (if more than one is present)\n",
    "\t\t\n",
    "       Artery\tAdjacent to arterial street\n",
    "       Feedr\tAdjacent to feeder street\t\n",
    "       Norm\tNormal\t\n",
    "       RRNn\tWithin 200' of North-South Railroad\n",
    "       RRAn\tAdjacent to North-South Railroad\n",
    "       PosN\tNear positive off-site feature--park, greenbelt, etc.\n",
    "       PosA\tAdjacent to postive off-site feature\n",
    "       RRNe\tWithin 200' of East-West Railroad\n",
    "       RRAe\tAdjacent to East-West Railroad\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
