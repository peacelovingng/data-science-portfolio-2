{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Using Gradient Ascent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "#Data\n",
    "#https://archive.ics.uci.edu/ml/datasets/Tic-Tac-Toe+Endgame\n",
    "\n",
    "#reading data\n",
    "tictactoe_raw = pd.read_csv(\"tic-tac-toe.txt\", sep=\",\")\n",
    "\n",
    "#turning all data as categorical and numerical\n",
    "for column in tictactoe_raw:\n",
    "    tictactoe_raw[column] = tictactoe_raw[column].astype('category')\n",
    "    tictactoe_raw[column] = tictactoe_raw[column].cat.codes\n",
    "    \n",
    "training_data = tictactoe_raw.sample(frac=0.5, replace=True)\n",
    "test_data = tictactoe_raw.sample(frac=0.5, replace=True)\n",
    "\n",
    "#Selecting win/lose as our Y dependent variable (as ndarray)\n",
    "Y_train = training_data['class'].values\n",
    "\n",
    "#Selecting the rest of the features for our independent X variables\n",
    "X_train = training_data[['top-left-square', 'top-middle-square', 'top-right-square',\n",
    " 'middle-left-square', 'middle-middle-square', 'middle-right-square',\n",
    " 'bottom-left-square', 'bottom-middle-square', 'bottom-right-square']].values\n",
    "\n",
    "#Test Data\n",
    "Y_test = test_data['class'].values\n",
    "X_test = test_data[['top-left-square', 'top-middle-square', 'top-right-square',\n",
    " 'middle-left-square', 'middle-middle-square', 'middle-right-square',\n",
    " 'bottom-left-square', 'bottom-middle-square', 'bottom-right-square']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Ascent Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Randomly generated weights for our Linear model\n",
    "w = np.array([random.random() for x in range(X_train.shape[1])])\n",
    "epochs = 1\n",
    "\n",
    "#Stochastic Gradient Descent\n",
    "for epoch in range(0, epochs):\n",
    "    #Re-shuffling X points\n",
    "    training_data = training_data.sample(frac=1, replace=False)\n",
    "    Y_train = training_data['class'].values\n",
    "    X_train = training_data[['top-left-square', 'top-middle-square', 'top-right-square',\n",
    " 'middle-left-square', 'middle-middle-square', 'middle-right-square',\n",
    " 'bottom-left-square', 'bottom-middle-square', 'bottom-right-square']].values\n",
    "    eta_initial = 1.0\n",
    "    s_loc = 0\n",
    "    #Stochastic sample by sample iteration step\n",
    "    for x_s in X_train:\n",
    "        #gradient (could also use regularization or the Hessian here)\n",
    "        w_error = np.dot(x_s.transpose(), Y_train[s_loc]-(1/(1+np.exp(np.dot(x_s, -w)))))\n",
    "        #decreasing eta rate\n",
    "        eta = eta_initial/(s_loc + 1)\n",
    "        #stepping in the direction of the gradient\n",
    "        w = w + eta*w_error\n",
    "        #incrementing the position of the sample in order to keep track of which y-value corresponds to the X sample\n",
    "        s_loc = s_loc + 1\n",
    "    \n",
    "print(\"Converged w:\", w)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = []\n",
    "\n",
    "#Sigmoid Function\n",
    "test_prediction = 1/(1+np.exp(np.dot(X_test, -w)))\n",
    "\n",
    "#If closer to 0 in sigmoid function allocate 0, if closer to 1, allocate 1 for the prediction\n",
    "for ysample in test_prediction:\n",
    "    if ysample <= 0.5:\n",
    "        test_predictions.append(0)\n",
    "    elif ysample > 0.5:\n",
    "        test_predictions.append(1)\n",
    "\n",
    "        \n",
    "print(\"Test Accuracy Stochastic Gradient Ascent:\", 100*sum(test_predictions == Y_train)/len(test_predictions),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Implementation With Newton Raphson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Randomly generated weights for our Linear model\n",
    "w = np.array([random.random() for x in range(X_train.shape[1])])\n",
    "epochs = 1\n",
    "\n",
    "#Used for the hessian\n",
    "I = np.eye(w.shape[0])\n",
    "\n",
    "#Stochastic Gradient Descent\n",
    "for epoch in range(0, epochs):\n",
    "    #Re-shuffling X points\n",
    "    training_data = training_data.sample(frac=1, replace=False)\n",
    "    Y_train = training_data['class'].values\n",
    "    X_train = training_data[['top-left-square', 'top-middle-square', 'top-right-square',\n",
    " 'middle-left-square', 'middle-middle-square', 'middle-right-square',\n",
    " 'bottom-left-square', 'bottom-middle-square', 'bottom-right-square']].values\n",
    "    eta_initial = 1.0\n",
    "    s_loc = 0\n",
    "    #Stochastic sample by sample iteration step\n",
    "    for x_s in X_train:\n",
    "        #gradient\n",
    "        gradient = np.dot(x_s.transpose(), Y_train[s_loc]-(1/(1+np.exp(np.dot(x_s, -w)))))\n",
    "        #Later used below in the Hessian calculation\n",
    "        P =(1/(1+np.exp(np.dot(x_s, -w))))\n",
    "        #Hessian\n",
    "        H = np.dot(np.dot(np.dot(x_s.transpose(), P), I-P), x_s)\n",
    "        #decreasing eta rate\n",
    "        eta = eta_initial/(s_loc + 1)\n",
    "        #stepping in the direction of the gradient using the Newton Raphson Method\n",
    "        w = w + eta*gradient/H\n",
    "        #incrementing the position of the sample in order to keep track of which y-value corresponds to the X sample\n",
    "        s_loc = s_loc + 1\n",
    "    \n",
    "print(\"Converged w:\", w)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = []\n",
    "\n",
    "#Sigmoid Function\n",
    "test_prediction = 1/(1+np.exp(np.dot(X_test, -w)))\n",
    "\n",
    "#If closer to 0 in sigmoid function allocate 0, if closer to 1, allocate 1 for the prediction\n",
    "for ysample in test_prediction:\n",
    "    if ysample <= 0.5:\n",
    "        test_predictions.append(0)\n",
    "    elif ysample > 0.5:\n",
    "        test_predictions.append(1)\n",
    "\n",
    "        \n",
    "print(\"Test Accuracy Newton Raphson:\", 100*sum(test_predictions == Y_train)/len(test_predictions),\"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
