{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5\n",
    "\n",
    "__Table of contents__\n",
    "\n",
    "1. [Module 7 walkthrough](#Module-7-walkthrough)\n",
    "1. [Module 8 walkthrough](#Module-8-walkthrough)\n",
    "1. [Assignment 5](#assignment)\n",
    "    1. [Acquire tweets](#Acquire-tweets)\n",
    "    1. [Remove username, URL](#Remove-username-URL)\n",
    "    1. [Remove punctuation](#Remove-punctuation)\n",
    "    1. [Remove apostrophes](#Remove-apostrophes)\n",
    "    1. [Word pattern formatting](#Word-pattern-formatting)\n",
    "    1. [Remove hashtags](#Remove-hashtags)\n",
    "    1. [Polarity analysis](#Polarity-analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T18:12:50.106466Z",
     "start_time": "2018-11-13T18:12:31.056256Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     /Users/petersontylerd/nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'/search/tweets': {'limit': 450, 'remaining': 0, 'reset': 1542133024}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import jsonpickle\n",
    "import json\n",
    "import tweepy\n",
    "import html.parser as HTMLParser\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download('sentiwordnet')\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "#from nltk.tag import ps_tag\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "modulePath = os.path.abspath(os.path.join('../../..'))\n",
    "if modulePath not in sys.path:\n",
    "    sys.path.append(modulePath)\n",
    "import config\n",
    "\n",
    "# Standard tweepy API setup\n",
    "\n",
    "auth = tweepy.OAuthHandler(config.apiKey, config.apiSec)\n",
    "auth.set_access_token(config.accessToken, config.accessSec)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "# Application authentication tweepy setup\n",
    "# Use application-only authentication for higher Twitter API rate limit\n",
    "# Twitter API returns a max of 100 tweets per query\n",
    "# Allows for 450 queries every 15 minutes\n",
    "# So we can gather 45,000 tweets every 15 minutes\n",
    "\n",
    "#Switching to application authentication\n",
    "auth = tweepy.AppAuthHandler(config.apiKey, config.apiSec)\n",
    "\n",
    "#Setting up new api wrapper, using authentication only\n",
    "api = tweepy.API(auth, wait_on_rate_limit = True\n",
    "                 ,wait_on_rate_limit_notify = True)\n",
    " \n",
    "# View rate limit status\n",
    "\n",
    "api.rate_limit_status()['resources']['search']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Module-7-walkthrough'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 7 walkthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T15:48:55.244686Z",
     "start_time": "2018-11-13T15:48:55.194974Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@user_@34 Life is great & I like it sooooooooo much. It's whatis life. #life #great#like http://lifeisgreat.com .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/petersontylerd/.pyenv/versions/jupyterMain/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: The unescape method is deprecated and will be removed in 3.5, use html.unescape() instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# \n",
    "\n",
    "htmlParser = HTMLParser.HTMLParser()\n",
    "\n",
    "tweet = \"@user_@34 Life is great & I like it sooooooooo much. It's whatis life. #life #great#like http://lifeisgreat.com .\"\n",
    "parsedTweet = htmlParser.unescape(tweet)\n",
    "print(parsedTweet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T15:48:55.251125Z",
     "start_time": "2018-11-13T15:48:55.246670Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@user_@34 Life is great & I like it sooooooooo much. It's whatis life. #life #great#like  .\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "\n",
    "urlPattern = re.compile('http\\S+')\n",
    "tweet_v1 = re.sub(urlPattern, '', parsedTweet)\n",
    "print(tweet_v1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T15:48:55.257822Z",
     "start_time": "2018-11-13T15:48:55.253377Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Life is great & I like it sooooooooo much. It's whatis life. #life #great#like  .\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "\n",
    "usernamePattern = re.compile('@\\S+')\n",
    "tweet_v2 = re.sub(usernamePattern, '', tweet_v1)\n",
    "print(tweet_v2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T15:48:55.310895Z",
     "start_time": "2018-11-13T15:48:55.259974Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Life is great & I like it so much. It's whatis life. #life #great#like  .\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "\n",
    "wordPattern = re.compile('s[o]+')\n",
    "tweet_v3 = re.sub(wordPattern, 'so', tweet_v2)\n",
    "print(tweet_v3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Module-8-walkthrough'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 8 walkthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T15:48:56.715644Z",
     "start_time": "2018-11-13T15:48:55.315169Z"
    }
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  Searched in:\n    - '/Users/petersontylerd/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/Users/petersontylerd/.pyenv/versions/jupyterMain/nltk_data'\n    - '/Users/petersontylerd/.pyenv/versions/jupyterMain/share/nltk_data'\n    - '/Users/petersontylerd/.pyenv/versions/jupyterMain/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/.ve/main/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.ve/main/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    674\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  Searched in:\n    - '/Users/petersontylerd/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/Users/petersontylerd/.pyenv/versions/jupyterMain/nltk_data'\n    - '/Users/petersontylerd/.pyenv/versions/jupyterMain/share/nltk_data'\n    - '/Users/petersontylerd/.pyenv/versions/jupyterMain/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f72440e37c73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'positive score: {0}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mswn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msenti_synsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'happy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'negative score: {0}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mswn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msenti_synsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'happy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneg_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'neutral score: {0}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mswn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msenti_synsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'happy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.ve/main/lib/python3.6/site-packages/nltk/corpus/reader/sentiwordnet.py\u001b[0m in \u001b[0;36msenti_synsets\u001b[0;34m(self, string, pos)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwordnet\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0msentis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0msynset_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msynset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msynset_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0msentis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msenti_synset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msynset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.ve/main/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.ve/main/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.ve/main/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.ve/main/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  Searched in:\n    - '/Users/petersontylerd/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/Users/petersontylerd/.pyenv/versions/jupyterMain/nltk_data'\n    - '/Users/petersontylerd/.pyenv/versions/jupyterMain/share/nltk_data'\n    - '/Users/petersontylerd/.pyenv/versions/jupyterMain/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "\n",
    "print('positive score: {0}'.format(list(swn.senti_synsets('happy','a'))[0].pos_score()))\n",
    "print('negative score: {0}'.format(list(swn.senti_synsets('happy','a'))[0].neg_score()))\n",
    "print('neutral score: {0}'.format(list(swn.senti_synsets('happy','a'))[0].obj_score()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T15:48:56.716353Z",
     "start_time": "2018-11-13T15:48:42.399Z"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "sentence = 'i am happy'\n",
    "tokens = nltk.tokensize.word_tokenize(sentence)\n",
    "for token in tokens:\n",
    "    print(swn.senti_synsets(token, '')[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T15:48:56.717417Z",
     "start_time": "2018-11-13T15:48:42.402Z"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "stop= stopwords.words('english')\n",
    "sentence = 'i am happy'\n",
    "newSentence = []\n",
    "for word in tokens:\n",
    "    if word not in stop:\n",
    "        newSentence.append(word)\n",
    "\n",
    "print('The sentence has been reduced from \\'{0}\\' \\n to \\'{1}\\''.format(sentence, newSentence))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'assignment'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5\n",
    "\n",
    "* Try cleaning the tweets that you have extracted in the the previous chapter. Apply the above rules and in addition to that apply the below mentioned rules as well:\n",
    "    * Remove Punctuations. Puntuations sometimes don't carry any weight. You can remove them. Try writing a regular expression to remove , from sentences. Dont remove question marks \"?\" or exclamatory marks as they have effect upon any sentence.\n",
    "    * Remove apostrophes and expand the words. For example in the sentence \"It's a great time to code!\" the first word It's can be expanded to 'it is'. You can do this either with regular expressions.\n",
    "    * Create a list of word patterns for word formatting. For example 'gud' should be substitued with 'good'\n",
    "\n",
    "* Calculate the polarity of a sentence and write a progam to calculate the polarity of all the tweets that you have extracted and preprocessed in the previous questions. You progam should also include the below features:\n",
    "\n",
    "    * Tweets have hashtags. Remove the hashtags and then find the polarity of each tweet.\n",
    "\n",
    "    * There might be words that are not present in the sentiwordnet lexicon.\n",
    "    * The program should handle these cases, by giving a zero score for such words.\n",
    "    *Depending on the questions,file uploads or screenshots are necessary to show your work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Acquire-tweets'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquire tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T02:26:30.976683Z",
     "start_time": "2018-11-13T18:13:22.493972Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find up to 500,000 tweets from the last week containing the word election.\n",
    "# Store in JSON file\n",
    "\n",
    "maxTweets = 500000\n",
    "tweetCount = 0\n",
    "with open('trumpTweets.json','w') as f:\n",
    "    for tweet in tweepy.Cursor(api.search, q = 'trump', tweet_mode = 'extended', lang = 'en').items(maxTweets):\n",
    "        f.write(jsonpickle.encode(tweet._json, unpicklable = False) + '\\n')\n",
    "        tweetCount += 1\n",
    "    print('Downloaded {0} tweets'.format(tweetCount))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T13:23:35.323967Z",
     "start_time": "2018-11-14T13:22:53.195342Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tweets loaded: 221072\n"
     ]
    }
   ],
   "source": [
    "# Load election tweets into memory\n",
    "\n",
    "data = []\n",
    "with open('./trumpTweets.json', 'r') as jsonFile:\n",
    "    for line in jsonFile:\n",
    "        data.append(json.loads(line))\n",
    "print('Total number of tweets loaded: {0}'.format(len(data)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T13:23:48.645496Z",
     "start_time": "2018-11-14T13:23:47.624645Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tweets extracted from json: 221072\n"
     ]
    }
   ],
   "source": [
    "# Unpack all tweets in data\n",
    "\n",
    "tweets = []\n",
    "for item in data:\n",
    "    if 'full_text' in item.keys():\n",
    "        tweet = item['full_text']\n",
    "        tweets.append(tweet)\n",
    "print('Total number of tweets extracted from json: {0}'.format(len(tweets)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T14:56:21.969545Z",
     "start_time": "2018-11-14T14:56:21.958381Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'contributors': None,\n",
       " 'coordinates': None,\n",
       " 'created_at': 'Tue Nov 13 18:16:54 +0000 2018',\n",
       " 'display_text_range': [0, 140],\n",
       " 'entities': {'hashtags': [],\n",
       "  'symbols': [],\n",
       "  'urls': [],\n",
       "  'user_mentions': [{'id': 93069110,\n",
       "    'id_str': '93069110',\n",
       "    'indices': [3, 13],\n",
       "    'name': 'Maggie Haberman',\n",
       "    'screen_name': 'maggieNYT'}]},\n",
       " 'favorite_count': 0,\n",
       " 'favorited': False,\n",
       " 'full_text': 'RT @maggieNYT: Ted OLSON, who Trump praised in one of his Fla tweets and who Trump tried repeatedly to hire for his own personal legal team…',\n",
       " 'geo': None,\n",
       " 'id': 1062408982797697024,\n",
       " 'id_str': '1062408982797697024',\n",
       " 'in_reply_to_screen_name': None,\n",
       " 'in_reply_to_status_id': None,\n",
       " 'in_reply_to_status_id_str': None,\n",
       " 'in_reply_to_user_id': None,\n",
       " 'in_reply_to_user_id_str': None,\n",
       " 'is_quote_status': True,\n",
       " 'lang': 'en',\n",
       " 'metadata': {'iso_language_code': 'en', 'result_type': 'recent'},\n",
       " 'place': None,\n",
       " 'quoted_status_id': 1062352820232445952,\n",
       " 'quoted_status_id_str': '1062352820232445952',\n",
       " 'retweet_count': 1946,\n",
       " 'retweeted': False,\n",
       " 'retweeted_status': {'contributors': None,\n",
       "  'coordinates': None,\n",
       "  'created_at': 'Tue Nov 13 15:47:49 +0000 2018',\n",
       "  'display_text_range': [0, 209],\n",
       "  'entities': {'hashtags': [],\n",
       "   'symbols': [],\n",
       "   'urls': [{'display_url': 'twitter.com/mkraju/status/…',\n",
       "     'expanded_url': 'https://twitter.com/mkraju/status/1062352820232445952',\n",
       "     'indices': [210, 233],\n",
       "     'url': 'https://t.co/BaOUgre0Zu'}],\n",
       "   'user_mentions': []},\n",
       "  'favorite_count': 7202,\n",
       "  'favorited': False,\n",
       "  'full_text': 'Ted OLSON, who Trump praised in one of his Fla tweets and who Trump tried repeatedly to hire for his own personal legal team (Olson said no), is repping CNN in suit against the White House re Acosta hard pass. https://t.co/BaOUgre0Zu',\n",
       "  'geo': None,\n",
       "  'id': 1062371464723210240,\n",
       "  'id_str': '1062371464723210240',\n",
       "  'in_reply_to_screen_name': None,\n",
       "  'in_reply_to_status_id': None,\n",
       "  'in_reply_to_status_id_str': None,\n",
       "  'in_reply_to_user_id': None,\n",
       "  'in_reply_to_user_id_str': None,\n",
       "  'is_quote_status': True,\n",
       "  'lang': 'en',\n",
       "  'metadata': {'iso_language_code': 'en', 'result_type': 'recent'},\n",
       "  'place': None,\n",
       "  'possibly_sensitive': False,\n",
       "  'quoted_status': {'contributors': None,\n",
       "   'coordinates': None,\n",
       "   'created_at': 'Tue Nov 13 14:33:44 +0000 2018',\n",
       "   'display_text_range': [0, 279],\n",
       "   'entities': {'hashtags': [],\n",
       "    'symbols': [],\n",
       "    'urls': [],\n",
       "    'user_mentions': [{'id': 22771961,\n",
       "      'id_str': '22771961',\n",
       "      'indices': [68, 75],\n",
       "      'name': 'Jim Acosta',\n",
       "      'screen_name': 'Acosta'}]},\n",
       "   'favorite_count': 6868,\n",
       "   'favorited': False,\n",
       "   'full_text': 'Ted Olson, solicitor under Bush and CNN attorney in case to restore @Acosta press credentials, says: “The Supreme Court has held in no uncertain terms that the First Amendment protects ‘robust political debate,’ including ‘speech that is critical of those who hold public office”',\n",
       "   'geo': None,\n",
       "   'id': 1062352820232445952,\n",
       "   'id_str': '1062352820232445952',\n",
       "   'in_reply_to_screen_name': None,\n",
       "   'in_reply_to_status_id': None,\n",
       "   'in_reply_to_status_id_str': None,\n",
       "   'in_reply_to_user_id': None,\n",
       "   'in_reply_to_user_id_str': None,\n",
       "   'is_quote_status': False,\n",
       "   'lang': 'en',\n",
       "   'metadata': {'iso_language_code': 'en', 'result_type': 'recent'},\n",
       "   'place': None,\n",
       "   'retweet_count': 1980,\n",
       "   'retweeted': False,\n",
       "   'source': '<a href=\"http://twitter.com/download/iphone\" rel=\"nofollow\">Twitter for iPhone</a>',\n",
       "   'truncated': False,\n",
       "   'user': {'contributors_enabled': False,\n",
       "    'created_at': 'Mon May 11 01:13:00 +0000 2009',\n",
       "    'default_profile': False,\n",
       "    'default_profile_image': False,\n",
       "    'description': 'Senior Congressional Correspondent, @CNN. Prowling the Capitol halls, covering the Hill and politics. Die-hard Chicago sports fan. Wisconsin Badger for life.',\n",
       "    'entities': {'description': {'urls': []},\n",
       "     'url': {'urls': [{'display_url': 'facebook.com/mkraju00',\n",
       "        'expanded_url': 'http://www.facebook.com/mkraju00',\n",
       "        'indices': [0, 23],\n",
       "        'url': 'https://t.co/vtt0v8C3y8'}]}},\n",
       "    'favourites_count': 2202,\n",
       "    'follow_request_sent': None,\n",
       "    'followers_count': 207469,\n",
       "    'following': None,\n",
       "    'friends_count': 3830,\n",
       "    'geo_enabled': True,\n",
       "    'has_extended_profile': False,\n",
       "    'id': 39155029,\n",
       "    'id_str': '39155029',\n",
       "    'is_translation_enabled': False,\n",
       "    'is_translator': False,\n",
       "    'lang': 'en',\n",
       "    'listed_count': 3523,\n",
       "    'location': 'The Capitol',\n",
       "    'name': 'Manu Raju',\n",
       "    'notifications': None,\n",
       "    'profile_background_color': '022330',\n",
       "    'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme6/bg.gif',\n",
       "    'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme6/bg.gif',\n",
       "    'profile_background_tile': False,\n",
       "    'profile_banner_url': 'https://pbs.twimg.com/profile_banners/39155029/1476497259',\n",
       "    'profile_image_url': 'http://pbs.twimg.com/profile_images/1036076785312718849/DqKNeMaJ_normal.jpg',\n",
       "    'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1036076785312718849/DqKNeMaJ_normal.jpg',\n",
       "    'profile_link_color': '0084B4',\n",
       "    'profile_sidebar_border_color': 'A8C7F7',\n",
       "    'profile_sidebar_fill_color': 'C0DFEC',\n",
       "    'profile_text_color': '333333',\n",
       "    'profile_use_background_image': True,\n",
       "    'protected': False,\n",
       "    'screen_name': 'mkraju',\n",
       "    'statuses_count': 28555,\n",
       "    'time_zone': None,\n",
       "    'translator_type': 'none',\n",
       "    'url': 'https://t.co/vtt0v8C3y8',\n",
       "    'utc_offset': None,\n",
       "    'verified': True}},\n",
       "  'quoted_status_id': 1062352820232445952,\n",
       "  'quoted_status_id_str': '1062352820232445952',\n",
       "  'retweet_count': 1946,\n",
       "  'retweeted': False,\n",
       "  'source': '<a href=\"http://twitter.com\" rel=\"nofollow\">Twitter Web Client</a>',\n",
       "  'truncated': False,\n",
       "  'user': {'contributors_enabled': False,\n",
       "   'created_at': 'Fri Nov 27 23:14:06 +0000 2009',\n",
       "   'default_profile': True,\n",
       "   'default_profile_image': False,\n",
       "   'description': \"Random woman, New Yorker, White House correspondent for NYTimes, analyst CNN. RTs don't imply agreement. maggie.haberman@nytimes.com\",\n",
       "   'entities': {'description': {'urls': []}},\n",
       "   'favourites_count': 44563,\n",
       "   'follow_request_sent': None,\n",
       "   'followers_count': 984298,\n",
       "   'following': None,\n",
       "   'friends_count': 3651,\n",
       "   'geo_enabled': True,\n",
       "   'has_extended_profile': False,\n",
       "   'id': 93069110,\n",
       "   'id_str': '93069110',\n",
       "   'is_translation_enabled': False,\n",
       "   'is_translator': False,\n",
       "   'lang': 'en',\n",
       "   'listed_count': 13166,\n",
       "   'location': '',\n",
       "   'name': 'Maggie Haberman',\n",
       "   'notifications': None,\n",
       "   'profile_background_color': 'C0DEED',\n",
       "   'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme1/bg.png',\n",
       "   'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme1/bg.png',\n",
       "   'profile_background_tile': False,\n",
       "   'profile_banner_url': 'https://pbs.twimg.com/profile_banners/93069110/1487303247',\n",
       "   'profile_image_url': 'http://pbs.twimg.com/profile_images/884779027625644033/6bUFtWPG_normal.jpg',\n",
       "   'profile_image_url_https': 'https://pbs.twimg.com/profile_images/884779027625644033/6bUFtWPG_normal.jpg',\n",
       "   'profile_link_color': '1DA1F2',\n",
       "   'profile_sidebar_border_color': 'C0DEED',\n",
       "   'profile_sidebar_fill_color': 'DDEEF6',\n",
       "   'profile_text_color': '333333',\n",
       "   'profile_use_background_image': True,\n",
       "   'protected': False,\n",
       "   'screen_name': 'maggieNYT',\n",
       "   'statuses_count': 190128,\n",
       "   'time_zone': None,\n",
       "   'translator_type': 'none',\n",
       "   'url': None,\n",
       "   'utc_offset': None,\n",
       "   'verified': True}},\n",
       " 'source': '<a href=\"http://twitter.com/download/iphone\" rel=\"nofollow\">Twitter for iPhone</a>',\n",
       " 'truncated': False,\n",
       " 'user': {'contributors_enabled': False,\n",
       "  'created_at': 'Mon Mar 23 03:15:03 +0000 2009',\n",
       "  'default_profile': False,\n",
       "  'default_profile_image': False,\n",
       "  'description': 'Journalist, foodie, cyclist, beach bum, Springsteen fan, Federer wannabe',\n",
       "  'entities': {'description': {'urls': []}},\n",
       "  'favourites_count': 127,\n",
       "  'follow_request_sent': None,\n",
       "  'followers_count': 59,\n",
       "  'following': None,\n",
       "  'friends_count': 108,\n",
       "  'geo_enabled': True,\n",
       "  'has_extended_profile': False,\n",
       "  'id': 25942782,\n",
       "  'id_str': '25942782',\n",
       "  'is_translation_enabled': False,\n",
       "  'is_translator': False,\n",
       "  'lang': 'en',\n",
       "  'listed_count': 8,\n",
       "  'location': 'Hartford, CT, USA',\n",
       "  'name': 'Matthew Moser',\n",
       "  'notifications': None,\n",
       "  'profile_background_color': 'BADFCD',\n",
       "  'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme12/bg.gif',\n",
       "  'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme12/bg.gif',\n",
       "  'profile_background_tile': False,\n",
       "  'profile_image_url': 'http://pbs.twimg.com/profile_images/1469588157/41002_1388412431140_1256947035_30887183_3057045_n_normal.jpg',\n",
       "  'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1469588157/41002_1388412431140_1256947035_30887183_3057045_n_normal.jpg',\n",
       "  'profile_link_color': 'FF0000',\n",
       "  'profile_sidebar_border_color': 'F2E195',\n",
       "  'profile_sidebar_fill_color': 'FFF7CC',\n",
       "  'profile_text_color': '0C3E53',\n",
       "  'profile_use_background_image': False,\n",
       "  'protected': False,\n",
       "  'screen_name': 'moser23',\n",
       "  'statuses_count': 2101,\n",
       "  'time_zone': None,\n",
       "  'translator_type': 'none',\n",
       "  'url': None,\n",
       "  'utc_offset': None,\n",
       "  'verified': False}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Remove-username-URL'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove username, URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Remove-punctuation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove punctuation\n",
    "\n",
    "- Remove ','\n",
    "- Keep '?','!'\n",
    "- Others?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Remove-apostrophes'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove apostrophes\n",
    "\n",
    "- Remove apostrophes and expand words\n",
    "    - \"It's\" becomes \"It is\", however \"Trump's\" stays \"Trump's\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It's, what's, whats, that's, thats\n",
    "- do a search through corpus for the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Word-pattern-formatting'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word pattern formatting\n",
    "\n",
    "- Condense extended strings of vowels and consonants down to form correctly spelled word\n",
    "    - \"Gooooooood\" becomes \"Good\"\n",
    "    - \"Realllllly\" becomes \"Really\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Remove-hashtags'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Polarity-analysis'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polarity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
