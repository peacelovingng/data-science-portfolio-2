{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Module 7/8 & Project__\n",
    "\n",
    "1. [Import](#Import)\n",
    "1. [Module 7 walkthrough](#Module-7-walkthrough)\n",
    "1. [Module 8 walkthrough](#Module-8-walkthrough)\n",
    "1. [Project](#Project)\n",
    "    1. [Get tweets and clean tweets](#Get-tweets-and-clean-tweets)\n",
    "        1. [Acquire tweets](#Acquire-tweets)\n",
    "        1. [Load tweets](#Load-tweets)\n",
    "        1. [HTML Parser](#HTML-Parser)\n",
    "        1. [Remove username, URL](#Remove-username-URL)\n",
    "        1. [Remove extraneous characters](#Remove-extraneous-characters)\n",
    "        1. [Remove apostrophes](#Remove-apostrophes)\n",
    "        1. [Remove punctuation](#Remove-punctuation)\n",
    "        1. [Word pattern formatting](#Word-pattern-formatting)\n",
    "        1. [Remove hashtags](#Remove-hashtags)\n",
    "        1. [Export to pickle file](#Export-to-pickle-file)\n",
    "        1. [Correct incorrectly spelled words](#Correct-incorrectly-spelled-words)\n",
    "    1. [Polarity analysis](#Polarity-analysis)\n",
    "        1. [Load tweets from pickle file](#Load-tweets-from-pickle-file)\n",
    "        1. [Calculate polarity](#Calculate-polarity)\n",
    "        1. [Evaluate results](#Evaluate-results)\n",
    "        1. [Conclusion](#Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Import'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T01:50:14.227130Z",
     "start_time": "2018-11-30T01:50:12.536330Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     /Users/petersontylerd/nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'/search/tweets': {'limit': 450, 'remaining': 450, 'reset': 1543543514}}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import jsonpickle\n",
    "import json\n",
    "import tweepy\n",
    "import html.parser as HTMLParser\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download('sentiwordnet')\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from textblob import TextBlob, Word\n",
    "\n",
    "modulePath = os.path.abspath(os.path.join('../../..'))\n",
    "if modulePath not in sys.path:\n",
    "    sys.path.append(modulePath)\n",
    "import config # stores the API and access keys for twitter. only on my machine so this will throw an error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard tweepy API setup\n",
    "auth = tweepy.OAuthHandler(config.apiKey, config.apiSec)\n",
    "auth.set_access_token(config.accessToken, config.accessSec)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "# Application authentication tweepy setup\n",
    "# Use application-only authentication for higher Twitter API rate limit\n",
    "# Twitter API returns a max of 100 tweets per query\n",
    "# Allows for 450 queries every 15 minutes\n",
    "# So we can gather 45,000 tweets every 15 minutes\n",
    "\n",
    "# switching to application authentication\n",
    "auth = tweepy.AppAuthHandler(config.apiKey, config.apiSec)\n",
    "\n",
    "# setting up new api wrapper, using authentication only\n",
    "api = tweepy.API(auth, wait_on_rate_limit = True\n",
    "                 ,wait_on_rate_limit_notify = True)\n",
    " \n",
    "# view rate limit status\n",
    "api.rate_limit_status()['resources']['search']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 7 walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Module-7-walkthrough'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T01:50:17.107548Z",
     "start_time": "2018-11-30T01:50:17.062269Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@user_@34 Life is great & I like it sooooooooo much. It's whatis life. #life #great#like http://lifeisgreat.com .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/petersontylerd/.pyenv/versions/jupyterMain/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: The unescape method is deprecated and will be removed in 3.5, use html.unescape() instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# parse sample tweet\n",
    "htmlParser = HTMLParser.HTMLParser()\n",
    "\n",
    "tweet = \"@user_@34 Life is great & I like it sooooooooo much. It's whatis life. #life #great#like http://lifeisgreat.com .\"\n",
    "parsedTweet = htmlParser.unescape(tweet)\n",
    "print(parsedTweet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T01:50:18.266464Z",
     "start_time": "2018-11-30T01:50:18.261894Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@user_@34 Life is great & I like it sooooooooo much. It's whatis life. #life #great#like  .\n"
     ]
    }
   ],
   "source": [
    "# remove URL\n",
    "urlPattern = re.compile('http\\S+')\n",
    "tweet_v1 = re.sub(urlPattern, '', parsedTweet)\n",
    "print(tweet_v1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T01:50:20.048388Z",
     "start_time": "2018-11-30T01:50:20.045310Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Life is great & I like it sooooooooo much. It's whatis life. #life #great#like  .\n"
     ]
    }
   ],
   "source": [
    "# remoe username\n",
    "usernamePattern = re.compile('@\\S+')\n",
    "tweet_v2 = re.sub(usernamePattern, '', tweet_v1)\n",
    "print(tweet_v2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T01:50:20.752834Z",
     "start_time": "2018-11-30T01:50:20.749718Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Life is great & I like it so much. It's whatis life. #life #great#like  .\n"
     ]
    }
   ],
   "source": [
    "# remove words repetive \"o's\"\n",
    "wordPattern = re.compile('s[o]+')\n",
    "tweet_v3 = re.sub(wordPattern, 'so', tweet_v2)\n",
    "print(tweet_v3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 8 walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Module-8-walkthrough'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T01:50:26.521164Z",
     "start_time": "2018-11-30T01:50:23.541087Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/petersontylerd/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "positive score for the word \"happy\": 0.875\n",
      "negative score for the word \"happy\": 0.0\n",
      "neutral score for the word \"happy\": 0.125\n"
     ]
    }
   ],
   "source": [
    "# load NLTK wordnet and sample sentiment score\n",
    "nltk.download('wordnet')\n",
    "\n",
    "print('positive score for the word \"happy\": {0}'.format(list(swn.senti_synsets('happy','a'))[0].pos_score()))\n",
    "print('negative score for the word \"happy\": {0}'.format(list(swn.senti_synsets('happy','a'))[0].neg_score()))\n",
    "print('neutral score for the word \"happy\": {0}'.format(list(swn.senti_synsets('happy','a'))[0].obj_score()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T01:50:44.359009Z",
     "start_time": "2018-11-30T01:50:44.158332Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/petersontylerd/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Tokens: ['i', 'am', 'happy']\n"
     ]
    }
   ],
   "source": [
    "# toeknize sample sentence\n",
    "nltk.download('punkt')\n",
    "\n",
    "sentence = 'i am happy'\n",
    "tokens = nltk.tokenize.word_tokenize(sentence)\n",
    "print('Tokens: {0}'.format(tokens))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T01:50:46.522591Z",
     "start_time": "2018-11-30T01:50:46.278621Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/petersontylerd/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('i', 'NN'), ('am', 'VBP'), ('happy', 'JJ')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# identify part of speech for each word\n",
    "from nltk.tag import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "pos_tag(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN - noun\n",
    "VBP - verb\n",
    "JJ - adjective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T01:50:48.716180Z",
     "start_time": "2018-11-30T01:50:48.709783Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentence has been reduced from 'i am happy' \n",
      " to '['happy']'\n"
     ]
    }
   ],
   "source": [
    "# remove stop words from sample sentence\n",
    "stop = stopwords.words('english')\n",
    "sentence = 'i am happy'\n",
    "newSentence = []\n",
    "for word in tokens:\n",
    "    if word not in stop:\n",
    "        newSentence.append(word)\n",
    "\n",
    "print('The sentence has been reduced from \\'{0}\\' \\n to \\'{1}\\''.format(sentence, newSentence))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project\n",
    "\n",
    "* Try cleaning the tweets that you have extracted in the the previous chapter. Apply the above rules and in addition to that apply the below mentioned rules as well:\n",
    "    * Remove Punctuations. Puntuations sometimes don't carry any weight. You can remove them. Try writing a regular expression to remove , from sentences. Dont remove question marks \"?\" or exclamatory marks as they have effect upon any sentence.\n",
    "    * Remove apostrophes and expand the words. For example in the sentence \"It's a great time to code!\" the first word It's can be expanded to 'it is'. You can do this either with regular expressions.\n",
    "    * Create a list of word patterns for word formatting. For example 'gud' should be substitued with 'good'\n",
    "\n",
    "* Calculate the polarity of a sentence and write a progam to calculate the polarity of all the tweets that you have extracted and preprocessed in the previous questions. You progam should also include the below features:\n",
    "\n",
    "    * Tweets have hashtags. Remove the hashtags and then find the polarity of each tweet.\n",
    "\n",
    "    * There might be words that are not present in the sentiwordnet lexicon.\n",
    "    * The program should handle these cases, by giving a zero score for such words.\n",
    "    *Depending on the questions,file uploads or screenshots are necessary to show your work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Project'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get tweets and clean tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Get-tweets-and-clean-tweets'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acquire tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Acquire-tweets'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T02:26:30.976683Z",
     "start_time": "2018-11-13T18:13:22.493972Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# find up to 500,000 tweets from the last week containing the word election.\n",
    "# store in JSON file\n",
    "maxTweets = 500000\n",
    "tweetCount = 0\n",
    "with open('trumpTweets.json','w') as f:\n",
    "    for tweet in tweepy.Cursor(api.search, q = 'trump', tweet_mode = 'extended', lang = 'en').items(maxTweets):\n",
    "        f.write(jsonpickle.encode(tweet._json, unpicklable = False) + '\\n')\n",
    "        tweetCount += 1\n",
    "    print('Downloaded {0} tweets'.format(tweetCount))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Load-tweets'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T01:51:23.053937Z",
     "start_time": "2018-11-30T01:50:52.959138Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tweets loaded: 221072\n"
     ]
    }
   ],
   "source": [
    "# load election tweets into memory\n",
    "data = []\n",
    "with open('./trumpTweets.json', 'r') as jsonFile:\n",
    "    for line in jsonFile:\n",
    "        data.append(json.loads(line))\n",
    "print('Total number of tweets loaded: {0}'.format(len(data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T02:06:33.690347Z",
     "start_time": "2018-11-30T02:06:33.524843Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tweets extracted from json: 221072\n"
     ]
    }
   ],
   "source": [
    "# unpack all tweets in data\n",
    "tweets = []\n",
    "for item in data:\n",
    "    if 'full_text' in item.keys():\n",
    "        tweet = item['full_text']\n",
    "        tweets.append(tweet)\n",
    "print('Total number of tweets extracted from json: {0}'.format(len(tweets)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T02:06:33.696500Z",
     "start_time": "2018-11-30T02:06:33.692076Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RT @pettyasamug: Trump hates pics of his hair circulating on social media #Retweet ♻️\\n\\n#NotMyPresident https://t.co/4uvEKDaING',\n",
       " 'RT @maggieNYT: Ted OLSON, who Trump praised in one of his Fla tweets and who Trump tried repeatedly to hire for his own personal legal team…',\n",
       " '@realDonaldTrump Stock goes up, credit to trump , goes down? Blame the Democrats. Got it 👌🏾',\n",
       " 'This. The crisis is here; it can’t be avoided, only mitigated. When I look at Trump’s GOP &amp; their supporters, I see people knowingly and gleefully poisoning my grandchildren and my planet. This is why bipartisanship is BS. I won’t compromise with murderers. https://t.co/mEYs7IszjB',\n",
       " 'RT @politico: Despite Democrats’ massive House gains — the party’s biggest since 1974, after Richard Nixon’s resignation — redistricting cl…']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print small sample of tweets\n",
    "tweets[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T02:06:33.778435Z",
     "start_time": "2018-11-30T02:06:33.698526Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove retweets\n",
    "tweets = [x for x in tweets if not x.startswith('RT ')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'HTML-Parser'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T02:06:33.816789Z",
     "start_time": "2018-11-30T02:06:33.780884Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove escapes\n",
    "import html\n",
    "\n",
    "for ix, tweet in enumerate(tweets):\n",
    "    parsedTweet = html.unescape(tweet)\n",
    "    tweets[ix] = parsedTweet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove username, URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Remove-username-URL'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T02:06:34.016743Z",
     "start_time": "2018-11-30T02:06:33.818615Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove URLs and usernames\n",
    "urlPattern = re.compile(r'(?:\\@|https?\\://)\\S+')\n",
    "for ix, tweet in enumerate(tweets):\n",
    "    parsedTweet = re.sub(urlPattern, '', tweet)\n",
    "    tweets[ix] = parsedTweet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove extraneous characters\n",
    "\n",
    "Remove extra white space, newlines, tabs, non-unicode characters\n",
    "and unrendered unicode strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Remove-extraneous-characters'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T02:06:34.596199Z",
     "start_time": "2018-11-30T02:06:34.018651Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove unnecessary white space, newlines and tabs\n",
    "stripPattern = re.compile(r'\\s+')\n",
    "for ix, tweet in enumerate(tweets):\n",
    "    parsedTweet = re.sub(stripPattern, ' ', tweet).strip()\n",
    "    tweets[ix] = parsedTweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T02:06:34.687419Z",
     "start_time": "2018-11-30T02:06:34.598007Z"
    }
   },
   "outputs": [],
   "source": [
    "# fix fancy single quote that's used as apostrophe in tweets with standard apostrophe\n",
    "apostropheFix = re.compile(u'\\u2019')\n",
    "for ix, tweet in enumerate(tweets):\n",
    "    parsedTweet = re.sub(apostropheFix, \"'\", tweet)\n",
    "    tweets[ix] = parsedTweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T02:06:34.931389Z",
     "start_time": "2018-11-30T02:06:34.689261Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove unicdoe string and emojis\n",
    "unicodeFix = re.compile('(?![ -~]).')\n",
    "for ix, tweet in enumerate(tweets):\n",
    "    parsedTweet = re.sub(unicodeFix, \"\", tweet)\n",
    "    tweets[ix] = parsedTweet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove apostrophes\n",
    "\n",
    "- Remove apostrophes and expand words\n",
    "    - \"It's\" becomes \"It is\", however \"Trump's\" stays \"Trump's\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Remove-apostrophes'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T02:06:36.674206Z",
     "start_time": "2018-11-30T02:06:34.933623Z"
    }
   },
   "outputs": [],
   "source": [
    "#  reformat contractsion\n",
    "\"\"\"\n",
    "I am modifying an approach I learned about through this stackoverflow post \n",
    "http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "\"\"\"\n",
    "\n",
    "cList = {\n",
    "          \"ain't\": \"am not\",\n",
    "          \"aren't\": \"are not\",\n",
    "          \"can't've\": \"cannot have\",\n",
    "          \"can't\": \"cannot\",\n",
    "          \"'cause\": \"because\",\n",
    "          \"couldn't've\": \"could not have\",\n",
    "          \"could've\": \"could have\",\n",
    "          \"couldn't\": \"could not\",\n",
    "          \"didn't\": \"did not\",\n",
    "          \"doesn't\": \"does not\",\n",
    "          \"don't\": \"do not\",\n",
    "          \"hadn't've\": \"had not have\",\n",
    "          \"hadn't\": \"had not\",\n",
    "          \"hasn't\": \"has not\",\n",
    "          \"haven't\": \"have not\",\n",
    "          \"he'd've\": \"he would have\",\n",
    "          \"he'd\": \"he would\",\n",
    "          \"he'll've\": \"he will have\",\n",
    "          \"he'll\": \"he will\",\n",
    "          \"he's\": \"he is\",\n",
    "          \"how'd'y\": \"how do you\",\n",
    "          \"how'd\": \"how did\",\n",
    "          \"how'll\": \"how will\",\n",
    "          \"how's\": \"how is\",\n",
    "          \"i'd've\": \"i would have\",\n",
    "          \"i'd\": \"i would\",\n",
    "          \"i'll've\": \"i will have\",\n",
    "          \"i'll\": \"i will\",\n",
    "          \"i'm\": \"i am\",\n",
    "          \"i've\": \"i have\",\n",
    "          \"isn't\": \"is not\",\n",
    "          \"it'd've\": \"it would have\",\n",
    "          \"it'd\": \"it had\",\n",
    "          \"it'll've\": \"it will have\",\n",
    "          \"it'll\": \"it will\",\n",
    "          \"it's\": \"it is\",\n",
    "          \"let's\": \"let us\",\n",
    "          \"ma'am\": \"madam\",\n",
    "          \"mayn't\": \"may not\",\n",
    "          \"might've\": \"might have\",\n",
    "          \"mightn't've\": \"might not have\",\n",
    "          \"mightn't\": \"might not\",\n",
    "          \"must've\": \"must have\",\n",
    "          \"mustn't've\": \"must not have\",\n",
    "          \"mustn't\": \"must not\",\n",
    "          \"needn't've\": \"need not have\",\n",
    "          \"needn't\": \"need not\",\n",
    "          \"o'clock\": \"of the clock\",\n",
    "          \"oughtn't've\": \"ought not have\",\n",
    "          \"oughtn't\": \"ought not\",\n",
    "          \"shan't\": \"shall not\",\n",
    "          \"shan't've\": \"shall not have\",\n",
    "          \"sha'n't\": \"shall not\",\n",
    "          \"she'd've\": \"she would have\",\n",
    "          \"she'd\": \"she would\",\n",
    "          \"she'll've\": \"she will have\",\n",
    "          \"she'll\": \"she will\",\n",
    "          \"she's\": \"she is\",\n",
    "          \"shouldn't've\": \"should not have\",\n",
    "          \"should've\": \"should have\",\n",
    "          \"shouldn't\": \"should not\",\n",
    "          \"so've\": \"so have\",\n",
    "          \"so's\": \"so is\",\n",
    "          \"that'd've\": \"that would have\",\n",
    "          \"that'd\": \"that would\",\n",
    "          \"that's\": \"that is\",\n",
    "          \"there'd've\": \"there would have\",\n",
    "          \"there'd\": \"there had\",\n",
    "          \"there's\": \"there is\",\n",
    "          \"they'd've\": \"they would have\",\n",
    "          \"they'd\": \"they would\",\n",
    "          \"they'll've\": \"they will have\",\n",
    "          \"they'll\": \"they will\",\n",
    "          \"they're\": \"they are\",\n",
    "          \"they've\": \"they have\",\n",
    "          \"to've\": \"to have\",\n",
    "          \"wasn't\": \"was not\",\n",
    "          \"we'd've\": \"we would have\",\n",
    "          \"we'd\": \"we had\",\n",
    "          \"we'll've\": \"we will have\",\n",
    "          \"we'll\": \"we will\",\n",
    "          \"we're\": \"we are\",\n",
    "          \"we've\": \"we have\",\n",
    "          \"weren't\": \"were not\",\n",
    "          \"what'll\": \"what will\",\n",
    "          \"what'll've\": \"what will have\",\n",
    "          \"what're\": \"what are\",\n",
    "          \"what's\": \"what is\",\n",
    "          \"what've\": \"what have\",\n",
    "          \"when's\": \"when is\",\n",
    "          \"when've\": \"when have\",\n",
    "          \"where'd\": \"where did\",\n",
    "          \"where's\": \"where is\",\n",
    "          \"where've\": \"where have\",\n",
    "          \"who'll\": \"who will\",\n",
    "          \"who'll've\": \"who will have\",\n",
    "          \"who's\": \"who is\",\n",
    "          \"who've\": \"who have\",\n",
    "          \"why's\": \"why is\",\n",
    "          \"why've\": \"why have\",\n",
    "          \"will've\": \"will have\",\n",
    "          \"won't\": \"will not\",\n",
    "          \"won't've\": \"will not have\",\n",
    "          \"would've\": \"would have\",\n",
    "          \"wouldn't\": \"would not\",\n",
    "          \"wouldn't've\": \"would not have\",\n",
    "          \"y'all'd've\": \"you all would have\",\n",
    "          \"y'all'd\": \"you all would\",\n",
    "          \"y'all're\": \"you all are\",\n",
    "          \"y'all've\": \"you all have\",\n",
    "          \"y'all\": \"you all\",\n",
    "          \"y'alls\": \"you alls\",\n",
    "          \"you'd've\": \"you would have\",\n",
    "          \"you'd\": \"you had\",\n",
    "          \"you'll've\": \"you you will have\",\n",
    "          \"you'll\": \"you you will\",\n",
    "          \"you're\": \"you are\",\n",
    "          \"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "contractionPatterns = re.compile('(%s)' % '|'.join(cList.keys()))\n",
    "\n",
    "def expandContractions(text, c_re = contractionPatterns):\n",
    "    def replace(match):\n",
    "        return cList[match.group(0)]\n",
    "    return c_re.sub(replace, text.lower())\n",
    "\n",
    "\n",
    "for ix, tweet in enumerate(tweets):\n",
    "    parsedTweet = expandContractions(tweet)\n",
    "    tweets[ix] = parsedTweet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove punctuation\n",
    "- Remove ellipses (...)\n",
    "- Remove ','\n",
    "- Keep '?','!'\n",
    "- Keep '#' for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Remove-punctuation'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T02:06:36.861160Z",
     "start_time": "2018-11-30T02:06:36.676108Z"
    }
   },
   "outputs": [],
   "source": [
    "# replace ellipses with a single space\n",
    "ellipsesPattern = re.compile(r'\\.{3,}')\n",
    "for ix, tweet in enumerate(tweets):\n",
    "    parsedTweet = re.sub(ellipsesPattern, ' ', tweet)\n",
    "    tweets[ix] = parsedTweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T02:06:37.204641Z",
     "start_time": "2018-11-30T02:06:36.863295Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove all punctuation except '?', '!', '#',and apostrophes\n",
    "punctuationPattern = re.compile(r\"[^\\w\\d\\s?!#']+\")\n",
    "for ix, tweet in enumerate(tweets):\n",
    "    parsedTweet = re.sub(punctuationPattern, '', tweet)\n",
    "    tweets[ix] = parsedTweet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word pattern formatting\n",
    "\n",
    "- Condense extended strings of vowels and consonants down to form correctly spelled word\n",
    "    - \"Gooooooood\" becomes \"Good\"\n",
    "    - \"Realllllly\" becomes \"Really\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Word-pattern-formatting'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T02:06:37.785645Z",
     "start_time": "2018-11-30T02:06:37.206410Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove all overly repetitive vowels and consonants\n",
    "repetitionPattern = re.compile(r'(.)\\1+')\n",
    "for ix, tweet in enumerate(tweets):\n",
    "    parsedTweet = re.sub(repetitionPattern, r'\\1\\1', tweet)\n",
    "    tweets[ix] = parsedTweet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove hashtags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Remove-hashtags'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T02:06:38.000612Z",
     "start_time": "2018-11-30T02:06:37.787360Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove all hashtags, including # and the associated word.\n",
    "hashtagPattern = re.compile(r'([#?])(\\w+)\\b')\n",
    "for ix, tweet in enumerate(tweets):\n",
    "    parsedTweet = re.sub(hashtagPattern, '', tweet)\n",
    "    tweets[ix] = parsedTweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T02:06:38.539163Z",
     "start_time": "2018-11-30T02:06:38.002617Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove unnecessary white space, newlines and tabs\n",
    "stripPattern = re.compile(r'\\s+')\n",
    "for ix, tweet in enumerate(tweets):\n",
    "    parsedTweet = re.sub(stripPattern, ' ', tweet).strip()\n",
    "    tweets[ix] = parsedTweet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct incorrectly spelled words\n",
    "\n",
    "The assignment prompt suggests that we identify specific words that are misspelled and replace these words with the correctly spelled word that was intended. As an example, the prompt says that \"gud\" should be \"good\".\n",
    "\n",
    "Due to the size of this dataset, which includes over 53,000 tweets (not to mention that tweets are not exactly known for accurate spelling), it would be quite difficult to find all incorrectly spelled and also make a definitive guess as to what the word should have been. It's not always clear what a misspelled word should have been.\n",
    "\n",
    "In fact, the \"gud\" to \"good\" example is particulary informative. I used a library called textblob to see how it would correct the word \"gud\" specifically, and it returned a series of words along with the probability of each word being the correct word. Interestingly, \"good\" is not among them. The most likely guess is that \"gud\" should have been \"god\", and second \"gun\". \"good\" is not on the list, as shown below.\n",
    "\n",
    "I am certainly intrigued by the potential utility in laboriously defining what incorrectly spelled words should have been depending on the context, but such an endeavor would not be practical due to the size of this text data set. A systematic approach is the only feasible solution, but a systematic approach needs to make assumptions that may not be correct, and may actually detract from the users initial message through incorrect \"corrections\" rather than truly correcting it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Correct-incorrectly-spelled-words'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T02:16:29.319421Z",
     "start_time": "2018-11-30T02:16:29.314787Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('god', 0.7453798767967146),\n",
       " ('gun', 0.1293634496919918),\n",
       " ('mud', 0.07392197125256673),\n",
       " ('gut', 0.028747433264887063),\n",
       " ('gum', 0.012320328542094456),\n",
       " ('bud', 0.006160164271047228),\n",
       " ('guy', 0.004106776180698152)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perform spell check on dummy word\n",
    "w = Word('gud')\n",
    "w.spellcheck()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T13:34:24.659184Z",
     "start_time": "2018-11-29T13:34:24.654376Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stock goes up credit to trump goes down? blame the democrats got it',\n",
       " \"this the crisis is here it cannot be avoided only mitigated when i look at trump's gop their supporters i see people knowingly and gleefully poisoning my grandchildren and my planet this is why bipartisanship is bs i will not compromise with murderers\",\n",
       " 'suburban white women who do not have a hard on for trump like',\n",
       " \"i am so weary of how often you lie about this it is early but still not going to work trump's tax cut was supposed to change corporate behavior here's what happened\",\n",
       " 'umm no trump claims he tried to salvage trip to french cemetery for us troops politico',\n",
       " 'we could not look more closely if we tried every single story msm gives us proves they are united with us against trump',\n",
       " \"putin will send a putin bear to trump in his new prison surrounding's and will make sure ivan is his daddy i mean cellmate\",\n",
       " 'donald trump gets back to the united states and someone explains what they were saying in europe',\n",
       " 'before trump the us would have been embarrassed to ally itself with russia and north korea and',\n",
       " 'they would not have done this when obama was in office or bush come to that but with trump in they feel he will back them',\n",
       " 'trump is vulgar but eu has always been under us heel george galloway',\n",
       " 'wrong person to use in gif sublimally a hateful person next time use the trump bear',\n",
       " \"europe's calculations shift on trump second term\",\n",
       " 'trump clashes with globalist european leaders trudeau as he embraces nationalism',\n",
       " 'every day all day thank you',\n",
       " 'inside the body of king henry vii full tudor documentary via ah nothing like finding about the health of some famous person of the past imagine if hillary had been elected? do you think henry could keep up wtrump?',\n",
       " 'this is not going to make anyone drink trump wine i guess macron is not your buddy anymore did someone get his feefees hurt?',\n",
       " 'did not trump fire someone after his last appearance with putin?',\n",
       " 'president trump took credit for retiring jeff flake he did not account for democrat kyrsten sinema who is about to make history on multiple fronts',\n",
       " 'if you actually read this short piece you would understand the cause of the increase hint not trump the fbi says although the number of attacks has increased so has the number of law enforcement agencies reporting hatecrime data']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Review a sample of the cleaned up tweets\n",
    "tweets[:20]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to pickle file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Export-to-pickle-file'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T13:35:04.361345Z",
     "start_time": "2018-11-29T13:35:04.326272Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('cleanTrumpTweets.pkl', 'wb') as fp:\n",
    "    pickle.dump(tweets, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polarity analysis\n",
    "\n",
    "You can simply start here and load the .pkl that was submitted along with the notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Polarity-analysis'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T02:52:11.445475Z",
     "start_time": "2018-11-30T02:52:10.111984Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     /Users/petersontylerd/nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import jsonpickle\n",
    "import json\n",
    "import tweepy\n",
    "import html.parser as HTMLParser\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "nltk.download('sentiwordnet')\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "from nltk.tag import pos_tag\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load tweets from pickle file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Load tweets from pickle file'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T02:52:11.499356Z",
     "start_time": "2018-11-30T02:52:11.448133Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of clean tweets: 53284\n"
     ]
    }
   ],
   "source": [
    "# load tweets\n",
    "with open ('cleanTrumpTweets.pkl', 'rb') as fp:\n",
    "    cleanTweets = pickle.load(fp)\n",
    "\n",
    "print('# of clean tweets: {}'.format(len(cleanTweets)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate polarity\n",
    "\n",
    "The following code block calculates a cumulative negative and positive sentiment for each tweet. The data is captured in a dictionary where the key is the tweet text and the value is a list containing two number - the first is the cumulative negative sentiment, and the second is the cumulative positive sentiment. Noun, verbs and adjectives are evaluated. If a word in not in the sentiwordnet lexicon, the word gets a positive/negative score of 0 by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Calculate polarity'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T02:54:19.968050Z",
     "start_time": "2018-11-30T02:52:11.501328Z"
    }
   },
   "outputs": [],
   "source": [
    "# determine sentiment of tweets\n",
    "sentimentDict = {}\n",
    "for tweet in cleanTweets:\n",
    "    sentimentDict[tweet] = [0., 0.]\n",
    "    tokens = nltk.tokenize.word_tokenize(tweet)\n",
    "    pos = pos_tag(tokens)    \n",
    "    for word, p in zip(tokens, pos):\n",
    "        if word not in stop:\n",
    "            if p[1] == 'NN':\n",
    "                wordPOS = 'n'\n",
    "            elif p[1] == 'VBP':\n",
    "                wordPOS = 'v'\n",
    "            elif p[1] == 'JJ':\n",
    "                wordPOS = 'a'\n",
    "            try: # exception catch to avoid crashing when no sentiment values available\n",
    "                sentimentDict[tweet][0] += list(swn.senti_synsets(word, wordPOS))[0].neg_score()\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                sentimentDict[tweet][1] += list(swn.senti_synsets(word, wordPOS))[0].pos_score()\n",
    "            except:\n",
    "                pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Evaluate-results'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T02:54:20.248427Z",
     "start_time": "2018-11-30T02:54:19.969526Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative tweet count: 27320\n",
      "Positive tweet count: 19131\n"
     ]
    }
   ],
   "source": [
    "# Count total number of postive and negative tweets\n",
    "\n",
    "import numpy as np\n",
    "sentimentTally = [0, 0]\n",
    "for val in sentimentDict.values():\n",
    "    if np.argmax(val) == 0:\n",
    "        sentimentTally[0] += 1\n",
    "    else:\n",
    "        sentimentTally[1] += 1\n",
    "print('Negative tweet count: {0}'.format(sentimentTally[0]))\n",
    "print('Positive tweet count: {0}'.format(sentimentTally[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T02:54:20.314736Z",
     "start_time": "2018-11-30T02:54:20.250462Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('silly sad ed always the hater and silent racist! using both hate and racism as wheels to turn the country against trump! lmao but he cannot most people not as stupid as ed him and his brother will doom that poor unborn baby because those guys are just miserable lousy at life!',\n",
       "  6.25),\n",
       " (\"gui lt guilt guilt guilt guilt guilt guilt guilt guilt day later dollar short! a con's story! trump lives in a television world of unreality he needs to take up bowling\",\n",
       "  5.5),\n",
       " ('trump is repugnant on so many levels and this is among the worst as one of the millions of americans who did not vote for this monstrosity i offer my humble apologies for our nation on behalf of this pathetic disrespectful petty man who is currently occupying the white house',\n",
       "  5.0),\n",
       " ('paul manafort guilty michael cohen guilty michael t flynn guilty rick gates guilty george papadopoulos guilty timothy nolan guilty richard pinedo guilty alex van der zwaan guilty who will be next roger stone ? donald trump jr ? donald j trump ?',\n",
       "  5.0),\n",
       " ('the buck stops a fine buck the best buck nobody had a buck like mine obama has a disastrous buck did you ever notice that buck and doe both mean money hillary should be locked up for what she did to bucks there! donald j i did not do it! fake news! witch hunt! trump',\n",
       "  5.0),\n",
       " ('sent my daughter to school today despite a threat of violence there worried and feeling a bit sick about it but i firmly believe a life lived in fear is a life half lived the current climate in this country is toxic i blame trump and his lies and those who support him sick',\n",
       "  4.75),\n",
       " ('my responses and posts tend to be snarky i am not particularly witty after that pathetic behavior in europe it is hard for me to be anything but disgusted trump is a stupid scary crazy man and i fear for our country and the world',\n",
       "  4.625),\n",
       " ('no lawyer worth anything wants to work for trump! he is a lawyers worst nightmare! he is a pathological liar uncontrollable unpredictable and every other negative word that i can think of!',\n",
       "  4.5),\n",
       " ('its a shame no matter who leaves the white house voluntary or involuntary the dysfunctional disjointed mannerisms remain the same this clearly shows the problem is not with the system the problem is trump himself remove trump problems go way',\n",
       "  4.375),\n",
       " ('whether it is fake or not every claim must be investigated it is truly sad that this hatred is so rampant i guess living in a trump society hatred and bigotry is a new class we all have to take to learn how to deal with i am sorry that you have to live in fear like that',\n",
       "  4.375)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# identify the most negative tweets\n",
    "sentimentDictNeg = {}\n",
    "for k, v in sentimentDict.items():\n",
    "    sentimentDictNeg[k] = v[0]\n",
    "negSentSorted = sorted(sentimentDictNeg.items(), key = lambda kv: kv[1])\n",
    "negSentSorted.reverse()\n",
    "    \n",
    "negSentSorted[:10]    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T02:54:20.373261Z",
     "start_time": "2018-11-30T02:54:20.316696Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"trump should control his temper in better way as president he should keep stable mood and be more gentle and decent than a boss whatever his ability and talent are good obama's different his excellence's only seducing people with beautiful words only for corrupt democracy\",\n",
       "  5.625),\n",
       " ('first mr trump how about if you make some effort to find your senses? or dignity? or decency? or civility? or honesty? or maturity? or morality? or loyalty? or humility? however let us all not hold our breath waiting for any of that to occur',\n",
       "  5.125),\n",
       " (\"trump's nationalism nationalism is the belief that your own country is better than all others it is important not to confuse nationalism with patriotism patriotism is a healthy pride in your country that brings about feelings of loyalty and a desire to help other citizens\",\n",
       "  5.0),\n",
       " ('no they are the opposite macron is correct but trump puts himself first ahead of the us patriotism is a healthy pride in your country that brings about feelings of loyalty and a desire to help other citizens nationalism is the belief that your country is superior no question',\n",
       "  4.625),\n",
       " ('sure but1no one campaigned in old district2trump won old by 6 or 7 in 16 while clinton won new district by 1 3dent used to win old district by 20 4 wild won new district by 10 lost in the new by fraction wild probably wins under old lines if they campaigned there',\n",
       "  4.5),\n",
       " ('4 year old mommy what does jerk off masterbation mean? the tv ppl are laughing! mom who left the tv on 18 yr old son i did mom you told me to put it on an entertainment show mom aargh!',\n",
       "  4.375),\n",
       " (\"trump looks at putin with complete servile adoration adoration for a murderous world criminal melania looks like she is checking out putin's manhood traitors! trump's are a clear national security risk\",\n",
       "  4.25),\n",
       " (\"eumacron growing concerns trump might prove less than willing to come to europe's defence in the face of a newly assertive russia truth 1 nato's job 'ensure a land war is restricted to europe' truth 2 russia is not assertive or aggressive that is nato's positioning\",\n",
       "  4.25),\n",
       " ('i will call a spade a spade and i am under no illusion that trump is perfect that being said based on his context and his describing it as the antithesis of globalism it is pretty clear that his usage is not controversial patriotism and national sovereignty are good things',\n",
       "  4.25),\n",
       " ('donnie not us your true faithful believers! we are going to rise above them all as we ! together! how i adore you my fearless and brave beyond compare donald trump i thank my lucky stars that you love us your children and bless us with your charm your wit',\n",
       "  4.25)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# identify the most negative tweets\n",
    "sentimentDictPos = {}\n",
    "for k, v in sentimentDict.items():\n",
    "    sentimentDictPos[k] = v[1]\n",
    "posSentSorted = sorted(sentimentDictPos.items(), key = lambda kv: kv[1])\n",
    "posSentSorted.reverse()\n",
    "    \n",
    "posSentSorted[:10]   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T02:54:20.385235Z",
     "start_time": "2018-11-30T02:54:20.375884Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative tweet: stock goes up credit to trump goes down? blame the democrats got it - Sentiment: 0.75\n",
      "Negative tweet: this the crisis is here it cannot be avoided only mitigated when i look at trump's gop their supporters i see people knowingly and gleefully poisoning my grandchildren and my planet this is why bipartisanship is bs i will not compromise with murderers - Sentiment: 0.875\n",
      "Negative tweet: suburban white women who do not have a hard on for trump like - Sentiment: 1.0\n",
      "Negative tweet: i am so weary of how often you lie about this it is early but still not going to work trump's tax cut was supposed to change corporate behavior here's what happened - Sentiment: 1.25\n",
      "Negative tweet: wrong person to use in gif sublimally a hateful person next time use the trump bear - Sentiment: 1.417\n",
      "Negative tweet: this is not going to make anyone drink trump wine i guess macron is not your buddy anymore did someone get his feefees hurt? - Sentiment: 0.875\n",
      "Negative tweet: did not trump fire someone after his last appearance with putin? - Sentiment: 0.25\n",
      "Negative tweet: if you actually read this short piece you would understand the cause of the increase hint not trump the fbi says although the number of attacks has increased so has the number of law enforcement agencies reporting hatecrime data - Sentiment: 0.75\n",
      "Negative tweet: he asked a question and the answer was you are a terrible person or some gibberish like that had trump actually answered the question the whole thing would not have happened but trump is as well behaved and patient as my 2 year old nephew - Sentiment: 1.625\n",
      "Negative tweet: the tories are throwing us off a cliff trump is a monstrous orange bollock but i do have a carrot that looks a bit like windy miller - Sentiment: 0.5\n"
     ]
    }
   ],
   "source": [
    "# show first ten negative tweets\n",
    "counter = 0\n",
    "for k, v in sentimentDict.items():\n",
    "    if counter == 10:\n",
    "        break\n",
    "    if np.argmax(v) == 0 and v[0] != v[1]: # neg score is highest and no ties\n",
    "        print('Negative tweet: {} - Sentiment: {}'.format(k, v[0]))\n",
    "        counter += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-30T02:54:20.395611Z",
     "start_time": "2018-11-30T02:54:20.387505Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive tweet: umm no trump claims he tried to salvage trip to french cemetery for us troops politico - Sentiment: 0.125\n",
      "Positive tweet: putin will send a putin bear to trump in his new prison surrounding's and will make sure ivan is his daddy i mean cellmate - Sentiment: 1.125\n",
      "Positive tweet: they would not have done this when obama was in office or bush come to that but with trump in they feel he will back them - Sentiment: 0.5\n",
      "Positive tweet: inside the body of king henry vii full tudor documentary via ah nothing like finding about the health of some famous person of the past imagine if hillary had been elected? do you think henry could keep up wtrump? - Sentiment: 1.375\n",
      "Positive tweet: president trump took credit for retiring jeff flake he did not account for democrat kyrsten sinema who is about to make history on multiple fronts - Sentiment: 0.875\n",
      "Positive tweet: mueller seeking more details on nigel farage key russia inquiry target says - Sentiment: 0.25\n",
      "Positive tweet: oh dear said no one cnn sues trump over jim acosta's credential suspension - Sentiment: 0.125\n",
      "Positive tweet: and of course all of you trump supporters are in here defending literal nazis - Sentiment: 0.25\n",
      "Positive tweet: it is as if everyone in trump's circle has never come into contact with our constitution - Sentiment: 0.375\n",
      "Positive tweet: you sound like a trump bashing liberal the real issue is the poor young soldiers made to fight and die for the upper class and not about trump? - Sentiment: 1.125\n"
     ]
    }
   ],
   "source": [
    "# show first ten positive tweets\n",
    "counter = 0\n",
    "for k, v in sentimentDict.items():\n",
    "    if counter == 10:\n",
    "        break\n",
    "    if np.argmax(v) == 1 and v[0] != v[1]: # pos score is highest and no ties\n",
    "        print('Positive tweet: {} - Sentiment: {}'.format(k, v[1]))\n",
    "        counter += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Conclusion'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Not surprisingly, the majority of the tweets were negative. \n",
    "\n",
    "I evaluated the positive and negative sentiment determinations both generally and specifically. First I took the specific approach by reviewing the most positive and the most negative tweets. This was accomplished by sorting the sentiment dictionary by the positive/negative value in descending order. The most negative tweets are clearly very negative, whereas the most positive tweets seems to be a mixture of somewhat positive or sarcastic tweets. The latter get mistaken for positive.\n",
    "\n",
    "In a general sense, I looked at the first handful of positive and negative tweets. The negative tweets were again pretty clearly negative, where the positive sentiment was less clear in the \"positive\" tweets. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
