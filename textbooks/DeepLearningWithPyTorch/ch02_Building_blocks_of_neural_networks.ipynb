{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Chapter 2 - Building blocks of neural networks__\n",
    "\n",
    "1. [Import](#Import)\n",
    "1. [Our first neural network](#Our-first-neural-network)\n",
    "    1. [Data preparation](#Data-preparation)\n",
    "    1. [Variables](#Variables)\n",
    "    1. [Creating data for our neural network](#Creating-data-for-our-neural-network)\n",
    "    1. [Creating learnable parameters](#Creating-learnable-parameters)\n",
    "    1. [Network implementation](#Network-implementation)\n",
    "    1. [Loss function](#Loss-function)\n",
    "    1. [Optimize the neural network](#Optimize-the-neural-network)\n",
    "    1. [Load data](#load-data)\n",
    "        1. [Dataset class](#Dataset-class)\n",
    "        1. [DataLoader class](#DataLoader-class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Import'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Standard libary and settings\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "import itertools\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "from IPython.core.display import display, HTML; display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "# Data extensions and settings\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold = np.inf, suppress = True)\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.options.display.float_format = '{:,.6f}'.format\n",
    "\n",
    "# pytorch tools\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "\n",
    "# Visualization extensions and settings\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# # Custom extensions and settings\n",
    "# sys.path.append('/main') if '/main' not in sys.path else None\n",
    "# import mlmachine as mlm\n",
    "# import quickplot as qp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our first neural network\n",
    "\n",
    "This example revolves around a made-up company called Wondermovies, which serves videos on demand. The training data contains a feature that represents the average hours spent by users watching movies on the platform. We are attempting to predict how much time each user will spend on the platform the following week. Some of the high level activities include:\n",
    "\n",
    "- __Data preparation__ - the get_data function prepares the tensors (arrays) containing input and output.\n",
    "- __Creating learningable parameters__ - the get_weights function returns tensors with initially random values that will be optimized to solve out problem.\n",
    "- __Network model__ - the simple_network function produces produces the output based on the input data by apply a linear rule, multiplying weights and the input data, and adding a bias term ($y = Wx + b$)\n",
    "- __Loss__ - the loss_fn provides information on how well the model is performing\n",
    "- __Optimi zeer__ - the optimize function adjusts the weights to help the model perform better.\n",
    "\n",
    "```python\n",
    "x, y = get_data()\n",
    "w, b = get_weights()\n",
    "\n",
    "for i in range(500):\n",
    "    y_pred = simple_network(x)\n",
    "    loss = loss_fun(y, y_pred)\n",
    "    \n",
    "if i % 50 == 0:\n",
    "    print(loss)\n",
    "    optimizer(learning_rate)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Our first-neural-network'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Data-preparation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Data preparation\n",
    "\n",
    "PyTorch provides two different types of data abstractions called tensors and variables. Tensors are similar to numpy array, and can be manipulated on GPUs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1])\n",
      "tensor([0.4167])\n"
     ]
    }
   ],
   "source": [
    "# scalar (0-D tensors) - PyTorch does not currently have a special tensor\n",
    "# with zeor dimension, so below we have a one-dimension tensor with one element\n",
    "x = torch.rand(1)\n",
    "print(x.size())\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([23.0000, 24.0000, 24.5000, 26.0000, 27.2000, 22.0000])\n",
      "torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "# vectors (1-D tensors) - a vector is an array of elements\n",
    "temp = torch.FloatTensor([23, 24, 24.5, 26, 27.2, 22.0])\n",
    "print(temp)\n",
    "print(temp.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6.3200e-03, 1.8000e+01, 2.3100e+00, 0.0000e+00, 5.3800e-01, 6.5750e+00,\n",
      "         6.5200e+01, 4.0900e+00, 1.0000e+00, 2.9600e+02, 1.5300e+01, 3.9690e+02,\n",
      "         4.9800e+00],\n",
      "        [2.7310e-02, 0.0000e+00, 7.0700e+00, 0.0000e+00, 4.6900e-01, 6.4210e+00,\n",
      "         7.8900e+01, 4.9671e+00, 2.0000e+00, 2.4200e+02, 1.7800e+01, 3.9690e+02,\n",
      "         9.1400e+00],\n",
      "        [2.7290e-02, 0.0000e+00, 7.0700e+00, 0.0000e+00, 4.6900e-01, 7.1850e+00,\n",
      "         6.1100e+01, 4.9671e+00, 2.0000e+00, 2.4200e+02, 1.7800e+01, 3.9283e+02,\n",
      "         4.0300e+00],\n",
      "        [3.2370e-02, 0.0000e+00, 2.1800e+00, 0.0000e+00, 4.5800e-01, 6.9980e+00,\n",
      "         4.5800e+01, 6.0622e+00, 3.0000e+00, 2.2200e+02, 1.8700e+01, 3.9463e+02,\n",
      "         2.9400e+00],\n",
      "        [6.9050e-02, 0.0000e+00, 2.1800e+00, 0.0000e+00, 4.5800e-01, 7.1470e+00,\n",
      "         5.4200e+01, 6.0622e+00, 3.0000e+00, 2.2200e+02, 1.8700e+01, 3.9690e+02,\n",
      "         5.3300e+00]], dtype=torch.float64)\n",
      "torch.Size([506, 13])\n"
     ]
    }
   ],
   "source": [
    "# matrices (2-D tensors) - a tensor representation of tabular data\n",
    "import sklearn.datasets as datasets\n",
    "boston = datasets.load_boston()\n",
    "\n",
    "boston_tensor = torch.from_numpy(boston.data)\n",
    "\n",
    "print(boston_tensor[:5])\n",
    "print(boston_tensor.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 3-D tensors - the result of combing multiple 3-D tensors. These are often used to\n",
    "# represent images. The three dimensions typically represent height, width, and color channel\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "panda = np.array(Image.open('data\\panda.jpg').resize((224,224)))\n",
    "panda_tensor = torch.from_numpy(panda)\n",
    "panda_tensor.size()\n",
    "\n",
    "plt.imshow(panda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1000.0000,  323.2000,  333.4000,  444.5000, 1000.0000])\n",
      "tensor([1000.0000,  323.2000,  333.4000])\n"
     ]
    }
   ],
   "source": [
    "# slice tensors - PyTorch tensors can be slice in a manner similar to numpy arrays\n",
    "sales = torch.FloatTensor([1000.0, 323.2, 333.4, 444.5, 1000.0, 323.2, 333.4, 444.5])\n",
    "\n",
    "print(sales[:5])\n",
    "print(sales[:-5])\n",
    "\n",
    "# print panda image with only one color channel\n",
    "plt.imshow(panda_tensor([:,:,0].numpy()))\n",
    "\n",
    "# crop the panda image\n",
    "plt.imshow(panda_tensor[25:175, 60:130, 0].numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 4-D tensors - a common example is a batch of images. Modern hardware is optimized to\n",
    "# perform the same operation and multiple examples at once. typical batch sizes are 16, 32, and 64\n",
    "cats = glob('data/' + '*.jpg')\n",
    "\n",
    "cat_imgs = np.array([np.array(Image.open(cat).resize((224,224))) for cat in cats[:64]])\n",
    "cat_imgs = cat_imgs.reshape(-1, 224, 224, 3)\n",
    "cat_tensors = torch.from_numpy(cat_imgs)\n",
    "cat_tensors.size()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables\n",
    "\n",
    "Deep learning algorithms are often represented as computation graphs, and nodes in the graph that hold data at a given state can be thought of as variables. In PyTorch, a variable is a wrapper around a tensor objects, its gradients and a reference to the function that created it. \n",
    "\n",
    "The autograd.Variable has three primary componenets - data, grad and creator. The data attribute simply access the tensor associated with the variable. The grad component includes the gradients, which describes rate of change of the loss function with respect to its parameters ($\\textbf{W}, \\textbf{b}$). For example, if the gradient of variable $\\textbf{a}$ is 2, then any change in the value of $\\textbf{a}$ would modify the value of $\\textbf{Y}$ by two times. The creator component refers to the function that created the variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Variables'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original tensor\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "\n",
      "gradient\n",
      "tensor([[0.2500, 0.2500],\n",
      "        [0.2500, 0.2500]])\n",
      "None\n",
      "<MeanBackward1 object at 0x0000021D8FC7AE48>\n"
     ]
    }
   ],
   "source": [
    "# create several variable and check the gradients and function\n",
    "x = Variable(torch.ones(2,2), requires_grad = True)\n",
    "y = x.mean()\n",
    "\n",
    "print('original tensor')\n",
    "print(x)\n",
    "\n",
    "# compute the gradients. By default, the gradients of variable are None\n",
    "y.backward()\n",
    "\n",
    "print('\\ngradient')\n",
    "print(x.grad)\n",
    "\n",
    "# the grad_fn of the variable points to the function is created.\n",
    "# if the variable is created by a user, like x, the function reference is None\n",
    "print(x.grad_fn)\n",
    "\n",
    "## here the grad_fn is associated with its function reference\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating data for our neural network\n",
    "\n",
    "We need to create dummy data. These are fixed Variables and do not require a gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Creating-data-for-our-neural-network'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functioin that creates our data\n",
    "def get_data():\n",
    "    train_X = np.asarray([3.3, 4.4 ,5.5, 6.71, 6.93, 4.168, 9.779, 6.182, 7.59, 2.167,\n",
    "                         7.042, 10.791, 5.313, 7.997, 5.654, 9.27, 3.1])\n",
    "    train_Y = np.asarray([1.7, 2.76, 2.09, 3.19, 1.694, 1.573, 3.366, 2.596, 2.53, 1.221,\n",
    "                         2.827, 3.465, 1.65, 2.904, 2.42, 2.94, 1.3])\n",
    "    dtype = torch.FloatTensor\n",
    "    X = Variable(torch.from_numpy(train_X).type(dtype), requires_grad = False).view(17, 1)\n",
    "    y = Variable(torch.from_numpy(train_Y).type(dtype), requires_grad = False)\n",
    "    return X, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating learnable parameters\n",
    "\n",
    "We need to create two learnable parameters, w and b, to work alongside our fixed parameters, x and y. The learnable parameters require tha tthe require_Grad parameter is set to True.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Creating-learnable-parameters'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  function for creating randomly initialized\n",
    "def get_weights():\n",
    "    w = Variable(torch.randn(1), requires_grad = True)\n",
    "    b = Variable(torch.randn(1), requires_grad = True)\n",
    "    return w, b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network implementation\n",
    "\n",
    "We need to perform matrix multiplication between x and w and add the bias b. This returns the predicted y. This can be performed in a custom function, and PyTorch also provides a higher-level abstraction in torch.nn called Layers. This takes care of most of the underlying processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Network-implementation '></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom-built function containing linear funciton\n",
    "def simple_network(x):\n",
    "    y_pred = torch.matmul(x,w)+b\n",
    "    return y_pred\n",
    "\n",
    "# built-in layer implementation\n",
    "f = nn.Linear(17, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "\n",
    "We use predictions generated by our simple network function above to understand how close our model is to reality. To do this, we use a loss function called sum of squared error (SSE). The PyTorhc library has many different loss function available, but in this case we will create the loss function by hand.\n",
    "\n",
    "In addition to the basic calculation of the loss, we also call the backward function, which calculates the gradients of the learnable parameters w and b, and we need to remove previously calculated gradient gradients when they exist. This operation is only skipped on the first loop of the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Loss-function'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custuom lost function\n",
    "def loss_fn(y, y_pred):\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    for param in [w, b]:\n",
    "        if not param.grad is None: param.grad.data.zero_()\n",
    "    loss.backward()\n",
    "    return loss.data.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize the neural network\n",
    "\n",
    "We begin with randomized weights used to predict the targets for each sample. We use a loss function to calculate current performance, and call the backward function to calculate the gradients of the learnable parameters. This process of calculating predictions, loss and gradients repeats for a set number of iterations, and it most real-world implementation, we also perform an optimization step in each iteration. This optimization uses the gradients to change the weight values, such that the loss is reduced.\n",
    "\n",
    "The optimization function multiplies the gradients, which denote the direction in which the parameters need to be adjusted, by a learning rate hyper-parameter, which controls the influence of the gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Optimize-the-neural-network'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  optimization function\n",
    "def optimizer(learning_rate):\n",
    "    w.data -= learning_rate * w.grad.data\n",
    "    b.data -= learning_rate * b.grad.data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "362.43896484375\n",
      "74052.1875\n",
      "15258321.0\n",
      "3144078848.0\n",
      "647858487296.0\n",
      "133495569514496.0\n",
      "2.750766350807859e+16\n",
      "5.668136372813169e+18\n",
      "1.1679570534378001e+21\n",
      "2.406653204568621e+23\n"
     ]
    }
   ],
   "source": [
    "#  first neural network\n",
    "learning_rate  = 0.01\n",
    "\n",
    "x, y = get_data()\n",
    "w, b = get_weights()\n",
    "\n",
    "for i in range(500):\n",
    "    y_pred = simple_network(x)\n",
    "    loss = loss_fn(y, y_pred)\n",
    "    \n",
    "    if i % 50 == 0:\n",
    "        print(loss)\n",
    "        optimizer(learning_rate)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Load data'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset class\n",
    "\n",
    "Any custom dataset class created to work with PyTorch will take the following general form. The class needs to implement two main functions:\n",
    "\n",
    "__len__(self) - returns the maximum number of elements in the dataset\n",
    "\n",
    "__getitem(self, idx) - returns a specific element based on the idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Dataset-class'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  custom dataset class\n",
    "from torch.utils.data import Dataset\n",
    "class DogsAndCatsDataset(Dataset):\n",
    "    def __init__(self,):\n",
    "        pass\n",
    "    def __len__(self):\n",
    "        pass\n",
    "    def __getitem__(self, idx):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# any required initialization occur inside the init method, fore example if we are reading the index\n",
    "# of the table and reading filenames of the images\n",
    "import glob\n",
    "class DogsAndCatsDataset(Dataset):\n",
    "    def __init__(self, root_dir, size = (224,224)):\n",
    "        self.files = glob(root_dir)\n",
    "        self.size = size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = np.asarray(Image.open(self.files(idx).resize(self.size)))\n",
    "        label = self.files[idx].split('/')[-2]\n",
    "        return img, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader class\n",
    "\n",
    "In practice, the DataLoader class provides many efficiencies that allow neural networks to to learn from input data in batches. The DataLoader class combines a dataset object with different samples and provides batches of images, using either a single or multi-process iterators.\n",
    "\n",
    "```python\n",
    "#  dataloader class\n",
    "dataloader = DataLoader(dogsdset, batch_size = 32, num_workers = 2)\n",
    "for imgs, labels in dataloader():\n",
    "    # apply deep learning process\n",
    "    pass\n",
    "```\n",
    "\n",
    "imgs wil contain a tensor of shape (32, 224, 224, 3), where 32 represents the batch size which contains image that are of size 224 by 224 with 3 color channels.\n",
    "\n",
    "The PyTorch ecosystem also includes torchvision and torch text, which utilize the Dataset and DataLoader classes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
