{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Chapter 5 - Deep learning for computer vision__\n",
    "\n",
    "1. [Import](#Import)\n",
    "1. [CNN introduction](#CNN-introduction)\n",
    "    1. [MNIST - Getting the data](#MNIST-Getting-the-data)\n",
    "    1. [The model](#The-model)\n",
    "        1. [Conv2d](#Conv2d)\n",
    "        1. [Pooling](#Pooling)\n",
    "        1. [Nonlinear activation - ReLU](#Nonlinear-activation-ReLU)\n",
    "        1. [View](#View)\n",
    "        1. [Linear layer](#Linear-layer)\n",
    "    1. [Training the model](#Training-the-model)\n",
    "1. [Classifying dogs and cats – CNN from scratch](#Classifying-dogs-and-cats–CNN-from-scratch)\n",
    "1. [Classifying dogs and cats using transfer learning](#Classifying-dogs-and-cats-using-transfer-learning)\n",
    "    1. [Creating and exploring a VGG16 model](#Creating-and-exploring-a-VGG16-model)\n",
    "        1. [Freezing the layers](#Freezing-the-layers)\n",
    "        1. [Fine-tuning VGG16](#Fine-tuning-VGG16)\n",
    "    1. [Training the VGG16 model](#Training-the-VGG16-model)\n",
    "1. [Calculating pre-convoluted features](#Calculating-pre-convoluted-features)\n",
    "1. [Understanding what a CNN model learns](#Understanding-what-a-CNN-model-learns)\n",
    "    1. [Visualizing outputs from intermediate layers](#Visualizing-outputs-from-intermediate-layers)\n",
    "1. [Visualizing weights of the CNN layer](#Visualizing-weights-of-the-CNN-layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Import'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T03:06:18.562804Z",
     "start_time": "2019-03-26T03:06:17.419256Z"
    }
   },
   "outputs": [],
   "source": [
    "# standard libary and settings\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "import itertools\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "# data extensions and settings\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(threshold=np.inf, suppress=True)\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.options.display.float_format = \"{:,.6f}\".format\n",
    "\n",
    "# pytorch tools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "# visualization extensions and settings\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN introduction\n",
    "CNNs offer an alternative to fully connected networks and are used with great results in image classification. The general architecture of a CNN model has the potential to reduce the total number of weights in the model, and more importantly, using convolutions to evaluate images helps to extract information form the spatial arrangement of pixels that make up the images.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'CNN introduction'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST - Getting the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'MNIST-Getting-the-data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T03:17:47.659520Z",
     "start_time": "2019-03-26T03:17:47.446452Z"
    }
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "transformation = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    ")\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "    \"/main/tmp/\", train=True, transform=transformation, download=True\n",
    ")\n",
    "test_dataset = datasets.MNIST(\n",
    "    \"/main/tmp/\", train=False, transform=transformation, download=True\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T03:17:49.883862Z",
     "start_time": "2019-03-26T03:17:49.879667Z"
    }
   },
   "outputs": [],
   "source": [
    "# dataset shape\n",
    "train_loader.dataset.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T03:17:50.943051Z",
     "start_time": "2019-03-26T03:17:50.937397Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot class : idx dictionary\n",
    "train_loader.dataset.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T03:17:53.847490Z",
     "start_time": "2019-03-26T03:17:53.836577Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# view raw data of first image\n",
    "train_loader.dataset.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T03:17:54.688003Z",
     "start_time": "2019-03-26T03:17:54.683122Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot sample image\n",
    "def plot_img(image):\n",
    "    image = image.numpy()\n",
    "    mean = 0.1307\n",
    "    std = 0.3081\n",
    "    image = (mean * image) + std\n",
    "    plt.imshow(image, cmap=\"gray\")\n",
    "\n",
    "\n",
    "# sample_data = next(iter(train_loader.dataset.data))\n",
    "sample_data = iter(train_loader.dataset.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T03:17:58.060360Z",
     "start_time": "2019-03-26T03:17:57.870545Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot 1st image\n",
    "plot_img(next(sample_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T03:17:58.269572Z",
     "start_time": "2019-03-26T03:17:58.068132Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot second image...\n",
    "plot_img(next(sample_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T03:17:58.474300Z",
     "start_time": "2019-03-26T03:17:58.277835Z"
    }
   },
   "outputs": [],
   "source": [
    "# ...plot third image...\n",
    "plot_img(next(sample_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model\n",
    "\n",
    "This first implementation will be built from scratch. This model will involve several different layers, including: Conv2d, MaxPool2d, Rectified linear unit (ReLU), View and Linear layer.\n",
    "\n",
    "The model will have the following sequence:\n",
    "\n",
    "- Conv2d\n",
    "- MaxPool2d\n",
    "- ReLU\n",
    "- Conv2d\n",
    "- Dropout\n",
    "- MaxPool2d\n",
    "- ReLU\n",
    "- View\n",
    "- FC\n",
    "- ReLU\n",
    "- Dropout\n",
    "- FC\n",
    "- Log_softmax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'The-model'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T03:18:04.055409Z",
     "start_time": "2019-03-26T03:18:04.044557Z"
    }
   },
   "outputs": [],
   "source": [
    "# implementation of strucutre described above\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Conv2d\n",
    "\n",
    "Conv2d applies a convolutional filter to the MNIST images. This operation can be thought of moving a window across the pixel grid, completing its evaluation at each step. As a simple example, we can illustrate a simple Conv1d operation with a small filter (kernel) operating on a length 7 tensor.\n",
    "\n",
    "The kernel has the values $[-0.5209, -0.0147, -0.4281]$\n",
    "\n",
    "The input tensor is $[0.2286, 2.4488, -0.9498, -0.5330, -0.6791, -0.6535, 0.6437]$\n",
    "\n",
    "The first output is:\n",
    "\n",
    "$$\n",
    "(-0.5209 \\times 0.2286) + (-0.0147 \\times -0.147) + (-0.4281 \\times -0.9498) = 0.2514\n",
    "$$\n",
    "\n",
    "The last output (totaling 5) is:\n",
    "\n",
    "$$\n",
    "(-0.5209 \\times -0.6791) + (-0.0147 \\times -0.6535) + (-0.4281 \\times -0.6437) = 0.0878\n",
    "$$\n",
    "\n",
    "The stride parameter determines the number of units that the kernel window moves forward after completing a convolution. In the example above, the stride is 1. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Conv2d'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T13:08:48.191259Z",
     "start_time": "2019-03-25T13:08:48.179273Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# simple convolution operation\n",
    "conv = nn.Conv1d(1, 1, 3, bias=False)\n",
    "sample = torch.randn(1, 1, 7)\n",
    "\n",
    "# print kernel weights and sampe tensor\n",
    "print(conv.weight)\n",
    "print(sample)\n",
    "\n",
    "# print the result of convolution\n",
    "conv(torch.autograd.Variable(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If the stride was set to 2, the filter would move by 2 positions, which would also result in a smaller output. Even a stride of 1 will reduce a size 7 1-d tensor to size 5. In order to ensure that the image size is not reduced, we can use padding, which adds additional zeros to allow the kernel sufficient room to move such that the output is the same size as the input.\n",
    "\n",
    "The weights in the kernel are initialized randomly and then gradient descent and backpropagation tune kernel values. The learned kernels identify different features, such as lines, curves, and specific object, such as eyes.\n",
    "\n",
    "Conv2d accepts a parameter kernel_size, which controls the size of the kernel. Common values are 1, 3, 5, and 7. The larger the kernel size, the larger the area that a filter can cover as it slides across the image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling\n",
    "\n",
    "Pooling layers commonly follow convolutional layers, because these layers have the benefit of reducing the size of the ocnvolutional layer outputs and the feature maps. Pooling fosters two benefits: first, it reduces the size of data to process, and second, it has a way of forcing the algorithm to not focus on small changes in the position of an object in an image. \n",
    "\n",
    "We will explore how ooling working using MaxPool2d. Similar to convolutional kernels, pooling accepts parameters for kernel size and stride, but differs in that it does not have an weights. Rather, it acts on the data generated by each filter from the previous layer. If the kernel size of a pooling operation is 2 x 2, then it considers a pixel grid of that size in an image and chooses the maximum value out of that 2 x 2 window. Similarly, average pooling average all values within the 2 x 2 kernel window and returns the average of the four values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Pooling'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonlinear activation - ReLU\n",
    "\n",
    "It is best practice to use a nonlinear layer after convolution or pooling is applied. Many of the most successful models use ReLU or some variation of ReLU. Whichever function is used, it gets applied to each element of the feature maps. In the case of ReLU, all negative values are set to 0 and non-negative values retain the same value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Nonlinear-activation-ReLU'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View\n",
    "\n",
    "A view layer flattens a matrix into a 1-d tensor. If our data has the shape 32 x 1 x 28 x 28, meaning we have a batch size of 32 images, where each image is a 28 by 28 pixel grid with one color channel, we need to be careful when flattening the data\n",
    "\n",
    "```python\n",
    "x.view(-1, 320)\n",
    "```\n",
    "\n",
    "The -1 in the view function above is an instruction to avoid flattening the data on the first dimension. Without this, the 32 images would be mixed together.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'View'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear layer\n",
    "\n",
    "At this point in the architecture, the data has been converted from a 2-d tensor to a 1-d tensor, and is then passed into a linear layer, followed by a nonlinear activation function. This example has two linear layer - the first is following by a ReLU nonlinear activation layer, and the second is followed by a log_softmax layer, which predicts which digit is contained in each image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Linear layer'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "This approach implements different logic for training and validation. In the train mode, dropout removes a set percentage of values, which does not happen in the validation phase. Also, we calculate gradients and change the model's parameter values, but this backpropagation step is not needed in the validation phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Training-the-model'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T03:18:12.528683Z",
     "start_time": "2019-03-26T03:18:12.506081Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "# transformation = transforms.Compose([transforms.ToTensor()\n",
    "#                                     ,transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "# train_dataset = datasets.MNIST('/main/tmp/', train = True, transform = transformation, download = True)\n",
    "# test_dataset = datasets.MNIST('/main/tmp/', train = False, transform = transformation, download = True)\n",
    "# # train_dataset = datasets.MNIST('C:\\\\Users\\\\petersont\\\\Desktop\\\\pytorch\\\\data', train = True, transform = transformation, download = True)\n",
    "# # test_dataset = datasets.MNIST('C:\\\\Users\\\\petersont\\\\Desktop\\\\pytorch\\\\data', train = False, transform = transformation, download = True)\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = 32, shuffle = True)\n",
    "# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = 32, shuffle = True)\n",
    "\n",
    "# for batch_idx, data in enumerate(train_loader.dataset):\n",
    "for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    print(batch_idx)\n",
    "    print(data)\n",
    "    #     print(target)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T03:29:54.950817Z",
     "start_time": "2019-03-26T03:29:54.941372Z"
    }
   },
   "outputs": [],
   "source": [
    "# function for training the model\n",
    "def fit(epoch, model, data_loader, phase=\"training\", volatile=False):\n",
    "    if phase == \"training\":\n",
    "        model.train()\n",
    "    if phase == \"validation\":\n",
    "        model.eval()\n",
    "        volatile = True\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(data_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile), Variable(target)\n",
    "        if phase == \"training\":\n",
    "            optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "\n",
    "        running_loss += F.nll_loss(output, target, reduction=\"sum\").data.item()\n",
    "        preds = output.data.max(dim=1, keepdim=True)[1]\n",
    "        running_correct += preds.eq(target.data.view_as(preds)).cpu().sum()\n",
    "        if phase == \"training\":\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    loss = running_loss / len(data_loader.dataset)\n",
    "    accuracy = 100.0 * running_correct / len(data_loader.dataset)\n",
    "\n",
    "    print(\n",
    "        \"{0} loss is {1} and {0} accuracy is {2}/{3} {4}\".format(\n",
    "            phase, loss, running_correct, len(data_loader.dataset), accuracy\n",
    "        )\n",
    "    )\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T03:46:06.389984Z",
     "start_time": "2019-03-26T03:35:52.268086Z"
    }
   },
   "outputs": [],
   "source": [
    "# run model for 20 epochs\n",
    "model = Net()\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "train_losses, train_accuracy = [], []\n",
    "val_losses, val_accuracy = [], []\n",
    "for epoch in range(1, 20):\n",
    "    epoch_loss, epoch_accuracy = fit(epoch, model, train_loader, phase=\"training\")\n",
    "    val_epoch_loss, val_epoch_accuracy = fit(\n",
    "        epoch, model, test_loader, phase=\"validation\"\n",
    "    )\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracy.append(epoch_accuracy)\n",
    "    val_losses.append(val_epoch_loss)\n",
    "    val_accuracy.append(val_epoch_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T03:46:06.618610Z",
     "start_time": "2019-03-26T03:46:06.393942Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot training and test loss\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, \"bo\", label=\"training loss\")\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, \"r\", label=\"validation loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T03:46:06.849733Z",
     "start_time": "2019-03-26T03:46:06.624565Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot accuracy\n",
    "plt.plot(\n",
    "    range(1, len(train_accuracy) + 1), train_accuracy, \"bo\", label=\"train accuracy\"\n",
    ")\n",
    "plt.plot(range(1, len(val_accuracy) + 1), val_accuracy, \"r\", label=\"val accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying dogs and cats – CNN from scratch\n",
    "\n",
    "The input dimensions for the first layer is 256 by 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Classifying-dogs-and-cats–CNN-from-scratch'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  model architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(56180, 500)\n",
    "        self.fc2 = nn.Linear(500, 50)\n",
    "        self.fc3 = nn.Linear(50, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = f.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = f.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for training the model\n",
    "def fit(epoch, model, data_loader, phase=\"training\", volatile=False):\n",
    "    if phase == \"training\":\n",
    "        model.train()\n",
    "    if phase == \"validation\":\n",
    "        model.eval()\n",
    "        volatile = True\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(data_loader):\n",
    "        if is_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile), Variable(target)\n",
    "        if phase == \"training\":\n",
    "            optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "\n",
    "        running_loss += F.nll_loss(output, target, size_average=False).data.item()\n",
    "        preds = output.data.max(dim=1, keepdim=True)[1]\n",
    "        running_correct += preds.eq(target.data.view_as(preds)).cpu().sum()\n",
    "        if phase == \"training\":\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    loss = running_loss / len(data_loader.dataset)\n",
    "    accuracy = 100.0 * running_correct / len(data_loader.dataset)\n",
    "\n",
    "    print(\n",
    "        f\"{phase} loss is {loss:{5}.{2}} and {phase} accuracy is {running_correct}/{len(data_loader.dataset)}{accuracy:{10}.{4}}\"\n",
    "    )\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run model for 20 epochs\n",
    "model = Net()\n",
    "if is_cude:\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "train_losses, train_accuracy = [], []\n",
    "val_losses, val_accuracy = [], []\n",
    "for epoch in range(1, 20):\n",
    "    epoch_loss, epoch_accuracy = fit(epoch, model, train_loader, phase=\"training\")\n",
    "    val_epoch_loss, val_epoch_accuracy = fit(\n",
    "        epoch, model, test_loader, phase=\"validation\"\n",
    "    )\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracy.append(epoch_accuracy)\n",
    "    val_losses.append(val_epoch_loss)\n",
    "    val_accuracy.append(val_epoch_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training and test loss\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, \"bo\", label=\"training loss\")\n",
    "plt.plot(range(1, len(val_losses) + 1), \"r\", label=\"validation loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot accuracy\n",
    "plt.plot(\n",
    "    range(1, len(train_accuracy) + 1), train_accuracy, \"bo\", label=\"train accuracy\"\n",
    ")\n",
    "plt.plot(range(1, len(val_accuracy) + 1), val_accuracy, \"r\", label=\"val accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Remarks - The training loss decreases for every iteration, but the validation loss plateaus and then gets worse. As for accuracy, the training accuracy increases after every iteration but the validation accuracy plateaus at 75%. Clearly, the model is not generalizing  to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying dogs and cats using transfer learning\n",
    "\n",
    "Transfer learning is the ability to resuse a trained algorithm on a similar dataset without training it from scratch. Just as human do not completely relearn form scratch how to recognize new images, computer can utilize previous understanding on other tasks when learning new data.\n",
    "\n",
    "The first few layers of a CNN model focus on smaller features, such as how a line or curve looks. It's not until the later layers that the CNN begins to look at more detailed features, such as eyes and fingers, and then the final layers seek to identify the exact category.\n",
    "\n",
    "An algorithm called VGG16 is one of the earliest algorithms to find success in ImageNet competitions. More modern algorithms exist, but this algorithm is popular due to its comparative simplicity. The VGG16 architecture contains five VGG blocks. Each block is a set of conolution layers, a nonlinear activation function and a max-pooling function. All parameters are tuned to achieve state-of-the-art results for classifying 1,000 categories.\n",
    "\n",
    "It is common practice to fine-tune only the last layers of the network, leaving the convolutional layers intact. We can do this because convolutional layers are mostly used for identifying shapes and features that are common across many image identificaiton problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Classifying-dogs-and-cats-using-transfer-learning'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and exploring a VGG16 model\n",
    "\n",
    "PyTorch includes several pre-trained model in the torchvision library. Below, we can see that the VGG16 model contains two sequential models: features and classifier. We are going to freeze the layers in the features sequential model and tune the layers in the classifier sequential model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Creating-and-exploring-a-VGG16-model'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load VGG16 model and review architecture\n",
    "from torchvision import models\n",
    "\n",
    "vgg = models.vgg16(pretrained=True)\n",
    "print(vgg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freezing the layers\n",
    "\n",
    "We will be freezing all of the layers in the features model. This is the model that contains the convolutional block. Freezing the weights in these layers has the effect of preventing the weights in the layers from being trained further than they already have been. Utilizing the \"knowledge\" that has already contributed to the calibration of these weights is the essence of transfer learning. This can be done simply by turning of the gradient component of the features model parameters:\n",
    "\n",
    "```python\n",
    "for param in vgg.features.parameters(): param.requires_grad = False\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Freezing-the-layers'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn off gradients for features model\n",
    "for param in vgg.features.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning VGG16\n",
    "\n",
    "We only need to classify dogs vs. cats, but the VGG16 model has been trained to classify objects belonging to 1,000 different categories. Do adapt the model to our purpose, we need to change the output features of the last layer from 1000 to 2. The 6th layer in the classifier sequential model contains the last/output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Fine-tuning-VGG16'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust output\n",
    "vgg.classifier[6].out_features = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass only the classifier parameters to the optimizer\n",
    "optimizer = optim.SGD(vgg.classifier.parameters(), lr=0.0001, momentum=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the VGG16 model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Training-the-VGG16-model'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for training the model\n",
    "def fit(epoch, model, data_loader, phase=\"training\", volatile=False):\n",
    "    if phase == \"training\":\n",
    "        model.train()\n",
    "    if phase == \"validation\":\n",
    "        model.eval()\n",
    "        volatile = True\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(data_loader):\n",
    "        if is_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile), Variable(target)\n",
    "        if phase == \"training\":\n",
    "            optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "\n",
    "        running_loss += F.nll_loss(output, target, size_average=False).data.item()\n",
    "        preds = output.data.max(dim=1, keepdim=True)[1]\n",
    "        running_correct += preds.eq(target.data.view_as(preds)).cpu().sum()\n",
    "        if phase == \"training\":\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    loss = running_loss / len(data_loader.dataset)\n",
    "    accuracy = 100.0 * running_correct / len(data_loader.dataset)\n",
    "\n",
    "    print(\n",
    "        f\"{phase} loss is {loss:{5}.{2}} and {phase} accuracy is {running_correct}/{len(data_loader.dataset)}{accuracy:{10}.{4}}\"\n",
    "    )\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEED to get the data loaders previously created\n",
    "# train model for 20 epochs\n",
    "train_losses, train_accuracy = [], []\n",
    "val_losses, val_accuracy = [], []\n",
    "for epoch in range(1, 20):\n",
    "    epoch_loss, epoch_accuracy = fit(epoch, vgg, train_data_loader, phase=\"training\")\n",
    "    val_epoch_loss, val_epoch_accuracy = fit(\n",
    "        epoch, vgg, valid_data_loader, phase=\"validation\"\n",
    "    )\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracy.append(epoch_accuracy)\n",
    "    val_losses.append(val_epoch_loss)\n",
    "    val_accuracy.append(val_epoch_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training and test loss\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, \"bo\", label=\"training loss\")\n",
    "plt.plot(range(1, len(val_losses) + 1), \"r\", label=\"validation loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot accuracy\n",
    "plt.plot(\n",
    "    range(1, len(train_accuracy) + 1), train_accuracy, \"bo\", label=\"train accuracy\"\n",
    ")\n",
    "plt.plot(range(1, len(val_accuracy) + 1), val_accuracy, \"r\", label=\"val accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can make various adjustments to improve the model's generalization abilities\n",
    "# adjust dropout from 0.5 to 0.2\n",
    "for layer in vgg.classifier.children():\n",
    "    if type(layer) == nn.Dropout:\n",
    "        layer.p = 0.2\n",
    "\n",
    "train_losses, train_accuracy = [], []\n",
    "val_losses, val_accuracy = [], []\n",
    "for epoch in range(1, 20):\n",
    "    epoch_loss, epoch_accuracy = fit(epoch, vgg, train_data_loader, phase=\"training\")\n",
    "    val_epoch_loss, val_epoch_accuracy = fit(\n",
    "        epoch, vgg, valid_data_loader, phase=\"validation\"\n",
    "    )\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracy.append(epoch_accuracy)\n",
    "    val_losses.append(val_epoch_loss)\n",
    "    val_accuracy.append(val_epoch_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempt to improve generalization through data augmentation. randomly flip certain images\n",
    "# and/or rotate image by a small angle. the torchvision library provides functions for doing\n",
    "# operations like these\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "train = ImageFolder(\"dogsandcats/train/\", train_transform)\n",
    "valid = ImageFolder(\"dogsandcats/valid/\", simple_transform)\n",
    "\n",
    "# execute\n",
    "train_losses, train_accuracy = [], []\n",
    "val_losses, val_accuracy = [], []\n",
    "for epoch in range(1, 20):\n",
    "    epoch_loss, epoch_accuracy = fit(epoch, vgg, train_data_loader, phase=\"training\")\n",
    "    val_epoch_loss, val_epoch_accuracy = fit(\n",
    "        epoch, vgg, valid_data_loader, phase=\"validation\"\n",
    "    )\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracy.append(epoch_accuracy)\n",
    "    val_losses.append(val_epoch_loss)\n",
    "    val_accuracy.append(val_epoch_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating pre-convoluted features\n",
    "\n",
    "Each epoch can take significant time to train. There is a trick that can be implemented to speed up this process. Here is the key idea - when we freeze the convolutional layers and then train the model, the input to the full connected layers (vgg.classifier) is always the same. We can think of the convolutional block (vgg.features) as a function that has learned weights that will no longer be chaning during training. So we can calculate the unchanging output of the convolutional block, and store this has something that is passed into the linear layers, rather than calculating and passing forward the convolutional output for each and every epoch. If the weights in the feature block are frozen, there is no benefit to redoing the calculation of these frozen weights every epoch.\n",
    "\n",
    "In  the code below, the preconv feat method takes in the dataset, and vgg model and returns the convoluted features with the associated labels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Calculating-pre-convoluted-features'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# capture pre-convoluted features\n",
    "vgg = models.vgg16(pretrained=True)\n",
    "vgg = vgg.cuda()\n",
    "features = vgg.features\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "    train, batch_size=32, num_workers=3, shuffle=False\n",
    ")\n",
    "validn_data_loader = torch.utils.data.DataLoader(\n",
    "    valid, batch_size=32, num_workers=3, shuffle=False\n",
    ")\n",
    "\n",
    "\n",
    "def preconvfeat(dataset, model):\n",
    "    conv_features = []\n",
    "    labels_list = []\n",
    "    for data in dataset:\n",
    "        inputs, labels = data\n",
    "        if is_cuda == True:\n",
    "            input, labels = inputs.cuda(), labels.cuda()\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        output = model(inputs)\n",
    "        conv_features.extend(output.data.cpu().numpy())\n",
    "        labels_list.extend(labels.data.cpu().numpy())\n",
    "    conv_features = np.concatenate([[feat] for feat in conv_features])\n",
    "    return (conv_features, labels_list)\n",
    "\n",
    "\n",
    "conv_feat_train, labels_train = preconvfeat(train_data_loader, features)\n",
    "conv_feat_val, labels_val = preconvfeat(valid_data_loader, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fuction for creating the data\n",
    "class My_dataset(Dataset):\n",
    "    def __init__(self, feat, labels):\n",
    "        self.conv_feat = feat\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.conv_feat)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.conv_feat[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "# DataLoader function\n",
    "train_feat_dataset = My_dataset(conv_feat_train, labels_train)\n",
    "val_feat_dataset = My_dataset(conv_feat_val, labels_val)\n",
    "\n",
    "train_feat_loader = DataLoader(train_feat_dataset, batch_size=64, shuffle=True)\n",
    "val_feat_loader = DataLoader(val_feat_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model, using code similar to previous example, this time using vgg.classifier\n",
    "train_losses, train_accuracy = [], []\n",
    "val_losses, val_accuracy = [], []\n",
    "for epoch in range(1, 20):\n",
    "    epoch_loss, epoch_accuracy = fit(\n",
    "        epoch, vgg.classifier, train_feat_loader, phase=\"training\"\n",
    "    )\n",
    "    val_epoch_loss, val_epoch_accuracy = fit(\n",
    "        epoch, vgg.classifier, valid_feat_loader, phase=\"validation\"\n",
    "    )\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracy.append(epoch_accuracy)\n",
    "    val_losses.append(val_epoch_loss)\n",
    "    val_accuracy.append(val_epoch_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding what a CNN model learns\n",
    "\n",
    "Three are techniques that can be used to interpret what happens inside of CNNs. This section will explore two popular techniques for understanding CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Understanding-what-a-CNN-model-learns'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing outputs from intermediate layers\n",
    "\n",
    "Visualizing the outputs from the intermediate layers will help facilitate understanding of how the input image is being transformed across the layers. An image's output from each layer can be thought of as an activation, and there are several methods we can use to extract that output from intermediate layer. PyTorch has a method called register_forward_hook, which can pass a function for extracting output from a certain layer.\n",
    "\n",
    "PyTorch models only maintain the output of the last layer by default. We can review how to extract outputs from the model first, and then emulate this approach for extracting activations from the intermediate layers.\n",
    "\n",
    "In the code below, the init function takes a model and the number for the layer for which the outputs need to be extracted as arguments. We then call the register_forward_hook method on the layer and pass in a function (hook_fn). When PyTorch is passing images through the layer, it calls the function that is given the to register_forward_hook method. This returns a handle, which can be used to deregister the function that is pass to register_forward_hook.\n",
    "\n",
    "The register_forward_hook method passes three values to the function given to it:\n",
    "\n",
    "- module - allows access to the layer itself\n",
    "- input - refers to the data that is flowing through the layer\n",
    "- output - allows access to the transformed inputs, or activation of the layer. This will be stored in the features variable in the LayerActivations class below.\n",
    "\n",
    "The function 'remove' takes the hook from the init function and deregisters the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Visualizing-outputs-from-intermediate-layers'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pre-trained VGG model, from which we will extract outputs of a certain layer\n",
    "vgg = models.vgg16(pretrained=True).cuda()\n",
    "\n",
    "# class instructing PyTorch to store the output of a layer to the features variable\n",
    "class LayerActivations:\n",
    "    features = None\n",
    "\n",
    "    def __init__(self, model, layer_num):\n",
    "        self.hook = model(layer_num).register_forward_hook(self.hook_fn)\n",
    "\n",
    "    def hook_fn(self, module, input, output):\n",
    "        self.features = output.cpu()\n",
    "\n",
    "    def remove(self):\n",
    "        self.hook.remove()\n",
    "\n",
    "\n",
    "conv_out = LayerActivations(vgg.features, 0)\n",
    "o = vgg(Variable(img.cuda()))\n",
    "conv_out.remove()\n",
    "act = conv_out.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize activations created by the first convolution layer\n",
    "fit = plt.figure(figsize=(20, 50))\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=0.8, hspace=0, wspace=0.2)\n",
    "for i in range(30):\n",
    "    ax = fig.add_subplot(12, 5, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(act[0][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "second layer here to visualize the activations after the ReLU layer. do you see eyes in the fifth image on the second row? it is said that when models aren't performing, these tricks can be used to troubleshoot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The early layers appear to detect lines and edges, and the last layers learn higher-level features that are less clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing weights of the CNN layer\n",
    "\n",
    "This is more straightforward that retrieving activations. Model weights can be accessed through the state_dict function, which returns a dictionary. The keys are layers and the weight are values.\n",
    "\n",
    "In the code below, we return a set of weights belonging to a filter that is of size 3 by 3. Each filter is trained to identify certain patterns in the image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Visualizing-weights-of-the-CNN-layer'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve weights from state_dict\n",
    "vgg.state_dict().keys()\n",
    "cnn_weights = vgg.state_dict()[\"features.0.weight\"].cpu()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
