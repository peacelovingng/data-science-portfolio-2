{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Chapter 3 - Diving deep into neural networks__\n",
    "\n",
    "1. [Import](#Import)\n",
    "1. [Layers – fundamental blocks of neural networks](#Layers–fundamental-blocks-of-neural-networks)\n",
    "1. [Non-linear activations](#Non-linear-activations)\n",
    "    1. [Sigmoid](#Sigmoid)\n",
    "    1. [Tanh](#Tanh)\n",
    "    1. [ReLU](#ReLU)\n",
    "    1. [Leaky ReLU](#Leaky-ReLU)\n",
    "1. [The PyTorch way of building deep learning algorithms](#The-PyTorch-way-of-building-deep-learning-algorithms)\n",
    "1. [Loss functions](#Loss-functions)\n",
    "1. [Optimizing network architecture](#Optimizing-network-architecture)\n",
    "1. [Image classification using deep learning](#Image-classification-using-deep-learning)\n",
    "    1. [Loading data into PyTorch tensors](#Loading-data-into-PyTorch-tensors)\n",
    "    1. [Building the network architecture](#Building-the-network-architecture)\n",
    "    1. [Training-the-model](#Training-the-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Import'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Standard libary and settings\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "import itertools\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "from IPython.core.display import display, HTML; display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "# Data extensions and settings\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold = np.inf, suppress = True)\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.options.display.float_format = '{:,.6f}'.format\n",
    "\n",
    "# pytorch tools\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "\n",
    "# Visualization extensions and settings\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers – fundamental blocks of neural networks\n",
    "\n",
    "\n",
    "PyTorch users built-in functionality referred to as layers to perform high-level operations in the execution of a neural network framework. There are many different types of layers. The linear layer, for example, applies the following linear transformation:\n",
    "\n",
    "$$\n",
    "\\textbf{Y} = \\textbf{W}\\textbf{x} + \\textbf{b}\n",
    "$$\n",
    "\n",
    "Multiple layers can be executed in sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Layers–fundamental-blocks-of-neural-networks'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=10, out_features=5, bias=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this linear function layer accepts a tensor of size 1- and outputs a tensor of size 5 after\n",
    "# applying a linear transformation\n",
    "myLayer = nn.Linear(in_features = 10, out_features = 5, bias = True)\n",
    "myLayer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1216, -0.2672, -0.0230, -0.0486,  0.3428]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feed tensor of random numbers into myLayer\n",
    "inp = Variable(torch.randn(1,10))\n",
    "myLayer = nn.Linear(in_features = 10, out_features = 5, bias = True)\n",
    "myLayer(inp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.2805, -0.1138, -0.0764, -0.1654, -0.2234,  0.2143, -0.3120,  0.2113,\n",
       "         -0.1404, -0.0071],\n",
       "        [-0.0678, -0.1961, -0.1274, -0.1762,  0.2015, -0.0586,  0.2808,  0.0115,\n",
       "         -0.2720,  0.1110],\n",
       "        [-0.2890, -0.2794,  0.0452,  0.0296, -0.0463,  0.1876, -0.2650, -0.1025,\n",
       "         -0.2593, -0.2105],\n",
       "        [-0.2610,  0.2440,  0.0402, -0.1724, -0.2122,  0.0567, -0.1304,  0.2527,\n",
       "         -0.0198, -0.1663],\n",
       "        [ 0.2486,  0.1449, -0.1611,  0.1039, -0.1841, -0.0791, -0.2486,  0.1288,\n",
       "          0.2283,  0.2717]], requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# review weights - a 5 row by 10 column matrix reprsenting the 50 connections between the 10 input\n",
    "# nodes and 5 output nodes\n",
    "myLayer.weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.2456, -0.2412, -0.2469, -0.1864,  0.2878], requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# review biases - 1 bias unit for each output node\n",
    "myLayer.bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1 weights:\n",
      "\n",
      "Parameter containing:\n",
      "tensor([[-0.0443,  0.1742, -0.1631,  0.2770,  0.1587, -0.0178, -0.0912, -0.0489,\n",
      "         -0.0830,  0.1217],\n",
      "        [ 0.0587, -0.1834,  0.3142,  0.2368, -0.3012, -0.1321,  0.3139, -0.3065,\n",
      "         -0.1032,  0.3081],\n",
      "        [-0.1573,  0.2013, -0.0747, -0.2215,  0.1140, -0.0723,  0.1311, -0.0481,\n",
      "         -0.3138, -0.2861],\n",
      "        [ 0.2715,  0.1204, -0.2546,  0.2739, -0.2667, -0.1236,  0.0196, -0.1411,\n",
      "         -0.1535,  0.1025],\n",
      "        [ 0.0460,  0.1520, -0.0842,  0.2361, -0.1351, -0.0012,  0.1637, -0.0214,\n",
      "          0.0751,  0.2445]], requires_grad=True)\n",
      "\n",
      "Layer 2 weights:\n",
      "\n",
      "Parameter containing:\n",
      "tensor([[-0.3129, -0.3872,  0.3887,  0.3883, -0.1893],\n",
      "        [-0.0532, -0.0024,  0.0653, -0.1922, -0.1748]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# create a network with multiple linear layers. NOTE - stacking two linear layer is not typically\n",
    "# done in practice because linear transformation are unable to capture more complicated non-linear\n",
    "# patterns\n",
    "myLayer1 = nn.Linear(10, 5)\n",
    "myLayer2 = nn.Linear(5, 2)\n",
    "myLayer2(myLayer1(inp))\n",
    "\n",
    "print('Layer 1 weights:\\n')\n",
    "print(myLayer1.weight)\n",
    "\n",
    "print('\\nLayer 2 weights:\\n')\n",
    "print(myLayer2.weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-linear activations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Non-linear-activations'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid\n",
    "\n",
    "The sigmoid activation function takes the following form:\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "It returns a number between 0 and 1. A large negative number returns a value close to zero, and a large positive number returns a value close to 1.\n",
    "\n",
    "This function is not especially popular in modern implementations of neural networks because when the output of the sigmoid function is close to 0 or 1, the gradients for the layers are close to zero. Consequently, the learnable parameters of the previous layer get gradients close to zero and the weights do not get adjusted often. This results in 'dead' neurons.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Sigmoid'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tanh\n",
    "\n",
    "Similar to the sigmoid function, the tanh function also squashes real-valued inputs to a narrow range. The tanh range is -1 to 1. This activation function encounters the same issue as the sigmoid function when it handles extreme values close to -1 and 1. However, it is preferred to sigmoid because the output of tanh is centered at zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Tanh'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU\n",
    "\n",
    "ReLU is very popular activation function. It hsa the mathematical form:\n",
    "\n",
    "$$\n",
    "f(x) = max(0,x)\n",
    "$$\n",
    "\n",
    "ReLU squashes any input that is negative to an output value of zero, and leaves positive numbers as they are. Some pros of ReLU include:\n",
    "\n",
    "- It helps the optimizer find the right weights sooner.\n",
    "- It speeds up convergence of stochastic gradient descent\n",
    "- It is computationally inexpensive, as we are simply thresholding rather than calculating.\n",
    "\n",
    "A disadvantage of ReLU is that when a large gradient passes through it during backpropagation, the function can become non-responsive, resulting in a 'dead' neuron. This can be controlled by carefully choosing the model learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 0., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple illustration of ReLU activation\n",
    "sample = Variable(torch.Tensor([[1, 2, -1, -1]]))\n",
    "myRelu = nn.ReLU()\n",
    "myRelu(sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'ReLU'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leaky ReLU\n",
    "\n",
    "Leaky ReLU attempts to solve the drawback of ReLU by insteading setting negative inputs to 0, it returns very small numbers suc as 0.001. It may provide superior performance but that is not guaranteed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Leaky ReLU'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  2.0000, -0.0100, -0.0100]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple illustration of Learky ReLU activation\n",
    "sample = Variable(torch.Tensor([[1, 2, -1, -1]]))\n",
    "myRelu = nn.LeakyReLU()\n",
    "myRelu(sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The PyTorch way of building deep learning algorithms\n",
    "\n",
    "\n",
    "All networks in PyTorch are implemented as classes by subclassing a PyTorch class called nn.Module. The class should also implement the init and forward methods. Inside the init function we initialize any layers, and the forward method passes our input data into the layer that were initialized in the init method before return the final output. The non-linear functions are often used in the forward function, and other time those function appear in the init method. Both are viable options depending on the use case.\n",
    "\n",
    "The types of layers used, and the number of neurons in the various layers, are generally driven by the type of problem. If we are modelling a regression problem, the output layer will have one neuron, which will contain the predicted continuous value. If we are modelling a multiclass classification problem, the output layer will have a number of neurons equal to the number of unique classes in the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'The-PyTorch-way-of-building-deep-learning-algorithms'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example PyTorch architecture. in the code below, super is used to pass arguments\n",
    "# of the child class (nn.Module) to the parents class\n",
    "class MyFirstNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MyFirstNetwork, self).__init(())\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.layer2 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def __forward__(self, input):\n",
    "        out = self.layer1(input)\n",
    "        out = nn.ReLU(out)\n",
    "        out = self.layer2(out)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss functions\n",
    "\n",
    "With the network architecture in place, there are two important remaining steps. The first is calculating how well the nwtwork is performing, and the second is optimizing the model weights to improve the model of performance.\n",
    "\n",
    "An optimizer accepts a scalar value, so the loss function needs to generate a scalar value, which is the value we are trying to minimize. In some use case, such as predicting where an obstacle is on the road and classifying whether or not that obstacle is a pedestrian or not, requires two or more loss functions. Even still, these loss function outputs would be combined to a single scalar for the optimizer to minimize.\n",
    "\n",
    "We will begin by implementing a built-in loss function called Mean squared error. This is a basic loss funciton used in regression problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Loss-functions'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE loss function\n",
    "loss = nn.MSELoss()\n",
    "input = Variable(torch.randn(3,5), requires_grad = True)\n",
    "target = Variable(torch.randn(3,5))\n",
    "output = loss(input, target)\n",
    "output.backward()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification problems, we use a corss-entropy loss. This loss function calculates the loss of a classification network by predicting the probabilities that an observation belongs to each class. Given that these are probabilities, the probabilities add up to 1. Cross-entropy loss increases when the predicted probability that an observation belongs to the correct class diverges from the correct probability. If the model makes a prediction similar to the actual label, the cross-entropy loss will be lower. In other words, if the predictions are bad, the loss will be high and if the predictions are good the loss will be low. Same as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom cross-entropy loss function\n",
    "def cross_entropy(true_label, predictions):\n",
    "    if true_label == 1:\n",
    "        return -log(prediction)\n",
    "    else:\n",
    "        return -log(1 - prediction)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch implementation of cross-entropy\n",
    "loss = nn.CrossEntropyLoss()\n",
    "input = Variable(torch.randn(3, 5), requires_grad = True)\n",
    "target = Variable(torch.LongTensor(3).random_(5))\n",
    "output = loss(input,target)\n",
    "output.backward()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample of other PyTorch loss functions include:\n",
    "\n",
    "- L1 loss - used as a regularizer\n",
    "- NLL loss - used for classification problems and allows us to use specific weights to handle imbalanced datasets\n",
    "- NLL loss2d - Used for pixel-wise classification, mostly used for image segmentation problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing network architecture\n",
    "\n",
    "With the loss calculated, we need to optimize the weights to reduce the loss in each subsequent iteration of the learning process, and consequently improve the accuracy of the algorithm. Some optimizers include:\n",
    "\n",
    "- ADADELTA\n",
    "- Adagrad\n",
    "- Adam\n",
    "- SparseAdam\n",
    "- Adamax\n",
    "- ASGD\n",
    "- LBFGS\n",
    "- RMSProp\n",
    "- Rprop\n",
    "- SGD\n",
    "\n",
    "Discussion regarding optimization algorithm specifics and pros and cons will follow in subsequent chapters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Optimizing-network-architecture'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 3 required positional arguments: 'input_size', 'hidden_size', and 'output_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-7845f4b38841>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# step involved in creating an optimizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMyFirstNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# once the optimizer is created, we need to call zero_grad() inside the loop to clear out the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 3 required positional arguments: 'input_size', 'hidden_size', and 'output_size'"
     ]
    }
   ],
   "source": [
    "# step involved in creating an optimizer\n",
    "model = MyFirstNetwork()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)\n",
    "\n",
    "# once the optimizer is created, we need to call zero_grad() inside the loop to clear out the \n",
    "# gradients calculated in the preivous optimizer call. loss.backward() calculates the  \n",
    "# gradients (quantity by which the weights/biases need to change), we call optimizer.step() which \n",
    "# applies the changes to the weights/biases\n",
    "for input, target in dataset:\n",
    "    optimizer.zero_grad()\n",
    "    output = model(input)\n",
    "    loss = loss_fn(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image classification using deep learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Image-classification-using-deep-learning'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images: 25000\n"
     ]
    }
   ],
   "source": [
    "# ideally, the images are arranged in a way where there is a train and test folder, and within\n",
    "# each of these folders is a subfolder for each class containg the respective images\n",
    "path = '..\\\\..\\\\kaggle\\\\DogsVsCats\\\\data\\\\train'\n",
    "localPath = 'C:\\\\Users\\\\petersont\\\\Desktop\\\\kaggle'\n",
    "\n",
    "# glob returns all files in the specified path. iglob is an alternative that creates a geneerator\n",
    "# rather than pulling everything into memory\n",
    "import shutil\n",
    "from glob import glob\n",
    "files = glob(os.path.join(path, '*.jpg'))\n",
    "numImages = len(files)\n",
    "print('Total number of images: {}'.format(numImages))\n",
    "\n",
    "# create a shuffled index for creating a validation dataset\n",
    "shuffle = np.random.permutation(numImages)\n",
    "\n",
    "# create directories with label names\n",
    "for t in ['trainSubset','validSubset']:\n",
    "    if not os.path.exists(os.path.join(localPath, t)):\n",
    "        os.mkdir(os.path.join(localPath, t))\n",
    "    for folder in ['dog','cat']:\n",
    "        if not os.path.exists(os.path.join(localPath, t, folder)):\n",
    "            os.mkdir(os.path.join(localPath, t, folder))\n",
    "        \n",
    "# copy a small subset of images to the validation folder\n",
    "for i in shuffle[0:2000]:\n",
    "    \n",
    "    folder = files[i].split('\\\\')[-1].split('.')[0]\n",
    "    image = files[i].split('\\\\')[-1]\n",
    "    shutil.copy2(os.path.join(path, image)\n",
    "                 ,os.path.join(localPath, 'trainSubset', folder))\n",
    "    \n",
    "# copy a small subset of images to the training folder\n",
    "for i in shuffle[2000:4001]:\n",
    "    folder = files[i].split('\\\\')[-1].split('.')[0]\n",
    "    image = files[i].split('\\\\')[-1]\n",
    "    shutil.copy2(os.path.join(path, image)\n",
    "                 ,os.path.join(localPath, 'validSubset', folder))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data into PyTorch tensors\n",
    "\n",
    "The PyTorch package torchvision.datasets provides a class called ImageFolder. It is common practice to perform the following preprocessing steps\n",
    "\n",
    "1. Resize all images to the same size. In most deep learnin architectures this is expected.\n",
    "2. Normalize the dataset with the mean and standard deviation of the dataset.\n",
    "3. Convert the image dataset to a PyTorch tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Loading-data-into-PyTorch-tensors'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images using the ImageFolder class\n",
    "simple_transform = transforms.Compose([transforms.Scale((224, 224))\n",
    "                                      ,transforms.ToTensor()\n",
    "                                      ,transforms.Normalize([0.485, 0.456,0.406]\n",
    "                                                            ,[0.229, 0.224, 0.225]\n",
    "                                                            )])\n",
    "train = ImageFolder('dogsandcats/train/', simple_transform)\n",
    "valid = ImageFolder('dogsandcats/valid/simple_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the train object holds all the image and associated labels. there are two important attributes: one\n",
    "# that gives a mapping between class and the associated index in the dataset, and a second that gives a\n",
    "# list of classes\n",
    "\n",
    "print(train.class_to_idx)\n",
    "print(train.classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize a sample of the data by reshaping and denormalizing the values\n",
    "def imshow(inp):\n",
    "    inp = inp.numpy().transpose([1, 2, 0])\n",
    "    mean = np.array([0.229, 0.2224, 0.225])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "\n",
    "imshow(train[50][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the DataLoader class to teturn a batch of images. num_workers is used for parallelization. it is common\n",
    "# practice to use a number of works fewer than the number of core available in the machine\n",
    "train_data_gen = torch.utils.data.DataLoader(train, batch_size = 64, num_workers = 3)\n",
    "valid_data_gen = torch.utils.data.DataLoader(valid, batch_size = 64, num_workers = 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the network architecture\n",
    "\n",
    "PyTorch includes many prebuilt neural network architectures. ResNet is a popular deep learning algorithm that we will use in this example. The inner-workings of the algorithm will be covered in more detail in chapter 5. For now we will implement a model using the torchvision.models module.\n",
    "\n",
    "When the argument 'pretrained' is set to True, the weights of the algorithm are already tuned for a particular ImageNet classification problem of predicting 1,000 different categories. It is already trained to predict the 1,000 ImageNet categories with a state-of-art level of accuracy. \n",
    "\n",
    "Since the weights are based on a particular classification task, we cannot use it directly. Generally speaking, algorithms tend to work better when started with fine-tuned weights rather than random weights, even if the use case is different, but we need to make adjustments. To tailor this for our use case, which involves predicting only one of two categories of dogs and cats, we alter the last layer of the ResNet model. This linear layer is changed such that the output only has 2 nodes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Building-the-network-architecture'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  implement ResNet. when the argument pretrained = True \n",
    "model_ft = models.resnet(pretrained = true)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "if is_cude:\n",
    "    model_ft = model_ft.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review model architecture\n",
    "print(model_ft)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Training-the-model'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create loss function based on cross-entropy and an optimizer based on SGD\n",
    "# The StepLR function dynamically changes the learning rate\n",
    "learning_rate = 0.001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr = 0.001, momentum = 0.9)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size = 7, gamme = 0.1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for running the model for several epochs\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs = 25):\n",
    "    since = time.time()\n",
    "    \n",
    "    best_model_wts = model.state_dict()\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 30)\n",
    "        \n",
    "        # each epoch has a training and validation phase\n",
    "        for phase in ['train','valid']\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train(True) # set model to training mode\n",
    "            else:\n",
    "                model.train(False)\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # iterate over data\n",
    "            for data in dataloaders[phase]:\n",
    "                # get inputs\n",
    "                inputs, labels = data\n",
    "\n",
    "                # wrap inputs and labels in a PyTorch Variable\n",
    "                if is_cuda:\n",
    "                    inputs = Variable(inputs.cuda())\n",
    "                    labels = Variable(labels.cuda())\n",
    "                else:\n",
    "                    inputs = Variable(inputs)\n",
    "                    labels = Variable(labels)\n",
    "\n",
    "                # zero out the gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs.data, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # backward + optimize only if in training step\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.data.item()\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects / dataset_sizes[phase]\n",
    "\n",
    "            print('{} loss: {} acc: {}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase = 'valid' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = model.state_dict()\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complet in {:.0f}m {:.0f}s'.format(time_elapsed // 60 time_elapsed % 60))\n",
    "    print('Best validation accuracy: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function above does the following:\n",
    "\n",
    "- Passes the images through the model and calculates the loss\n",
    "- Backpropagates during the training phase only. The validation phase does not involve weight adjustment\n",
    "- The loss is accumulated across batches for each epoch\n",
    "- the best moel is stored and validation accuracy is printed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
