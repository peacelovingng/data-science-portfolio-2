{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Chapter 3 - Diving deep into neural networks__\n",
    "\n",
    "1. [Import](#Import)\n",
    "1. [Layers – fundamental blocks of neural networks](#Layers–fundamental-blocks-of-neural-networks)\n",
    "1. [Non-linear activations](#Non-linear-activations)\n",
    "    1. [Sigmoid](#Sigmoid)\n",
    "    1. [Tanh](#Tanh)\n",
    "    1. [ReLU](#ReLU)\n",
    "    1. [Leaky ReLU](#Leaky-ReLU)\n",
    "1. [The PyTorch way of building deep learning algorithms](#The-PyTorch-way-of-building-deep-learning-algorithms)\n",
    "1. [](#)\n",
    "1. [](#)\n",
    "1. [](#)\n",
    "1. [](#)\n",
    "1. [](#)\n",
    "1. [](#)\n",
    "1. [](#)\n",
    "1. [](#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Import'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Standard libary and settings\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "import itertools\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "from IPython.core.display import display, HTML; display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "# Data extensions and settings\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold = np.inf, suppress = True)\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.options.display.float_format = '{:,.6f}'.format\n",
    "\n",
    "# pytorch tools\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "\n",
    "# Visualization extensions and settings\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Layers – fundamental blocks of neural networks\n",
    "\n",
    "\n",
    "PyTorch users built-in functionality referred to as layers to perform high-level operations in the execution of a neural network framework. There are many different types of layers. The linear layer, for example, applies the following linear transformation:\n",
    "\n",
    "$$\n",
    "\\textbf{Y} = \\textbf{W}\\textbf{x} + \\textbf{b}\n",
    "$$\n",
    "\n",
    "Multiple layers can be executed in sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Layers–fundamental-blocks-of-neural-networks'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=10, out_features=5, bias=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this linear function layer accepts a tensor of size 1- and outputs a tensor of size 5 after\n",
    "# applying a linear transformation\n",
    "myLayer = nn.Linear(in_features = 10, out_features = 5, bias = True)\n",
    "myLayer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2251, -0.4737,  0.3719, -0.0200,  0.1534]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feed tensor of random numbers into myLayer\n",
    "inp = Variable(torch.randn(1,10))\n",
    "myLayer = nn.Linear(in_features = 10, out_features = 5, bias = True)\n",
    "myLayer(inp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.1058,  0.0475, -0.1087,  0.2467, -0.0248, -0.0428, -0.1703, -0.0919,\n",
       "         -0.2566,  0.1299],\n",
       "        [ 0.0930,  0.0178,  0.2726, -0.2643, -0.2069, -0.2342, -0.1533,  0.1485,\n",
       "          0.3090,  0.2094],\n",
       "        [-0.2547,  0.1003, -0.0412,  0.0987,  0.2688, -0.0383,  0.0628, -0.2042,\n",
       "          0.1284,  0.1556],\n",
       "        [ 0.0162,  0.0529,  0.2653,  0.1618,  0.0902, -0.2695, -0.2497,  0.2101,\n",
       "          0.2665, -0.2102],\n",
       "        [-0.0355,  0.2456,  0.0833, -0.0785,  0.0578,  0.1010, -0.1808,  0.2231,\n",
       "          0.0419, -0.2705]], requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# review weights - a 5 row by 10 column matrix reprsenting the 50 connections between the 10 input\n",
    "# nodes and 5 output nodes\n",
    "myLayer.weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.0694,  0.1742,  0.1781,  0.2609, -0.1346], requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# review biases - 1 bias unit for each output node\n",
    "myLayer.bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1 weights:\n",
      "\n",
      "Parameter containing:\n",
      "tensor([[ 0.2979,  0.1654,  0.2752,  0.1576, -0.1869, -0.0838,  0.2349, -0.0735,\n",
      "         -0.1603,  0.1928],\n",
      "        [-0.1303, -0.0924, -0.2520,  0.1261, -0.1366,  0.0719,  0.2128,  0.2838,\n",
      "         -0.0277,  0.0056],\n",
      "        [-0.2782,  0.2058,  0.0431, -0.2941,  0.2881, -0.0104, -0.1394,  0.1627,\n",
      "         -0.0551,  0.1939],\n",
      "        [-0.3060,  0.0599, -0.1529, -0.1520, -0.1153,  0.0205, -0.1728,  0.1376,\n",
      "         -0.0044,  0.2036],\n",
      "        [-0.1926, -0.2671,  0.0307,  0.2874,  0.2832, -0.1891, -0.0860, -0.1008,\n",
      "          0.2368, -0.0984]], requires_grad=True)\n",
      "\n",
      "Layer 2 weights:\n",
      "\n",
      "Parameter containing:\n",
      "tensor([[-0.2163, -0.1791, -0.4168,  0.0401, -0.2722],\n",
      "        [-0.0049, -0.2296,  0.3167, -0.4103,  0.1534]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# create a network with multiple linear layers. NOTE - stacking two linear layer is not typically\n",
    "# done in practice because linear transformation are unable to capture more complicated non-linear\n",
    "# patterns\n",
    "myLayer1 = nn.Linear(10, 5)\n",
    "myLayer2 = nn.Linear(5, 2)\n",
    "myLayer2(myLayer1(inp))\n",
    "\n",
    "print('Layer 1 weights:\\n')\n",
    "print(myLayer1.weight)\n",
    "\n",
    "print('\\nLayer 2 weights:\\n')\n",
    "print(myLayer2.weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-linear activations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Non-linear-activations'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid\n",
    "\n",
    "The sigmoid activation function takes the following form:\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "It returns a number between 0 and 1. A large negative number returns a value close to zero, and a large positive number returns a value close to 1.\n",
    "\n",
    "This function is not especially popular in modern implementations of neural networks because when the output of the sigmoid function is close to 0 or 1, the gradients for the layers are close to zero. Consequently, the learnable parameters of the previous layer get gradients close to zero and the weights do not get adjusted often. This results in 'dead' neurons.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Sigmoid'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tanh\n",
    "\n",
    "Similar to the sigmoid function, the tanh function also squashes real-valued inputs to a narrow range. The tanh range is -1 to 1. This activation function encounters the same issue as the sigmoid function when it handles extreme values close to -1 and 1. However, it is preferred to sigmoid because the output of tanh is centered at zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Tanh'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU\n",
    "\n",
    "ReLU is very popular activation function. It hsa the mathematical form:\n",
    "\n",
    "$$\n",
    "f(x) = max(0,x)\n",
    "$$\n",
    "\n",
    "ReLU squashes any input that is negative to an output value of zero, and leaves positive numbers as they are. Some pros of ReLU include:\n",
    "\n",
    "- It helps the optimizer find the right weights sooner.\n",
    "- It speeds up convergence of stochastic gradient descent\n",
    "- It is computationally inexpensive, as we are simply thresholding rather than calculating.\n",
    "\n",
    "A disadvantage of ReLU is that when a large gradient passes through it during backpropagation, the function can become non-responsive, resulting in a 'dead' neuron. This can be controlled by carefully choosing the model learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 0., 0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple illustration of ReLU activation\n",
    "sample = Variable(torch.Tensor([[1, 2, -1, -1]]))\n",
    "myRelu = nn.ReLU()\n",
    "myRelu(sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'ReLU'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leaky ReLU\n",
    "\n",
    "Leaky ReLU attempts to solve the drawback of ReLU by insteading setting negative inputs to 0, it returns very small numbers suc as 0.001. It may provide superior performance but that is not guaranteed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Leaky ReLU'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  2.0000, -0.0100, -0.0100]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple illustration of Learky ReLU activation\n",
    "sample = Variable(torch.Tensor([[1, 2, -1, -1]]))\n",
    "myRelu = nn.LeakyReLU()\n",
    "myRelu(sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The PyTorch way of building deep learning algorithms\n",
    "\n",
    "\n",
    "All networks in PyTorch are implemented as classes by subclassing a PyTorch class called nn.Module. The class should also implement the init and forward methods. Inside the init function we initialize any layers, and the forward method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'The-PyTorch-way-of-building-deep-learning-algorithms'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = ''></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = ''></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = ''></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = ''></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
