{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Chapter 7 - Generative Networks__\n",
    "\n",
    "1. [Import](#Import)\n",
    "1. [Neural style transfer](#Neural-style-transfer)\n",
    "    1. [Loading the data](#Loading-the-data)\n",
    "    1. [Creating the VGG model](#Creating-the-VGG-model)\n",
    "    1. [Content loss](#Content-loss)\n",
    "    1. [Style loss](#Style-loss)\n",
    "    1. [Extracting the losses](#Extracting-the-losses)\n",
    "    1. [Creating loss function for each layers](#Creating-loss-function-for-each-layers)\n",
    "    1. [Creating the optimizer](#Creating-the-optimizer)\n",
    "    1. [Training](#Training)\n",
    "1. [Generative adversarial networks](#Generative-adversarial-networks)\n",
    "1. [](#)\n",
    "1. [](#)\n",
    "1. [](#)\n",
    "1. [](#)\n",
    "1. [](#)\n",
    "1. [](#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Import'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libary and settings\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "import itertools\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "from IPython.core.display import display, HTML; display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "# Data extensions and settings\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold = np.inf, suppress = True)\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.options.display.float_format = '{:,.6f}'.format\n",
    "\n",
    "# pytorch tools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "# Visualization extensions and settings\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural style transfer\n",
    "\n",
    "Given a content image and a style image, generate a new image that combines the content of the content image and the style of the style image.\n",
    "\n",
    "The style of an image is captured across multiple layers in a CNN by a technique called gram matrix. This calculates the correlation between the features maps captures across each layer. Similarly styled images have similar values for a gram matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Neural-style-transfer'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Loading-the-data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix the image size\n",
    "imsize = 512\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# convert image for training with VGG model\n",
    "prep = transforms.Compose([transforms.Resize(imsize)\n",
    "                          ,transforms.ToTensor()\n",
    "                          ,transforms.Lambda(lambda x: x[torch.LongTensor([2, 1, 0])]) # turn to BGR\n",
    "                          ,transforms.Normalize(mean = [0.40760392, 0.45795686, 0.48501961]\n",
    "                                               std = [1, 1, 1]) # subtract imagenet mean \n",
    "                          ,transforms.Lambda(lambda x: x.mul_(255))\n",
    "                          ])\n",
    "\n",
    "# convert the generated image back to a format that can be visualized\n",
    "postpa = transforms.Compose([transforms.Lambda(lambda x: x.mul_(1. / 255))\n",
    "                            ,transforms.Normalize(mean = [0.40760392, 0.45795686, 0.48501961]\n",
    "                                                 ,std = [1, 1, 1])\n",
    "                            ,transforms.Lambda(lambda x: x[torch.LongTensor([2, 1, 0 ])]) # turn to RGB\n",
    "                            ])\n",
    "postpb = transforms.Compose([transform.ToPILImage()])\n",
    "\n",
    "# ensure data in the image does not cross the permissible range of values\n",
    "def postp(tensor):\n",
    "    t = postpa(tensor)\n",
    "    t[t > 1] = 1\n",
    "    t[t < 0 ] = 0\n",
    "    img = postpb(t)\n",
    "    return img\n",
    "\n",
    "# ease data loading\n",
    "def image_loader(image_name):\n",
    "    image = Image.open(image_name)\n",
    "    image = Variable(prep(image))\n",
    "    image = image.unsqueeze(0)\n",
    "    return image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "style_img = image_loader('images/vangogh_starry_night.jpg')\n",
    "convert_img = image_loader('images/tuebinge_neckarfront.jpg')\n",
    "\n",
    "opt_img = Variable(content_img.data.clone(), requires_grad = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the VGG model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Creating-the-VGG-model'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a VGG model, grabbing only the convolution block (features) and freeze the parameters\n",
    "vgg = vgg19(pretrained = True).features\n",
    "\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Content-loss'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "target_layer = dummy_fn(content_img)\n",
    "noise_layer = dummy_fn(noise_img)\n",
    "criterion = nn.MSELoss()\n",
    "content_loss = criterion(target_layer, noise_layer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Style loss\n",
    "\n",
    "Style loss is the MSE of the gram matrix generated for each feature map. Envision a feature map with dimensions representing bacth_size by color channels and values (which in this example is itself a 3 by 3 window). To calculate the gram matrix, the 9 values in each channel are flattened into a 9 value vector, and then the correlation coefficient is calculated by multiplying the flattened vector by its transpose.\n",
    "\n",
    "The class below is written in a way so that it can be used like another PyTorch layer.  First, the batch, channel, height and width are maintained, and then the features are reshaped such that the batch and channel dimensions remain intact, and the values are flattened along the height and width dimension. The gram matrix is calculated using the PyTorch batch matrix multiplication function torch.bmm(), which will multiply the flattened values with its transposed vector. The final step normalizes the values of the gram matrix by dividing it by the number of elements. Without this, a feature map with an especially high number of values would tend to dominate the score.\n",
    "\n",
    "The second class below calculates the style loss, which is also implemented as a PyTorch layer. It calculates the MSE between the input image gram matrix and the style image gram matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Style-loss'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "class GramMatrix(nn.Module):\n",
    "    def forward(self, input):\n",
    "        b, c, h, w = input.size()\n",
    "        features = input.view(b, c, h * w)\n",
    "        gram_matrix = torch.bmm(features, features.tranpose(1, 2))\n",
    "        gram_matrix.div_(h * w)\n",
    "        return gram_matrix\n",
    "\n",
    "class StyleLoss(nn.Module):\n",
    "    def forward(self, inputs, targets):\n",
    "        out = nn.MSELoss()(GramMatrix()(inputs), targets)\n",
    "        return (out)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the losses\n",
    "\n",
    "Just as activations can be extracted from convolution layers use the register_forward_hook(), we can extract losses of different convolutional layers required to calculate style loss and content loss. The key difference is that rather than extracting from a single layer, we need to extract outputs from several layers.\n",
    "\n",
    "In the class below, the init method takes in the model on which we will call register_forward_hook() as well as the layer ID number for layers from which we will extract the outputs. The init method's for loop iterates through the layer IDs and registers the forward hook required the pull outputs.\n",
    "\n",
    "hook_fn is passed to register_forward_hook() and is called by PyTorch after the current layer is registered. Inside the function, the output is captured and stored in the features array.\n",
    "\n",
    "Lastly, the remove function is called to clear the outputs captured, otherwise this process may result in memory issues.\n",
    "\n",
    "The extract_layers function extracts the outputs for the style and content images. Inside this funciton, we call LayerActivations and pass in the model and the layer numbers. We follow this by ensuring we have an empty list. The an image is passed through the model, and we will review the outputs generated in the features array \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Extracting-the-losses'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "class LayerActivations():\n",
    "    features = []\n",
    "    \n",
    "    def __init__(self, model, layer_nums):\n",
    "        self.hooks = []\n",
    "        for layer_num in layer_nums:\n",
    "            self.hooks.append(model[layer_num].register_forward_hook(self.hook_fn))\n",
    "            \n",
    "    def hook_fn(self, module, input, output):\n",
    "        self.features.append(output)\n",
    "        \n",
    "    def remove(self):\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "\n",
    "def extract_layers(layers, img, model = None):\n",
    "    la = LayerActivations(model, layers)\n",
    "    la.features = []\n",
    "    out = model(img)\n",
    "    la.remove()\n",
    "    return la.features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the outputs needs to be detached from the graphs that created them\n",
    "content_targets = extract_layers(content_layers, content_img, model = vgg)\n",
    "style_targets = extract_layers(style_layers, style_img, model = vgg)\n",
    "\n",
    "content_targets [t.detach() for t in content_targets]\n",
    "style_targets = [GramMatrix()(t).detach() for t in style_targets]\n",
    "\n",
    "# add all targets into one list\n",
    "target = style_targets + content_targets\n",
    "\n",
    "# specify layers to be extracted\n",
    "style_layers = [1, 6, 11, 20, 25]\n",
    "content_layers = [21]\n",
    "loss_layers = style_layers + content_layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the optimizer needs a single scalar to minimize, so the losses frome each layer are summed\n",
    "style_weights = [1e3 / n**2 for n in [64, 128, 256, 512, 512]]\n",
    "content_weights = [1e0]\n",
    "weights = style_weights + content_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review layers selected\n",
    "print(vgg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating loss function for each layers\n",
    "\n",
    "We need to create the loss layers for the separate style losses and content losses. The variable loss_fns is a list containing several style loss objects and content loss objects that are based on the lengths of the arrays created.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Creating-loss-function-for-each-layers'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "loss_fns = [StyleLoss()] * len(style_layers) + [nn.MSELoss()] * len(content_layers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the optimizer\n",
    "\n",
    "An optimizer typically receives the parameters of the model, but in this case we are using VGG models as feature extracts, and therefore cannot pass the VGG parameters. Instead, we will provide the parameters of the opt_img variable. These are the parameters that will be optimized to make the image have the required content and style.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Creating-the-optimizer'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "optimizer = optim.LBFGS([opt_img])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "This training method will calculate loss for multiple layers. Each time the optimizer is called, it will chang the input image so that the content and style gets nearer to the target's content and style.\n",
    "\n",
    "In the function below, each iteration involves calcualting the output from different layers of the VGG model using extract_layers. The only values that change here are the values of the style image (opt_img). Once the outputs are calculated, we calcualte the losses by iterations through the outputs and passing them to the associated loss functions along with the targets. The losses are summed and the backward function is called.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Training'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "max_iter = 500\n",
    "show_iter = 50\n",
    "n_iter = [0]\n",
    "\n",
    "while n_iter[0] <= max_iter:\n",
    "    def closure()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out = extract_layers(loss, opt_img, model = vgg)\n",
    "        layer_losses = [weights[a] * loss_fns[a](A, targets[a]) for a, A in enumerate(out)]\n",
    "        loss = sum(layer_losses)\n",
    "        loss.backward()\n",
    "        n_iter[0] += 1\n",
    "        \n",
    "        print(loss)\n",
    "        \n",
    "        if n_iter[0] % show_iter == (show_iter - 1):\n",
    "            print('Iteration: {}, loss: {}'.format(n_iter[0] + 1, loss.data[0]))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "optimizer.step(closure)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative adversarial networks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Generative-adversarial-networks'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = ''></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = ''></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = ''></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = ''></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = ''></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = ''></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
