{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Chapter 6 - Deep learning with sequence data and text__\n",
    "\n",
    "1. [Import](#Import)\n",
    "1. [Word embedding](#Word-embedding)\n",
    "    1. [Training word embedding by building a sentiment classifier](#Training-word-embedding-by-building-a-sentiment-classifier)\n",
    "    1. [torchtext.datasets](#torchtextdatasets)\n",
    "    1. [Building vocabulary](#Building-vocabulary)\n",
    "    1. [Generate batches of vectors](#Generate-batches-of-vectors)\n",
    "1. [](#)\n",
    "1. [](#)\n",
    "1. [](#)\n",
    "1. [](#)\n",
    "1. [](#)\n",
    "1. [](#)\n",
    "1. [](#)\n",
    "1. [](#)\n",
    "1. [](#)\n",
    "1. [](#)\n",
    "1. [](#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Import'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Standard libary and settings\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "import itertools\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "from IPython.core.display import display, HTML; display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "# Data extensions and settings\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold = np.inf, suppress = True)\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.options.display.float_format = '{:,.6f}'.format\n",
    "\n",
    "# pytorch tools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "# Visualization extensions and settings\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embedding\n",
    "\n",
    "Word embedding is a popular way of representing text data in problems that are solved by deep learning algorithms. This technique provides a dense representation of a word filled with floats. The vector dimension varies based on the vocabulary size. It is common to use a word emebedding of dimension size 50, 100, 256, 300 and occassionally 1,000. This size is a hyperparameter.\n",
    "\n",
    "Contrasting this with on-hot encoding, if we have a vocabulary of 20,000 words, then we end up with 20,000 x 20,000 numbers, the vast majority of which will be zero. This same vocabulary can be represented as a word emebedding of size 20,000 x (dimension size).\n",
    "\n",
    "One method for creating word embeddings is to start with dense vectors of random numbers for each token, then train a model (such as a document classifier or sentiment classifier). The floating point numbers in the vectors, which collectively represent the tokens, are adjusted in a way such that semantically 'close' words will have similar represented.\n",
    "\n",
    "Word embeddings may not be feasible if there isn't enough data. In these case, embeddings trained by some other machine learning algorithm can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Word-embedding'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training word embedding by building a sentiment classifier\n",
    "\n",
    "Using a dataset called IMDB (which contains movie reviews), we will build a sentiment classifier. In the processing training the model, we will also train word embedding for the words in the IMDB dataset. This will be done using a library called torchtext.\n",
    "\n",
    "The torchtext.data module has a class called Field, which defines how the data needs to be read and tokenized. Below, we define two Field objects, one for the text itself and a second for the labels. The Field constructor also accepts a tokenize argument, which by default use the str.split function. We can override this by passing in a tokenizer of choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Training-word-embedding-by-building-a-sentiment-classifier'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "from torchtext import data\n",
    "text = data.Field(lower = True, batch_first = True, fix_length = 20)\n",
    "label = data.Field(sequential = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torchtext.datasets\n",
    "\n",
    "torchtext.datasets provides wrappers for several different datasets, such as IMDB. This utility abstracts away the process of downloading, tokenizing and splitting the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'torchtextdatasets'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download IMDB\n",
    "train, test = datasets.IMDB.splits(text, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "print('train.fields', train.fields)\n",
    "\n",
    "# results\n",
    "print(vars(train[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building vocabulary\n",
    "\n",
    "We can use the build_vocab method to take in an object from which we will build a vocabulary. Below, we pass in the train object, and using the dim argument, initialize vectors with pretrained mebeddings of dimension 300. The max_size instance limits the number of words in the vocabulary, and min_freq removes any word which has not occurred more than 10 times.\n",
    "\n",
    "Once the vocabulary is built we can obtain different values such as frequency, word index and the vector representation of each word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Building-vocabulary'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the vocabulary\n",
    "text.build_vocab(train, vectors = GloVe(name = '6B', dim = 300), max_size = 10000, min_freq = 10)\n",
    "label.build(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print word frequencies\n",
    "print(text.vocab.freqs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print word vectors, which displays the 300 dimension vector for each word\n",
    "print(text.vocab.vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print word and their indexes\n",
    "print(text.vocab.stoi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate batches of vectors\n",
    "\n",
    "BucketIterator is a tools that helps to batch the text and replace the words with the index number of the individual words. The following code creates iterators that generate batches for the train and test objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Generate-batches-of-vectors'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "train_iter, test_iter = data.BuckerIterator.splits((train, test), batch_size = 18, device = -1, shuffle = True)\n",
    "\n",
    "batch = next(iter(train_iter))\n",
    "print(batch.text)\n",
    "\n",
    "print(batch.label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = ''></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
