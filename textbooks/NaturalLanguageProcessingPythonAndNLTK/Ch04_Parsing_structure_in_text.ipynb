{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  __Chapter 4 - Parsing structure in text__\n",
    "\n",
    "1. [Import](#Import)\n",
    "1. [Why we need parsing](#Why-we-need-parsing)\n",
    "1. [Different types of parsers](#Different-types-of-parsers)\n",
    "1. [A regex parser](#A-regex-parser)\n",
    "1. [Dependency parsing](#Dependency-parsing)\n",
    "1. [Chunking](#Chunking)\n",
    "1. [Information extraction](#Information-extraction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Import'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Standard libary and settings\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "import itertools\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "# Data extensions and settings\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(threshold=np.inf, suppress=True)\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.options.display.float_format = \"{:,.6f}\".format\n",
    "\n",
    "# Modeling extensions\n",
    "import nltk\n",
    "\n",
    "# Visualization extensions and settings\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why we need parsing\n",
    "\n",
    "Parsing is useful for defining a set of rules that can be used as a tempalte to write sentence and arrange words in the proper order. As humans learn their native language in childhood, we instinctively take to the rules of our language. We try to replicate this process in NLP through text parsing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Why-we-need-parsing'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'CPG' from 'nltk' (C:\\Users\\petersont\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-8490ebd3adb8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCPG\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m to_grammar = nltk.CPG.fromstring(\"\"\"\n\u001b[0;32m      4\u001b[0m \u001b[0mS\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mNP\u001b[0m \u001b[0mVP\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mVP\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mV\u001b[0m \u001b[0mNP\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'CPG' from 'nltk' (C:\\Users\\petersont\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# grammar rules with a very limited vocabulary and generic rules\n",
    "# some example sentences would be 'president eats apple' and 'obama drinks coke'\n",
    "from nltk import CPG\n",
    "\n",
    "to_grammar = nltk.CPG.fromstring(\n",
    "    \"\"\"\n",
    "S -> NP VP\n",
    "VP -> V NP\n",
    "V -> \"eats\" | \"drinks\"\n",
    "NP -> Det N\n",
    "Det -> \"a\" | \"an\" | \"the\"\n",
    "N -> \"president\" | \"Obama\" | \"apple\" | \"coke\"\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different types of parsers\n",
    "\n",
    "A parser is a procedural interpretation of grammar that searches through a defined space and finds an optimal pathway to create a sentence. There are several parsers available:\n",
    "\n",
    "__Recursive descent parser__\n",
    "\n",
    "This straightforward form of parsing that involves a top-down process where the parser attempt to verify that the syntax of the input stream is correct as read from left to right. A basic summary of this operation is that the process reads characters from the input stream and evaluates compliance  with the grammar rules. For example, the parser looks ahead one character and advances the the process when it gets a proper match.\n",
    "\n",
    "__Shift-reduced parser__\n",
    "\n",
    "This is a simple bottom-up parser, which involves comparing the left hand side of a grammar rule set and replaces the placeholders with the output specified on the right hand side of the grammar rules.\n",
    "\n",
    "__Chart parser__\n",
    "\n",
    "A chart paraser is a dynamic programming method that stores intermediate results and reuses them when it makes se.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Different-types-of-parsers'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A regex parser\n",
    "A regex parser uses regular expressions uses grammar rules and POS-tagged strings. The parser uses these rules to parse sentences and generate a tree.\n",
    "\n",
    "In the example below, regex is used to analyze the POS tag of each word. The rules define the kinds of patterns that are believed to create phrases. For example, anything that has a POS tag matching `{<DT>? <JJ>* <NN>*}`, which means it starts with a determiner, followed by an adjective, and then followed by a noun is most likely a noun phrase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'A-regex-parser'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Mr./NNP\n",
      "  Obama/NNP\n",
      "  (VP\n",
      "    (V played/VBD)\n",
      "    (NP a/DT big/JJ role/NN)\n",
      "    (PP (P in/IN) (NP the/DT health/NN insurance/NN bill/NN))))\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "from nltk.chunk.regexp import *\n",
    "\n",
    "chunk_rules = ChunkRule(\"<.*>+\", \"chunk everything\")\n",
    "reg_parser = RegexpParser(\n",
    "    \"\"\"\n",
    "NP: {<DT>? <JJ>* <NN>*}     # NP\n",
    "P: {<IN>}                   # preposition\n",
    "V: {<V.*>}                  # verb\n",
    "PP: {<P> <NP>}              # PP -> P NP\n",
    "VP: {<V> <NP|PP>*}          # VP -> V (NP | PP)*\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "test_sent = \"Mr. Obama played a big role in the health insurance bill\"\n",
    "test_sent_pos = nltk.pos_tag(nltk.word_tokenize(test_sent))\n",
    "parsed_out = reg_parser.parse(test_sent_pos)\n",
    "print(parsed_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependency parsing\n",
    "\n",
    "Dependency parsing deploys the philosophy that each word is connected with other words by a direct link. These links are called dependencies. Phrase structure trees capture the relationship between words and phrases, and then between phrases. Dependency trees, on the other hand, would evaluate a sentence such as \"The big dog runs\" and conclude, among other things, that big is dependent on dog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Dependency-parsing'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stanford parser\n",
    "from nltk.parse.stanford import StanfordParser\n",
    "\n",
    "english_parser = StanfordParser(\n",
    "    \"stanford-parser.jar\", \"standford-parser-3.4-models.jar\"\n",
    ")\n",
    "english_parser.raw_parse_sents(\"this is the english parser test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking\n",
    "\n",
    "Chunking is a shallow parsing technique that tries to determine combinations of words that together constitute some meaning. A chunk can be thought of as the minimal unit needed to convey a certain message. For example, \"The President speaks about the health care reforms\" can be separated into two chunks. \"The President\" is noun dominated, and is consequently identified as a noun phrase (NP) and the remaining part of the sentence is dominated by the verb \"speaks\", which makes it a verb phrase (VP). Within this second component, there is a subchunk \"The Health Care Reforms\", which is an NP. What we have at each tier is a set of non-overlapping groups of words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Chunking'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  The/DT\n",
      "  prime/JJ\n",
      "  minister/NN\n",
      "  (VP announced/VBD he/PRP)\n",
      "  (VP had/VBD asked/VBN)\n",
      "  the/DT\n",
      "  chief/JJ\n",
      "  government/NN\n",
      "  whip/NN\n",
      "  ,/,\n",
      "  Philip/NNP\n",
      "  Ruddock/NNP\n",
      "  ,/,\n",
      "  to/TO\n",
      "  (VP call/VB)\n",
      "  a/DT\n",
      "  special/JJ\n",
      "  party/NN\n",
      "  room/NN\n",
      "  meeting/NN\n",
      "  for/IN\n",
      "  9am/CD\n",
      "  on/IN\n",
      "  Monday/NNP\n",
      "  to/TO\n",
      "  (VP consider/VB)\n",
      "  the/DT\n",
      "  spill/NN\n",
      "  motion/NN\n",
      "  ./.)\n",
      "(S\n",
      "  (NP The/DT prime/JJ minister/NN)\n",
      "  announced/VBD\n",
      "  he/PRP\n",
      "  had/VBD\n",
      "  asked/VBN\n",
      "  (NP the/DT chief/JJ government/NN whip/NN)\n",
      "  ,/,\n",
      "  (NP Philip/NNP Ruddock/NNP)\n",
      "  ,/,\n",
      "  to/TO\n",
      "  call/VB\n",
      "  (NP a/DT special/JJ party/NN room/NN meeting/NN)\n",
      "  for/IN\n",
      "  9am/CD\n",
      "  on/IN\n",
      "  (NP Monday/NNP)\n",
      "  to/TO\n",
      "  consider/VB\n",
      "  (NP the/DT spill/NN motion/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# Basic chunking example\n",
    "from nltk.chunk.regexp import *\n",
    "\n",
    "test_sent = \"The prime minister announced he had asked the chief government whip, Philip Ruddock, to call a special party room meeting for 9am on Monday to consider the spill motion.\"\n",
    "test_sent_pos = nltk.pos_tag(nltk.word_tokenize(test_sent))\n",
    "\n",
    "rule_vp = ChunkRule(r\"(<VB.*>)?(<VB.*>)+(<PRP>)?\", \"Chunk VPs\")\n",
    "parser_vp = RegexpChunkParser([rule_vp], chunk_label=\"VP\")\n",
    "print(parser_vp.parse(test_sent_pos))\n",
    "\n",
    "rule_np = ChunkRule(r\"(<DT>?<RB>?)?<JJ|CD>*(<JJ|CD><,>)*(<NN.*>)+\", \"Chunk NPs\")\n",
    "parser_np = RegexpChunkParser([rule_np], chunk_label=\"NP\")\n",
    "print(parser_np.parse(test_sent_pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information extraction\n",
    "\n",
    "A typical NLP pipeline involves these steps:\n",
    "\n",
    "1. Raw text intake\n",
    "2. Sentence tokenization (list of strings)\n",
    "3. Word tokenization (list of lists of strings)\n",
    "4. Part of speech tagging (tuples)\n",
    "5. Named entity detection\n",
    "6. Relationship extraction\n",
    "\n",
    "The only topic not covered so far is relations extraction. Just like it sounds, this is the process of extracting relationships that exists between entities. For example authorship is a relationship that defines how a book and the writer of that book is defined.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Information-extraction'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple pipeline\n",
    "f = open()\n",
    "text = f.read()\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "tagged_sentences = [nltk.pos_tag(sentence) for sentence in tokenized_sentences]\n",
    "for sent in tagged_sentences:\n",
    "    print(nltk.ne_chunk())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ORG: 'WHYY'] 'in' [LOC: 'Philadelphia']\n",
      "[ORG: 'McGlashan &AMP; Sarrail'] 'firm in' [LOC: 'San Mateo']\n",
      "[ORG: 'Freedom Forum'] 'in' [LOC: 'Arlington']\n",
      "[ORG: 'Brookings Institution'] ', the research group in' [LOC: 'Washington']\n",
      "[ORG: 'Idealab'] ', a self-described business incubator based in' [LOC: 'Los Angeles']\n",
      "[ORG: 'Open Text'] ', based in' [LOC: 'Waterloo']\n",
      "[ORG: 'WGBH'] 'in' [LOC: 'Boston']\n",
      "[ORG: 'Bastille Opera'] 'in' [LOC: 'Paris']\n",
      "[ORG: 'Omnicom'] 'in' [LOC: 'New York']\n",
      "[ORG: 'DDB Needham'] 'in' [LOC: 'New York']\n",
      "[ORG: 'Kaplan Thaler Group'] 'in' [LOC: 'New York']\n",
      "[ORG: 'BBDO South'] 'in' [LOC: 'Atlanta']\n",
      "[ORG: 'Georgia-Pacific'] 'in' [LOC: 'Atlanta']\n"
     ]
    }
   ],
   "source": [
    "# relations extaction workflow - orgaization in location\n",
    "import re\n",
    "\n",
    "IN = re.compile(r\".*\\bin\\b(?!\\b.+ing)\")\n",
    "\n",
    "for doc in nltk.corpus.ieer.parsed_docs(\"NYT_19980315\"):\n",
    "    for rel in nltk.sem.extract_rels(\"ORG\", \"LOC\", doc, corpus=\"ieer\", pattern=IN):\n",
    "        print(nltk.sem.rtuple(rel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PER: 'Miller'] \"started talking. ``Fresh Air'' started as a local show in\" [LOC: 'Philadelphia']\n",
      "[PER: 'Drudge'] 'be sued in the' [LOC: 'District of Columbia']\n",
      "[PER: 'Alan Brody'] ', an independent media analyst in' [LOC: 'Scarsdale']\n",
      "[PER: 'Jerry Yang'] ', co-founder of the company, which is based in' [LOC: 'Santa Clara']\n",
      "[PER: 'Frank'] \"'s account of growing up in Depression\" [LOC: 'Ireland']\n",
      "[PER: 'Wilson'] 'has always stirred the strongest reactions. Especially in' [LOC: 'Europe']\n",
      "[PER: 'Dominique Sanda'] ') in' [LOC: 'Milan']\n",
      "[PER: 'Tania Leon'] 'in' [LOC: 'Geneva']\n"
     ]
    }
   ],
   "source": [
    "# relations extaction workflow - people in location\n",
    "import re\n",
    "\n",
    "IN = re.compile(r\".*\\bin\\b(?!\\b.+ing)\")\n",
    "\n",
    "for doc in nltk.corpus.ieer.parsed_docs(\"NYT_19980315\"):\n",
    "    for rel in nltk.sem.extract_rels(\"PERSON\", \"LOC\", doc, corpus=\"ieer\", pattern=IN):\n",
    "        print(nltk.sem.rtuple(rel))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
