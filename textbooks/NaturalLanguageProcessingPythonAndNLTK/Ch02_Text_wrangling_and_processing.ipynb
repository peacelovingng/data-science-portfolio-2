{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  __Chapter 2 - Text wrangling and processing__\n",
    "\n",
    "1. [Import](#Import)\n",
    "1. [Text wrangling](#Text-wrangling)\n",
    "1. [Tokenization](#Tokenization)\n",
    "1. [Stemming](#Stemming)\n",
    "1. [Lemmatization](#Lemmatization)\n",
    "1. [Stop word removal](#Stop-word-removal)\n",
    "1. [Spelling correction](#Spelling-correction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Import'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Standard libary and settings\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "import itertools\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "from IPython.core.display import display, HTML; display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "# Data extensions and settings\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold = np.inf, suppress = True)\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.options.display.float_format = '{:,.6f}'.format\n",
    "\n",
    "# Modeling extensions\n",
    "import nltk\n",
    "\n",
    "# Visualization extensions and settings\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text wrangling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Text-wrangling'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this is an example sent.', 'the sentence splitter will split on sent markers.', 'Ohh really!', '!']\n"
     ]
    }
   ],
   "source": [
    "# split into sentences using sent_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "inputstring = 'this is an example sent. the sentence splitter will split on sent markers. Ohh really!!'\n",
    "\n",
    "all_sent = sent_tokenize(inputstring)\n",
    "print(all_sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a custom sentence splitter\n",
    "import nltk.tokenize.punkt\n",
    "tokenizer = nltk.tokenize.PunktSentenceTokenizer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "A token, aka a word, is the minimal unit that a machine can evaluate and process. Tokenization is the process of splitting text data down to the point of building a collection of individual words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Tokenization'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'everyone!', 'hola', 'gr8']\n"
     ]
    }
   ],
   "source": [
    "# simple split using basic Python\n",
    "s = 'Hi everyone! hola gr8'\n",
    "print(s.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi', 'everyone', '!', 'hola', 'gr8']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple split nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'everyone', 'hola', 'gr8']\n",
      "['8']\n",
      "['Hi', 'everyone', '!', 'hola', 'gr8']\n",
      "['Hi everyone! hola gr8']\n"
     ]
    }
   ],
   "source": [
    "# basic examples with various tokenizers\n",
    "from nltk.tokenize import regexp_tokenize, wordpunct_tokenize, blankline_tokenize\n",
    "print(regexp_tokenize(s, pattern = '\\w+'))\n",
    "print(regexp_tokenize(s, pattern = '\\d+'))\n",
    "print(wordpunct_tokenize(s))\n",
    "print(blankline_tokenize(s))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "\n",
    "Stemming is the process of reducing a token down to its stem, i.e. reducing 'eating' down to 'eat'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Stemming'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eat\n",
      "shop\n"
     ]
    }
   ],
   "source": [
    "# basic stemming examples\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "pst = PorterStemmer()\n",
    "lst = LancasterStemmer()\n",
    "print(lst.stem('eating'))\n",
    "print(pst.stem('shopping'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "\n",
    "Lemmatization is a more precide way of converting tokens to their roots. Lemmatization uses context and parts of speech to determine how to get to the root, aka lemma.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Lemmatization'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dog'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lemmatization that uses wordnet, a semantic dictionary for performing lookups\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wlem = WordNetLemmatizer()\n",
    "wlem.lemmatize(\"dogs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop word removal\n",
    "\n",
    "Stop word removal is the process is removing words that occur commonly across documents and generally have no significance. These stop words lists are typically hand-curated lists of words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Stop-word-removal'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test,', 'test']\n"
     ]
    }
   ],
   "source": [
    "# remove stop words from a sample sentence\n",
    "from nltk.corpus import stopwords\n",
    "stoplist = stopwords.words('english')\n",
    "text = 'this is just a test, only a test'\n",
    "cleanwords = [word for word in text.split() if word not in stoplist]\n",
    "print(cleanwords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spelling correction\n",
    "\n",
    "NLTK includes an algorithm called edit-distance that can be used to perform fuzzy string matching.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Spelling-correction'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate Levenshtein distance between two words\n",
    "from nltk.metrics import edit_distance\n",
    "edit_distance('rain','shine')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
