{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  __Chapter 5 - NLP_applications__\n",
    "\n",
    "1. [Import](#Import)\n",
    "1. [News article summarization](#News-article-summarization)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Import'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Standard libary and settings\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "import itertools\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "# Data extensions and settings\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(threshold=np.inf, suppress=True)\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.options.display.float_format = \"{:,.6f}\".format\n",
    "\n",
    "# Modeling extensions\n",
    "import nltk\n",
    "\n",
    "# Visualization extensions and settings\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News article summarization\n",
    "\n",
    "Summarizing a news article is an interesting application of NLP that requires a process that can attain deep understanding of not only individual sentences, but also how sentences relate to each other to form an overall message. We also want to be able to understand genre and theme.\n",
    "\n",
    "This can be attained through an approach that ranks individual sentences based on their importance. Generally speaking, a sentence that has a higher number of entities and nouns has greater importance. We can use this rule of thumb to create an importance score.\n",
    "\n",
    "This is a simplistic but powerful rule that can be expanded upon by weighting senetences at the beginning of the article higher than the sentences at the end of article. This is based on the assumption that an article will typically fronload the details that tend to summarize the article and the sentences following the introduction will provide the details that expand upon the summary.\n",
    "\n",
    "Another possibility is to evaluate the term frequency - inverse document frequency (TF-IDF) of each and every word. This enables us to identify discriminatory words, which assumes that sentences that include discriminatory words are especially important. By calculating the TF-IDF for each word, we can determine the average score of each sentence and prioritize accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'News-article-summarization'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new article summarization example\n",
    "import sys\n",
    "\n",
    "f = open(\"nyt.txt\", \"r\")\n",
    "news_content = f.read()\n",
    "\n",
    "results = []\n",
    "for sent_no, sentence in enumerate(nltk.sent_tokenize(news_content)):\n",
    "    no_of_tokens = len(nltk.word_tokenize(sentence))\n",
    "    print(no_of_tokens)\n",
    "\n",
    "    tagged = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "\n",
    "    # count the numeber of nouns\n",
    "    no_of_nouns = len([word for word, pos in tagged if pos in [\"NN\", \"NNP\"]])\n",
    "\n",
    "    # count named entities\n",
    "    ners = nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sentence)), binary=False)\n",
    "    no_of_ners = len([chunk for chunk in ners if hasattr(chunk, \"node\")])\n",
    "    score = (no_of_ners + no_of_nouns) / float(no_of_tokens)\n",
    "\n",
    "    results.append((sent_no, no_of_tokens, no_of_nouns, no_of_ners, score, sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print top scores\n",
    "for sent in sorted(results, key=lambda x: x[4], reverse=True):\n",
    "    print(sent[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['american', 'and', 'are', 'at', 'baltimore', 'ban', 'between', 'by', 'call', 'camden', 'communities', 'community', 'conference', 'crises', 'davis', 'defining', 'department', 'director', 'doubt', 'during', 'ease', 'effort', 'enforcement', 'ferguson', 'house', 'in', 'is', 'justice', 'law', 'minority', 'mo', 'moment', 'monday', 'mr', 'obama', 'of', 'office', 'on', 'organized', 'oriented', 'part', 'planned', 'policing', 'promote', 'push', 'reaction', 'reporters', 'ronald', 'services', 'sitting', 'tensions', 'the', 'to', 'told', 'visit', 'we', 'white', 'without']\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.30993994 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.30993994 0.         0.30993994 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.30993994 0.20757039 0.20757039 0.\n",
      "  0.         0.30993994 0.         0.         0.         0.30993994\n",
      "  0.         0.30993994 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.14768804 0.35144723 0.\n",
      "  0.30993994 0.         0.         0.        ]\n",
      " [0.         0.21532526 0.         0.         0.21532526 0.21532526\n",
      "  0.21532526 0.         0.         0.         0.21532526 0.\n",
      "  0.         0.21532526 0.         0.         0.         0.\n",
      "  0.         0.         0.21532526 0.         0.21532526 0.21532526\n",
      "  0.         0.24416171 0.21532526 0.         0.21532526 0.21532526\n",
      "  0.21532526 0.         0.         0.14420584 0.14420584 0.14420584\n",
      "  0.         0.         0.         0.         0.21532526 0.\n",
      "  0.         0.         0.21532526 0.21532526 0.         0.\n",
      "  0.         0.         0.21532526 0.17372306 0.24416171 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.16834073 0.         0.16834073 0.28502563 0.         0.\n",
      "  0.         0.16834073 0.16834073 0.         0.         0.16834073\n",
      "  0.16834073 0.         0.16834073 0.16834073 0.16834073 0.16834073\n",
      "  0.16834073 0.         0.         0.         0.         0.\n",
      "  0.16834073 0.19088498 0.         0.16834073 0.         0.\n",
      "  0.         0.16834073 0.         0.         0.         0.23659702\n",
      "  0.16834073 0.         0.16834073 0.16834073 0.         0.\n",
      "  0.28502563 0.         0.         0.         0.16834073 0.16834073\n",
      "  0.16834073 0.16834073 0.         0.19141722 0.         0.16834073\n",
      "  0.         0.16834073 0.16834073 0.16834073]]\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF example\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "results = []\n",
    "news_content = \"\"\"\n",
    "Mr. Obama planned to promote the effort on Monday during a visit to Camden, N.J. The ban is\n",
    "part of Mr. Obama's push to ease tensions between law enforcement and minority \\communities\n",
    "in reaction to the crises in Baltimore; Ferguson, Mo. We are, without a doubt, sitting at a \n",
    "defining moment in American policing, Ronald L. Davis, the director of the Office of \n",
    "Community Oriented Policing Services at the Department of Justice, told reporters in a \n",
    "conference call organized by the White House\n",
    "\"\"\"\n",
    "\n",
    "sentences = nltk.sent_tokenize(news_content)\n",
    "vectorizer = TfidfVectorizer(\n",
    "    norm=\"l2\", min_df=0, use_idf=True, smooth_idf=False, sublinear_tf=True\n",
    ")\n",
    "\n",
    "sklearn_binary = vectorizer.fit_transform(sentences)\n",
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "print(sklearn_binary.toarray())\n",
    "\n",
    "for i in sklearn_binary.toarray():\n",
    "    results.append(i.sum() / float(len(i.nonzero()[0])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
