{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Chapter 5 - Text I: Working with Text and Sequences, and TensorBoard Visualization__\n",
    "\n",
    "1. [Import](#Import)\n",
    "1. [The importance of sequence data](#The-importance-of-sequence-data)\n",
    "1. [Introduction to recurrent neural networks](#Introduction-to-recurrent-neural-networks)\n",
    "    1. [MNIST images as sequences](#MNIST-images-as-sequences)\n",
    "        1. [The RNN step](#The-RNN-step)\n",
    "        1. [Sequential outputs](#Sequential-outputs)\n",
    "        1. [RNN classification](#RNN-classification)\n",
    "1. [Visualizing the model with TensorBoard](#Visualizing-the-model-with-TensorBoard)\n",
    "1. [TensorFlow built-in RNN functions](#TensorFlow-built-in-RNN-functions)\n",
    "1. [RNN for Text Sequences](#RNN-for-Text-Sequences)\n",
    "    1. [Text sequences](#text-sequences)\n",
    "    1. [Supervised word embeddings](#Supervised-word-embeddings)\n",
    "    1. [LSTM and using sequence length](#LSTM-and-using-sequence-length)\n",
    "    1. [Training embeddings and the LSTM classifier](#Training-embeddings-and-the-LSTM-classifier)\n",
    "    1. [Stacking multiple LSTMs](#Stacking-multiple-LSTMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Import'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T19:57:04.451735Z",
     "start_time": "2019-02-28T19:57:01.747693Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Standard libary and settings\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "import itertools\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "from IPython.core.display import display, HTML; display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "# Data extensions and settings\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold = np.inf, suppress = True)\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.options.display.float_format = '{:,.6f}'.format\n",
    "\n",
    "import tensorflow as tf\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Visualization extensions and settings\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Custom extensions and settings\n",
    "sys.path.append('/main') if '/main' not in sys.path else None\n",
    "import mlmachine as mlm\n",
    "import quickplot as qp\n",
    "\n",
    "# Magic functions\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'The-importance-of-sequence-data'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The importance of sequence data\n",
    "\n",
    "The previous chapter highlighted that exploiting the structure of data is the key to success - spatial structure of image pixels can be quite informative. Another important structure is sequential structure. This occurs in the context, including video, audio, genomics gene sequences, longitudinal medical records in healthcare and financial data in the stock market, etc.\n",
    "\n",
    "A particularly import type of sequential data with strong structure is natural language. Deep learning can exploit the inherent structure of text that appears between individual characters, words, sentences, paragraphs and even entire documents. Common pursuits include document classification, automated question answersing and humner-level conversational bots.\n",
    "\n",
    "This chapter will focus on the basic building blocks and tasks associated with sequence data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Introduction-to-recurrent-neural-networks'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to recurrent neural networks\n",
    "\n",
    "The idea behind RNN models is that each new element in the sequence contributes new information, which updates the current state of the model. The intuition if close to our commplace understanding of how we process sequential information in our day-to-day. Our memory is not cleared upon arriving at new information - rather, our memory is updated.\n",
    "\n",
    "A fundamental building block often used for modeling sequential patterns via machine learning is the Markov chain model. In a general sense, we can view our data sequences as \"chains\", with each node in the chain dependent in some way on the previous node, in effect carrying forward the history of the sequence.\n",
    "\n",
    "RNN models are also based on this notion of a chain. Types of RNNs vary in how they maintain and update information. RNNs apply some form of loop where an input $x_t$ (perhaps a word in a sequence) is observed at time $t$, and the network updates its \"state vector\" to $h_t$ from the previous vector $h_{t-1}$. Then the subsequent update will be dependent on $h_t$, which in effect retains the history of the sequence up to that point. This can be thought of as one long unrolled chain, where link in the chain involved the same kind of processing step based on the history understood up to that point in the chain.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'MNIST-images-as-sequences'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST images as sequences\n",
    "\n",
    "The intuition behind CNNs and pixel arrangement is more readily apparent than the potential application of RNNs to image identification tasks, but RNNs offer a different angle at understanding the structure of image data which can be an informative compliment to CNN techniques.\n",
    "\n",
    "A simplist view of a 28 x 28 MNIST image sample is to think of it as a sequence of rows (or columns). Each image can be viewed as a sequence of length 28, and each element in the sequence is a vector of 28 pixel values. With this construct in mind, the RNN can be thought of as a scanner, moving from the top of the image to the bottom (when looking at row sequences) or left to right (when looking at column sequences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T19:57:16.055173Z",
     "start_time": "2019-02-28T19:57:04.455329Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-59954fc053b2>:4: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /main/tmp/data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /main/tmp/data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting /main/tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /main/tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('/main/tmp/data/', one_hot = True)\n",
    "\n",
    "# Parameters\n",
    "element_size = 28\n",
    "time_steps = 28\n",
    "num_classes = 10\n",
    "batch_size = 128\n",
    "hidden_layer_size = 128\n",
    "\n",
    "# location to save TensoBoard model summaries\n",
    "logDir = '/main/logs/RNN_with_summaries'\n",
    "\n",
    "# create placeholders for inputs, labels\n",
    "_inputs = tf.placeholder(tf.float32\n",
    "                         ,shape = [None, time_steps, element_size]\n",
    "                         ,name = 'inputs')\n",
    "y = tf.placeholder(tf.float32\n",
    "                   ,shape = [None, num_classes]\n",
    "                   ,name = 'labels'\n",
    "                  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Remarks - element_size is the dimension of each vector in our sequence (# of pixels) and time_steps is the number of such elements of that size in a full sequence. hidden_layer_size is arbitraily set to 128 and controls the size of the hidden RNN state vector described above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'The-RNN-step'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The RNN step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T19:57:16.188827Z",
     "start_time": "2019-02-28T19:57:16.061784Z"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "\n",
    "# reshape data to get 28 sequences of 28 pixels\n",
    "batch_x = batch_x.reshape((batch_size, time_steps, element_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T19:57:16.491740Z",
     "start_time": "2019-02-28T19:57:16.198531Z"
    }
   },
   "outputs": [],
   "source": [
    "# Helper function for logging summary data for TensorBoard\n",
    "\n",
    "def variable_summaries(var):\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)\n",
    "        \n",
    "# Weights and bias for input and hiden layer\n",
    "with tf.name_scope('rnn_weights'):\n",
    "    with tf.name_scope('W_x'):\n",
    "        Wx = tf.Variable(tf.zeros([element_size, hidden_layer_size]))\n",
    "        variable_summaries(Wx)\n",
    "    with tf.name_scope('W_h'):\n",
    "        Wh = tf.Variable(tf.zeros([hidden_layer_size, hidden_layer_size]))\n",
    "        variable_summaries(Wh)\n",
    "    with tf.name_scope('Bias'):\n",
    "        b_rnn = tf.Variable(tf.zeros([hidden_layer_size]))\n",
    "        variable_summaries(b_rnn)\n",
    "    \n",
    "# Apply RNN step with tf.scan()\n",
    "def rnn_step(previous_hidden_state, x):\n",
    "    current_hidden_state = tf.tanh(\n",
    "        tf.matmul(previous_hidden_state, Wh) + \n",
    "        tf.matmul(x, Wx) + b_rnn\n",
    "    )\n",
    "    return current_hidden_state\n",
    "    \n",
    "    \n",
    "# Processing inputs to work with scan function\n",
    "# current input shape: (batch_size, time_steps, element_size)\n",
    "processed_input = tf.transpose(_inputs, perm = [1,0,2])\n",
    "# current input shape now - (time_steps, batch_size, element_size)\n",
    "\n",
    "initial_hidden = tf.zeros([batch_size, hidden_layer_size])\n",
    "\n",
    "# Getting all state vectors across time\n",
    "all_hidden_states = tf.scan(rnn_step\n",
    "                            ,processed_input\n",
    "                            ,initializer = initial_hidden\n",
    "                            ,name = 'states')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Remarks - the inputs are reshaped from [batch_size, time_steps, element_size] to [time_steps, batch_size, element_size]. The perm argument to tf.transpose() tells TensorFlow which axes to switch around such that the first axis in our input Tensor now represents the time axis. We then use the built-in tf.scan() function which repeatedly applies a function to a sequence of elements in order. tf.scan() is used to introduce loops into the computation graph, which allows us to avoid 'unrolling' the loops explicitly by adding more and more replications of the same operation. This functions enables the graph to have a dynamic nmber of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T19:57:16.626734Z",
     "start_time": "2019-02-28T19:57:16.502926Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'T', b'Te', b'Ten', b'Tens', b'Tenso', b'Tensor', b'Tensor ',\n",
       "       b'Tensor f', b'Tensor fl', b'Tensor flo', b'Tensor flow'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf.scan() demonstration\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "elems = np.array(['T','e','n','s','o','r',' ','f','l','o','w'])\n",
    "scan_sum = tf.scan(lambda a, x: a + x, elems)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(scan_sum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Remarks - tf.scan() is used to sequentially concatenate characters to a string, which mimics an arithmetic cumulative sum operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Sequential-outputs'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T19:57:16.781272Z",
     "start_time": "2019-02-28T19:57:16.629329Z"
    }
   },
   "outputs": [],
   "source": [
    "# Weights for outputs layers\n",
    "\n",
    "with tf.name_scope('linear_layer_weights') as scope:\n",
    "    with tf.name_scope('W_linear'):\n",
    "        W1 = tf.Variable(tf.truncated_normal([hidden_layer_size\n",
    "                                            ,num_classes]\n",
    "                                            ,mean = 0\n",
    "                                            ,stddev = 0.01))\n",
    "        variable_summaries(W1)\n",
    "    with tf.name_scope('Bias_linear'):\n",
    "        b1 = tf.Variable(tf.truncated_normal([num_classes]\n",
    "                                            ,mean = 0\n",
    "                                            ,stddev = 0.01))\n",
    "        variable_summaries(b1)\n",
    "\n",
    "# apply linear layer to state vector\n",
    "def get_linear_layer(hidden_state):\n",
    "    return tf.matmul(hidden_state, W1) + b1\n",
    "\n",
    "with tf.name_scope('linear_layer_weights') as scope:\n",
    "    # iterate across time, apply linear layer to all RNN outputs\n",
    "    all_outputs = tf.map_fn(get_linear_layer, all_hidden_states)\n",
    "    \n",
    "    # get last output\n",
    "    output = all_outputs[-1]\n",
    "    tf.summary.histogram('outputs', output)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Remarks - The RNN input is sequential and so is the output. In this example, the last state vector is passed through a fully connected linear to extract an output vector (which then gets passed through a softmax activation funtion to generate predictions). This operates on the assumption that the last state vector has accumulated information representing the entire sequence.\n",
    "To implement this, we define the linear layer's weights and bias term variables, and create a factory function for this layer. Then this layer is applied to all outputs with tf.map_fn(), which applies a function to sequences in an element-wise manner. Lastly, we extract the last output for each instance in the batch (with negative indexing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'RNN-classification'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN classification\n",
    "\n",
    "To train the classifier, we need to define operations for loss function computation, optimization and predictions, as well as add more summaries for TensorBoard, and merge all of these summaries into one operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T19:57:17.178312Z",
     "start_time": "2019-02-28T19:57:16.783961Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-784fc619542f>:5: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "\n",
    "with tf.name_scope('cross_entropy'):\n",
    "    cross_entropy = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits = output, labels = y)\n",
    "    )\n",
    "    tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "    \n",
    "with tf.name_scope('train'):\n",
    "    train_step = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cross_entropy)\n",
    "    \n",
    "with tf.name_scope('accuracy'):\n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(output,1))\n",
    "    accuracy = (tf.reduce_mean(tf.cast(correct_prediction, tf.float32)))*100\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "# merge all the summaries\n",
    "merged = tf.summary.merge_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T20:06:48.162776Z",
     "start_time": "2019-02-28T19:57:17.180556Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, Minibatch Loss = 2.302064, Training Accuracy = 7.03125\n",
      "Iter 1000, Minibatch Loss = 1.020973, Training Accuracy = 63.28125\n",
      "Iter 2000, Minibatch Loss = 0.599687, Training Accuracy = 82.81250\n",
      "Iter 3000, Minibatch Loss = 0.255602, Training Accuracy = 90.62500\n",
      "Iter 4000, Minibatch Loss = 0.144663, Training Accuracy = 95.31250\n",
      "Iter 5000, Minibatch Loss = 0.157701, Training Accuracy = 96.09375\n",
      "Iter 6000, Minibatch Loss = 0.162208, Training Accuracy = 95.31250\n",
      "Iter 7000, Minibatch Loss = 0.097686, Training Accuracy = 98.43750\n",
      "Iter 8000, Minibatch Loss = 0.021354, Training Accuracy = 100.00000\n",
      "Iter 9000, Minibatch Loss = 0.051803, Training Accuracy = 98.43750\n",
      "Test accuracy: 96.875\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "\n",
    "test_data = mnist.test.images[:batch_size].reshape((-1, time_steps, element_size))\n",
    "test_label = mnist.test.labels[:batch_size]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # write summaries to log directory for TensorBoard\n",
    "    train_writer = tf.summary.FileWriter(logDir + '/train'\n",
    "                                        ,graph = tf.get_default_graph())\n",
    "    test_writer = tf.summary.FileWriter(logDir + '/test'\n",
    "                                        ,graph = tf.get_default_graph())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(10000):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        \n",
    "        # reshape data to get 28 sequences of 28 pixels\n",
    "        batch_x = batch_x.reshape([batch_size, time_steps, element_size])\n",
    "        summary, _ = sess.run([merged, train_step]\n",
    "                             ,feed_dict = {_inputs : batch_x, y : batch_y})\n",
    "        # add to summaries\n",
    "        train_writer.add_summary(summary, 1)\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            acc, loss, = sess.run([accuracy, cross_entropy]\n",
    "                                 ,feed_dict = {_inputs : batch_x, y : batch_y})\n",
    "            print('Iter ' + str(i) + ', Minibatch Loss = ' + '{:.6f}'.format(loss)\\\n",
    "                 + ', Training Accuracy = ' + '{:.5f}'.format(acc))\n",
    "        if i % 10:\n",
    "            # calculate accuracy for 128 MNIST test images and add to summaries\n",
    "            summary, acc = sess.run([merged, accuracy]\n",
    "                                   ,feed_dict = {_inputs : test_data, y : test_label})\n",
    "            test_writer.add_summary(summary, i)\n",
    "    test_acc = sess.run(accuracy, feed_dict = {_inputs : test_data, y : test_label})\n",
    "    print('Test accuracy: {:.5}'.format(test_acc))\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Visualizing-the-model-with-TensorBoard'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the model with TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-28T19:46:05.288681Z",
     "start_time": "2019-02-28T19:46:05.243557Z"
    }
   },
   "source": [
    "In a terminal window, run:\n",
    "\n",
    "\n",
    "tensorboard --logdir <log directory here>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Remarks - For each run, Ensure that logDir references a new set of log files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'TensorFlow-built-in-RNN-functions'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow built-in RNN functions\n",
    "\n",
    "The following code segment contains a shorter version of the example above, this time utilizing select TensorFlow built-in functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "mnist = input_data.read_data_sets('/tmp/data/', one_hot = True)\n",
    "element_size = 28; time_steps = 28; num_classes = 10\n",
    "batch_size = 128; hidden_layer_size = 128\n",
    "\n",
    "_inputs = tf.placeholder(tf.float32, shape = [None, time_steps\n",
    "                                             ,element_size, name = 'inputs'])\n",
    "y = tf.placeholder(tf.float32, shape = [None, num_classes], name = 'inputs')\n",
    "\n",
    "# TensorFlow built-in functions\n",
    "rnn_cell = tf.contrib.rnn.BasicRNNCell(hidden_layer_size)\n",
    "outputs, _ = tf.nn.dynamic_rn(run_cell, _inputs, dtype = tf.float32)\n",
    "\n",
    "W1 = tf.Variable(tf.truncate_normal([hidden_layer_size, num_classes]\n",
    "                                   ,mean = 0, stddev = 0.01))\n",
    "b1 = tf.Variable(tf.truncate_normal([num_classes],mean = 0, stddev = 0.01))\n",
    "\n",
    "def get_linear_layer(vector)\n",
    "    return tf.matmul(vector, W1) + b1\n",
    "\n",
    "last_rnn_output = outputs[:, -1, :]\n",
    "final_output = get_linear_layer(last_rnn_output)\n",
    "\n",
    "softmax = tf.nn.softmax_cross_entropy_with_logits(logits = final_output, labels = y)\n",
    "cross_entropy = tf.reduce_mean(softma)\n",
    "train_step = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argma(final_output, 1))\n",
    "accuracy = (tf.reduce_mean(tf.cast(correct_prediction, tf.float32))) * 100\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "test_data = mnist.test.images[:batch_size].reshape((-1, time_steps, element_size))\n",
    "test_label = mnist.test.labels[:batch_size]\n",
    "\n",
    "for i in range(3001):\n",
    "    batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "    batch_x = batch_x.reshape((batch_size, time_steps, element_size))\n",
    "    sess.run(train_step, feed_dict = {_inputs : batch_x, y : batch_y})\n",
    "    if i % 1000 == 0:\n",
    "        acc = sess.run(accuracy, feed_dict = {_inputs : batch_x, y : batch_y})\n",
    "        loss = sess.run(cross_entropy, feed_dict = {_inputs : batch_x, y : batch_y})\n",
    "        print('Iter ' + str(i) + ', Minibatch Loss = ' + '{:.6f}'.format(loss)\\\n",
    "                 + ', Training Accuracy = ' + '{:.5f}'.format(acc))\n",
    "\n",
    "print('Testing accuracy: {}'.format(sess.run(accuracy, feed_dict = {_inputs : batch_x, y : batch_y})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Remarks \n",
    "- tf.contrib.rnn.BasicRNNCell and tf.nn.dynamic_rnn are abastractions that represent the basic operations that each recurrent cell carries out, as well as the associated state. They replace the rnn_step() function and the associated variables required.\n",
    "- Once the rnn_cell variable is created, it gets fed into tf.nn.dynamic_rnn(). This function replaces tf.scan() in the implementation above and creates and RNN specified by rnn_cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'RNN-for-Text-Sequences'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN for Text Sequences\n",
    "\n",
    "The MNIST RNN example was useful for instilling the intuition behind RNNs, but a more prominent application of RNNs is analyzing text data.\n",
    "\n",
    "We will eventually model movie review data, but we will start with some example data and discuss some key properties of text datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'text-sequences'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text sequences\n",
    "\n",
    "Text sequences can be letters in a word, words in a sentence, sentences in a paragraph, or even documents in a documents corpus. \n",
    "\n",
    "Take this sentence as an example: \"Our company provides smart agriculture solutions for farms, with advanced AI, deep-learning.\" We'll pretend this is a sentence from an online news blog, and we want to process it in our machine learning system.\n",
    "\n",
    "Each word in the sentence can be represented with an ID - an integer, commonly referred to as a token ID in NLP. So the word \"agriculture\" could be mapped to the integer 3452, the word \"farm\" to 12, and \"deep-learning\" to 0. This representation of the text data is quite different that the pixel vectors we have used up to this point.\n",
    "\n",
    "We create a simulated data consisting of two classes of very short \"sentences\". One is composed of odd digits and the other is composed of even digits. The digits are represented with the English word. We want to generate sentences built of words  representing even and odd numbers, and our goal is to learn to classify each sentence as either odd or even as a supervised text-classification task.\n",
    "\n",
    "What follows is a contrived example for illustrative purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "batch_size = 128; embedding_dimension = 64; num_classes = 2\n",
    "hidden_layer_size = 32; time_steps = 6; element_size = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create sentences by randomly sampling digits and mapping them to the corresponding English word, i.e. 1 is mapped to \"One\". The sentences will have varying lengths between 3 and 6 words. That being said, in order for all input sentence to be put into one tensor, we need the sentences to be the same size. To do this, we add padding to all sentences to the extent needed. This process is called zero padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "digit_to_word_map = {1 : 'One', 2 : 'Two', 3 : 'Three'\n",
    "                    ,4 : 'Four', 5 : 'Five', 6 : 'Six'\n",
    "                    ,7 : 'Seven', 8 : 'Eight', 9 : 'Nine'}\n",
    "digit_to_word_map[0] = 'PAD'\n",
    "\n",
    "even_sentences = []\n",
    "odd_sentences = []\n",
    "seqlens = []\n",
    "for i in range(10000):\n",
    "    rand_seq_len = np.random.choice(range(3, 7))\n",
    "    seqlens.append(rand_seq_len)\n",
    "    rand_odd_ints = np.random.choice(range(1, 10, 2), rand_seq_len)\n",
    "    rand_even_ints = np.random.choice(range(2, 10, 2), rand_seq_len)\n",
    "        \n",
    "    # padding\n",
    "    if rand_seq_len < 6:\n",
    "        rand_odd_ints = np.append(rand_odd_ints, [0] * (6 - rand_seq_len))\n",
    "        rand_even_ints = np.append(rand_even_ints, [0] * (6 - rand_seq_len))\n",
    "    even_sentences.append(' '.join([digit_to_word_map[r] for r in rand_even_ints]))\n",
    "    odd_sentences.append(' '.join([digit_to_word_map[r] for r in rand_odd_ints]))\n",
    "\n",
    "# concat\n",
    "data = even_sentences + odd_sentences\n",
    "seqlens *= 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# even sentences\n",
    "\n",
    "even_sentences[0:6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# odd sentences\n",
    "\n",
    "odd_sentences[0:6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original sequence lengths\n",
    "\n",
    "seqlens[0:6]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Remarks - We keep the original sentence lengths because adding zero-padding solves one problem but creates another - if we pass padded sentences through the RNN, it will process our uselss PAD symbols. We solve this problem by storing the original lengths in seqlens and then tell tf.nn.dynamic_rnn() where each sentence truly ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map words to arbitrarily chosen indices with a dictionary\n",
    "# also create the inverse\n",
    "\n",
    "word2index_map = {}\n",
    "index = 0\n",
    "for sent in data:\n",
    "    for word in sent.lower().split():\n",
    "        if word not in word2index_map:\n",
    "            word2index_map[word] = index\n",
    "            index += 1\n",
    "\n",
    "# inverse map\n",
    "index2word_map = {index : word for word, index in word2index_map.items()}\n",
    "vocabulary_size = len(index2word_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create array of lebs in one-hot format\n",
    "\n",
    "labels = [1] * 10000 + [0] * 10000\n",
    "for i in range(len(labels)):\n",
    "    label = labels[i]\n",
    "    one_hot_encoding = [0] * 2\n",
    "    one_hot_encoding[label] = 1\n",
    "    labels[i] = one_hot_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split\n",
    "\n",
    "data_indices = list(range(len(data)))\n",
    "np.random.shuffle(data_indices)\n",
    "data = np.array(data)[data_indices]\n",
    "\n",
    "labels = np.array(labels)[data_indices]\n",
    "seqlens = np.array(seqlens)[data_indices]\n",
    "train_x = data[:10000]\n",
    "train_y = labels[:10000]\n",
    "train_seqlens = seqlens[:10000]\n",
    "\n",
    "test_x = data[10000:]\n",
    "test_y = labels[10000:]\n",
    "test_seqlens = seqlens[10000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create batches of sentences comprised of integer IDs\n",
    "\n",
    "def get_sentence_batch(batch_size, data_x, data_y, data_seqlens):\n",
    "    instance_indices = list(range(len(data_)))\n",
    "    np.random.shuffle(instance_indices)\n",
    "    batch = instance_indices[:batch_size]\n",
    "    x = [[word2index_map[word] for word in data_x[i].lower().split()] for i in batch]\n",
    "    y = [data_y[i] for i in batch]\n",
    "    seqlens = [data_seqlens[i] for i in batch]\n",
    "    return x, y, seqlens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create placeholders for data\n",
    "\n",
    "_inputs = tf.placeholder(tf.int32, shape = [batch_size, time_steps])\n",
    "_labels = tf.placeholder(tf.float32, shape = [batch_size, num_classes])\n",
    "_seqlens = tf.placeholder(tf.int32, shape = [batch_size])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Supervised-word-embeddings'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised word embeddings\n",
    "\n",
    "The text data is encoded as lists of word IDs. This atomic-style of representation is not scalable for training deep learning models with large vocabularies. When the vocabulary is large, we could end up with millions of word IDs, each encoded in one-hot fasion, which leads to great data sparsity and computational issues.\n",
    "\n",
    "Word embedding is potential solution for this problem. Embeddings are mappings from high-dimensional one-hot vectors that encode word to lower-dimensional dense vectors. For example, if the vocabulary is 100,000 words, each word in one-hot representation would be of the same size. The high-dimensional one-hot vectors are embedded into a continuous vector space with much lower dimensionality. A popular implementation of this word2vec, will be explored in chapter 6.\n",
    "\n",
    "Word embeddings can be thought of as has tables or lookup tables, mapping words to their dense vector values. These vectors are optimized as part of the training process. Previously, each word was associated with an integer index, and sentences are then represented as sequences of these indices. Now, to obtain a word's vector, we use the built-in tf.nn.embedding_lookup() function, which retrieves the vectors for each word in a given sequence of word indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "with tf.name_scope('embeddings'):\n",
    "    embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size, embedding_dimension]\n",
    "                         , -1.0, 1.0), name = 'embedding'\n",
    "    )\n",
    "    embed = tf.nn.embedding_lookup(embeddings, _inputs)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'LSTM-and-using-sequence-length'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM and using sequence length\n",
    "\n",
    "The basic RNN implementation above is generally not used in practice. More advanced models differ mainly by how they update their hidden state and propagate information through time. A popular method is the long short-term memory (LSTM) network. It differs from our basic RNN by having some special memory mechanisms that enable the recurrent cells to better store information for long period of time. This captures long-term dependencies better than basic RNNs.\n",
    "\n",
    "The efficiencies in LSTMs arise from additional parameters added to each recurrent cell which, generally speaking, enable the RNN to overcome optimizations issues. These parameters filter the information that is 'worth remembering' and passing forward from the information that is worth 'forgetting'.\n",
    "\n",
    "To implement this, we create an LSTM cell with tf.contrib.rnn.BasicLSTMCell() and feed it into tf.nn.dynamic_rnn(), just as we did in the implementation above. We also give dynamic_rnn() the length of each sequence through the variable seqlens. This allows TensorFlow to stop all RNN steps once the last 'real' element is reached in each sample, which effectively ignores the PAD elements. It also returns all output vectors over time. These appear in the outputs tensor, and are zero-padded beyond the end of the true sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "with tf.variable_scope('lstm'):\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(hidden_layer_size, forget_bias = 1.0)\n",
    "    outputs, states = tf.n..dynamic_rnn(lstm_cell, embed, sequence_length = _seqlens, dtype = tf.float32)\n",
    "    \n",
    "weights = {'linear_layer' : tf.Variable(tf.truncated_normal([hidden_layer_size, num_classes], mean = 0, stddev = 0.01))}\n",
    "biases = {'linear_layer' : tf.Variable(tf.truncated_normal([num_classes], mean = 0, stddev = 0.01))}\n",
    "\n",
    "# extract the last output and use in a linear layer\n",
    "final_output = tf.matmul(states[1], weights['linear_layer']) + biases['linear_layer']\n",
    "softmax = tf.nn.softmax_cross_entropy_with_logits(logits = final_output, labels = _labels)\n",
    "cross_entropy = tf.reduce_mean(softmax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Remarks - dynamic_rnn() returns a tensor called states, and from this we can retriee the last valid output vector and pass it through a linear layer (and the softmax function), using it as our final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Training-embeddings-and-the-LSTM-classifier'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training embeddings and the LSTM classifier\n",
    "\n",
    "Combine all of the pieces to create an end-to-end training of both word vectors and a classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "train_step = tf.train.RMSPorpOptimizer(0.001, 0.9).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(_labels, 1), tf.argmax(final_output, 1))\n",
    "accuracy = (tf.reduce_mean(tf.cast(correct_prediction, tf.float32))) * 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(1000):\n",
    "        x_batch, y_batch, seqlen_batch = get_sentence_batch(batch_size, train_x, train_y, train_seqlens)\n",
    "        sess.run(train_step, feed_dict = {_inputs : x_batch, _labels : y_batch, _seqlens : seqlen_batch})\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            acc = sess.run(accuracy, feed_dict = {_inputs : x_batch, _labels : y_batch, _seqlens : seqlen_batch})\n",
    "            print('Accuracy at {:d}: {:.5f}'.format(step, acc))\n",
    "            \n",
    "    for test_batch in range(5):\n",
    "        x_test, y_test, seqlen_test = get_sentence_batch(batch_size, test_x, test_y, test_seqlens)\n",
    "        batch_pred, batch_acc = sess.run([tf.argmax(final_output, 1), accuracy], feed_dict = {_inputs : x_test\n",
    "                                                                                             ,_labels : y_test\n",
    "                                                                                             ,_seqlens : seqlen_test})\n",
    "        print('Test batch accuracy {:d}: {:.5f}'.format(test_batch, batch_acc))\n",
    "    \n",
    "    output_example = sess.run([outputs], feed_dict = {_inputs : x_test, _labels : y_test, _seqlens : seqlen_test})\n",
    "    states_example = sess.run([states[1]], feed_dict = {_inputs : x_test, _labels : y_test, _seqlens : seqlen_test})\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "seqlen[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "output_example[0][1].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "output_example[0][1][:6, 0:3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Remarks - The original sequence length was X, so it makes sense that the last two time steps have zero vectors due to padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "states_example[0][1][0:3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Remarks - The states vector returned by dynamic_rnn() stores the last releant output ector. Note that the values match the last relevant output vector before zero-padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Stacking-multiple-LSTMs'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking multiple LSTMs\n",
    "\n",
    "The LSTM example above utilizes only a one-layer LSTM network. Adding more layers is straightforward when using the MultiRNNCell() wrapper that combines multiple RNN cells.\n",
    "\n",
    "We first define an LSTM cell as before, then feed it into the wrapper. The following network has two layers of LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "num_LSTM_layers = 2\n",
    "with tf.variable_scope('lstm'):\n",
    "    lstm_cell_list = [tf.contrib.rnn.BasicLSTMCell(hidden_layer_size, forget_bias = 1.0)\n",
    "                     for ii in range(num_LSTM_layers)]\n",
    "    cell = tf.contrib.rnn.MultiRNNCell(cells = lstm_cell_list, state_is_tuple = True)\n",
    "    outputs, states = tf.nn.dynamic_rnn(cell, embed, sequence_length = _seqlens, dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This introduces some changes to the shape, which changes how we access the final state. To get the final state of the second layer, the indexing needs to be adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the final state and use in a linear layer\n",
    "\n",
    "final_output = tf.matmul(states[num_LSTM_layers - 1][1], weights['linear_layer']) + biases['linear_layer']\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
