{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Kaggle competition - house prices__\n",
    "\n",
    "1. [Kaggle competition - house prices](#Kaggle-competition-house-prices)\n",
    "1. [Import](#Import)\n",
    "    1. [Tools](#Tools)\n",
    "    1. [Data](#Data)    \n",
    "1. [Initial EDA](#Initial-EDA)\n",
    "    1. [Categorical feature EDA](#Categorical-feature-EDA)\n",
    "        1. [Univariate & feature vs. target](#Univariate-&-feature-vs.-target)\n",
    "    1. [Continuous feature EDA](#Continuous-feature-EDA)\n",
    "        1. [Univariate & feature vs. target](#Univariate-&-feature-vs.-target2)\n",
    "        1. [Correlation](#Correlation)\n",
    "            1. [Correlation (all samples)](#Correlation-all-samples)\n",
    "            1. [Correlation (top vs. target)](#Correlation-top-vs-target)\n",
    "        1. [Pair plot](#Pair-plot)\n",
    "    1. [Target variable evaluation](#Target-variable-evaluation)    \n",
    "1. [Data cleaning](#Data-cleaning)\n",
    "    1. [Outliers (preliminary)](#Outliers-preliminary)\n",
    "        1. [Training](#Training5)\n",
    "        1. [Validation](#Validation5)\n",
    "    1. [Missing data](#Missing-data)\n",
    "        1. [Evaluate](#Evaluate1)\n",
    "        1. [Training](#Training1)\n",
    "        1. [Validation](#Validation1)\n",
    "    1. [Engineering](#Engineering)\n",
    "        1. [Evaluate](#Evaluate3)\n",
    "        1. [Training](#Training3)\n",
    "        1. [Validation](#Validation3)\n",
    "    1. [Encoding](#Encoding)\n",
    "        1. [Evaluate](#Evaluate2)\n",
    "        1. [Training](#Training2)\n",
    "        1. [Validation](#Validation2)\n",
    "    1. [Transformation](#Transformation)\n",
    "        1. [Evaluate](#Evaluate4)\n",
    "        1. [Training](#Training4)\n",
    "        1. [Validation](#Validation4)\n",
    "    1. [Outliers (final)](#Outliers-final)\n",
    "        1. [Training](#Training6)\n",
    "1. [Data evaluation](#Data-evaluation)\n",
    "    1. [Feature importance](#Feature-importance)\n",
    "    1. [Rationality](#Rationality)\n",
    "    1. [Value override](#Value-override)\n",
    "    1. [Continuous feature EDA](#Continuous-feature-EDA3)\n",
    "        1. [Univariate & feature vs. target](#Univariate-&-feature-vs.-target3)\n",
    "        1. [Correlation](#Correlation3)\n",
    "            1. [Correlation (top vs. target)](#Correlation-top-vs-target3)\n",
    "1. [Modeling](#Modeling)\n",
    "    1. [Prepare training data](#Prepare-training-data)\n",
    "    1. [Prepare validation data](#Prepare-validation-data)\n",
    "    1. [GridSearch](#GridSearch)\n",
    "        1. [Evaluation](#Evaluation)\n",
    "        1. [Model explanability](#Model-explanability)\n",
    "            1. [Permutation importance](#Permutation-importance)\n",
    "            1. [Partial plots](#Partial-plots)\n",
    "            1. [SHAP values](#SHAP-values)\n",
    "    1. [Stacking](#Stacking)\n",
    "        1. [Primary models](#Primary-models)\n",
    "        1. [Meta model](#Meta-model)        \n",
    "1. [Submission](#Submission)\n",
    "    1. [Stack](#Stack)\n",
    "    1. [Standard](#Standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle competition - house prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Kaggle-competition-house-prices'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Import'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Tools'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:46:59.753360Z",
     "start_time": "2019-07-13T18:46:58.610748Z"
    }
   },
   "outputs": [],
   "source": [
    "# standard libary and settings\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "import itertools\n",
    "import csv\n",
    "import ast\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "global ITERATION\n",
    "import time\n",
    "from functools import reduce\n",
    "\n",
    "rundate = time.strftime(\"%Y%m%d\")\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "# data extensions and settings\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(threshold=np.inf, suppress=True)\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.options.display.float_format = \"{:,.6f}\".format\n",
    "\n",
    "# modeling extensions\n",
    "import sklearn.base as base\n",
    "import sklearn.cluster as cluster\n",
    "import sklearn.datasets as datasets\n",
    "import sklearn.decomposition as decomposition\n",
    "import sklearn.discriminant_analysis as discriminant_analysis\n",
    "import sklearn.ensemble as ensemble\n",
    "import sklearn.feature_extraction as feature_extraction\n",
    "import sklearn.feature_selection as feature_selection\n",
    "import sklearn.gaussian_process as gaussian_process\n",
    "import sklearn.linear_model as linear_model\n",
    "import sklearn.kernel_ridge as kernel_ridge\n",
    "import sklearn.metrics as metrics\n",
    "import sklearn.model_selection as model_selection\n",
    "import sklearn.naive_bayes as naive_bayes\n",
    "import sklearn.neighbors as neighbors\n",
    "import sklearn.pipeline as pipeline\n",
    "import sklearn.preprocessing as preprocessing\n",
    "import sklearn.svm as svm\n",
    "import sklearn.tree as tree\n",
    "import sklearn.utils as utils\n",
    "\n",
    "import eif as iso\n",
    "\n",
    "from scipy import stats, special\n",
    "import xgboost\n",
    "import lightgbm\n",
    "import catboost\n",
    "\n",
    "from hyperopt import hp, tpe, Trials, fmin, STATUS_OK\n",
    "from hyperopt.pyll.stochastic import sample\n",
    "\n",
    "# visualization extensions and settings\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# custom extensions and settings\n",
    "sys.path.append(\"/home/mlmachine\") if \"/home/mlmachine\" not in sys.path else None\n",
    "sys.path.append(\"/home/prettierplot\") if \"/home/prettierplot\" not in sys.path else None\n",
    "\n",
    "import mlmachine as mlm\n",
    "from prettierplot.plotter import PrettierPlot\n",
    "import prettierplot.style as style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:46:59.826179Z",
     "start_time": "2019-07-13T18:46:59.755565Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load data and print dimensions\n",
    "dfTrain = pd.read_csv(\"/home/data-science-portfolio/data/kaggleHousingPrices/train.csv\")\n",
    "dfValid = pd.read_csv(\"/home/data-science-portfolio/data/kaggleHousingPrices/test.csv\")\n",
    "\n",
    "print(\"Training data dimensions: {}\".format(dfTrain.shape))\n",
    "print(\"Validation data dimensions: {}\".format(dfValid.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:46:59.908374Z",
     "start_time": "2019-07-13T18:46:59.829102Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# display info and first 5 rows\n",
    "dfTrain.info()\n",
    "display(dfTrain[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:46:59.919969Z",
     "start_time": "2019-07-13T18:46:59.911750Z"
    }
   },
   "outputs": [],
   "source": [
    "# review counts of different column types\n",
    "dfTrain.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:46:59.934000Z",
     "start_time": "2019-07-13T18:46:59.922322Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load training data into mlmachine\n",
    "train = mlm.Machine(\n",
    "    data=dfTrain,\n",
    "    target=[\"SalePrice\"],\n",
    "    removeFeatures=[\"Id\"],\n",
    "    overrideCat=[\n",
    "        \"MSSubClass\",\n",
    "        \"OverallQual\",\n",
    "        \"OverallCond\",\n",
    "        \"YearBuilt\",\n",
    "        \"YearRemodAdd\",\n",
    "        \"MoSold\",\n",
    "        \"YrSold\",\n",
    "    ],\n",
    "    targetType=\"continuous\",\n",
    ")\n",
    "print(train.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:46:59.952268Z",
     "start_time": "2019-07-13T18:46:59.939424Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load training data into mlmachine\n",
    "valid = mlm.Machine(\n",
    "    data=dfValid,\n",
    "    removeFeatures=[\"Id\"],\n",
    "    overrideCat=[\n",
    "        \"MSSubClass\",\n",
    "        \"OverallQual\",\n",
    "        \"OverallCond\",\n",
    "        \"YearBuilt\",\n",
    "        \"YearRemodAdd\",\n",
    "        \"MoSold\",\n",
    "        \"YrSold\",\n",
    "    ],\n",
    ")\n",
    "print(valid.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Initial-EDA'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical feature EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Categorical-feature-EDA'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate & feature vs. target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Univariate-&-feature-vs.-target'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:47:39.621127Z",
     "start_time": "2019-07-13T18:46:59.954222Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# categorical features\n",
    "train.edaNumTargetCatFeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous feature EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Continuous-feature-EDA'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate & feature vs. target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Univariate-&-feature-vs.-target2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:48:10.510097Z",
     "start_time": "2019-07-13T18:47:39.628054Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# continuous features\n",
    "train.edaNumTargetNumFeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Correlation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation (all samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Correlation-all-samples'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:48:12.467559Z",
     "start_time": "2019-07-13T18:48:10.520620Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# correlation heat map\n",
    "p = PrettierPlot(chartProp=25)\n",
    "ax = p.makeCanvas()\n",
    "p.prettyCorrHeatmap(df=train.data, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation (top vs. target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Correlation-top-vs-target'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:48:12.964955Z",
     "start_time": "2019-07-13T18:48:12.470007Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# correlation heat map with most highly correlated features relative to the target\n",
    "p = PrettierPlot()\n",
    "ax = p.makeCanvas()\n",
    "p.prettyCorrHeatmapTarget(df=train.data, target=train.target, thresh=0.6, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Remarks - There are three pairs of highly correlated features:\n",
    "    - 'GarageArea' and 'GarageCars'\n",
    "    - 'TotRmsAbvGrd' and 'GrLivArea'\n",
    "    - '1stFlrSF' and 'TotalBsmtSF\n",
    "This makes sense, given what each feature represents and how each pair items relate to each other. We likely only need one feature from each pair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pair plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Pair-plot'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:49:31.390463Z",
     "start_time": "2019-07-13T18:48:12.967433Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pair plot\n",
    "p = PrettierPlot(chartProp=10)\n",
    "p.prettyPairPlot(\n",
    "    df=train.data,\n",
    "    cols=[\n",
    "        \"LotFrontage\",\n",
    "        \"LotArea\",\n",
    "        \"MasVnrArea\",\n",
    "        \"BsmtFinSF1\",\n",
    "        \"BsmtFinSF2\",\n",
    "        \"BsmtUnfSF\",\n",
    "        \"TotalBsmtSF\",\n",
    "        \"1stFlrSF\",\n",
    "        \"2ndFlrSF\",\n",
    "        \"GrLivArea\",\n",
    "        \"TotRmsAbvGrd\",\n",
    "        \"GarageYrBlt\",\n",
    "        \"GarageArea\",\n",
    "        \"WoodDeckSF\",\n",
    "        \"OpenPorchSF\",\n",
    "    ],\n",
    "    diag_kind=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target variable evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Target-variable-evaluation'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:49:32.162285Z",
     "start_time": "2019-07-13T18:49:31.393096Z"
    }
   },
   "outputs": [],
   "source": [
    "# evaluate distribution of target variable\n",
    "train.edaTransformInitial(data=train.target, name=train.target.name)\n",
    "train.edaTransformLog1(data=train.target, name=train.target.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:49:32.168827Z",
     "start_time": "2019-07-13T18:49:32.164346Z"
    }
   },
   "outputs": [],
   "source": [
    "# log + 1 transform target\n",
    "train.target = np.log1p(train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Data-cleaning'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers (preliminary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Outliers-preliminary'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Training5'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:49:32.191697Z",
     "start_time": "2019-07-13T18:49:32.170908Z"
    }
   },
   "outputs": [],
   "source": [
    "# identify columns that have zero missing values\n",
    "nonNull = train.data.columns[train.data.isnull().sum() == 0].values.tolist()\n",
    "\n",
    "# identify intersection between non-null columns and continuous columns\n",
    "nonNullNumCol = list(set(nonNull).intersection(train.featureByDtype_[\"continuous\"]))\n",
    "print(nonNull)\n",
    "print(nonNullNumCol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:49:32.276206Z",
     "start_time": "2019-07-13T18:49:32.193887Z"
    }
   },
   "outputs": [],
   "source": [
    "# identify outliers using IQR\n",
    "trainPipe = pipeline.Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"outlier\",\n",
    "            train.OutlierIQR(\n",
    "                outlierCount=5, iqrStep=1.5, features=nonNullNumCol, dropOutliers=False\n",
    "            ),\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "train.data = trainPipe.transform(train.data)\n",
    "\n",
    "# capture outliers\n",
    "iqrOutliers = np.array(sorted(trainPipe.named_steps[\"outlier\"].outliers_))\n",
    "print(iqrOutliers)\n",
    "\n",
    "# remove outliers\n",
    "# train.target = np.delete(train.target, trainPipe.named_steps['outlier'].outliers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:49:32.721188Z",
     "start_time": "2019-07-13T18:49:32.278572Z"
    }
   },
   "outputs": [],
   "source": [
    "# identify outliers using Isolation Forest\n",
    "clf = ensemble.IsolationForest(\n",
    "    behaviour=\"new\", max_samples=train.data.shape[0], random_state=0, contamination=0.02\n",
    ")\n",
    "clf.fit(train.data[nonNullNumCol])\n",
    "preds = clf.predict(train.data[nonNullNumCol])\n",
    "# np.unique(preds, return_counts = True)\n",
    "\n",
    "# evaluate index values\n",
    "mask = np.isin(preds, -1)\n",
    "ifOutliers = np.where(mask)\n",
    "print(ifOutliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:49:39.109866Z",
     "start_time": "2019-07-13T18:49:32.724918Z"
    }
   },
   "outputs": [],
   "source": [
    "# identify outliers using Extended Isolation Forest\n",
    "if_eif = iso.iForest(\n",
    "    train.data[nonNullNumCol].values, ntrees=100, sample_size=256, ExtensionLevel=1\n",
    ")\n",
    "\n",
    "# calculate anomaly scores\n",
    "anomalies_ratio = 0.009\n",
    "anomaly_scores = if_eif.compute_paths(X_in=train.data[nonNullNumCol].values)\n",
    "anomaly_scores_sorted = np.argsort(anomaly_scores)\n",
    "eifOutliers = anomaly_scores_sorted[\n",
    "    -int(np.ceil(anomalies_ratio * train.data.shape[0])) :\n",
    "]\n",
    "print(sorted(eifOutliers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:49:39.116845Z",
     "start_time": "2019-07-13T18:49:39.112199Z"
    }
   },
   "outputs": [],
   "source": [
    "# identify outliers that are identified in multiple algorithms\n",
    "# reduce(np.intersect1d, (iqrOutliers, ifOutliers, eifOutliers))\n",
    "outliers = reduce(np.intersect1d, (ifOutliers, eifOutliers))\n",
    "print(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:49:39.135167Z",
     "start_time": "2019-07-13T18:49:39.119520Z"
    }
   },
   "outputs": [],
   "source": [
    "# capture index values of known outliers\n",
    "knownOutliers = (\n",
    "    train.data[train.data[\"LotArea\"] > 60000].index.values.tolist()\n",
    "    + train.data[train.data[\"LotFrontage\"] > 300].index.values.tolist()\n",
    "    + train.data[train.data[\"GrLivArea\"] > 4000].index.values.tolist()\n",
    ")\n",
    "knownOutliers = sorted(set(knownOutliers))\n",
    "print(knownOutliers)\n",
    "\n",
    "# train.data = train.data.drop(train.data.index[outliers])\n",
    "# train.target = np.delete(train.target, outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:49:39.144919Z",
     "start_time": "2019-07-13T18:49:39.137705Z"
    }
   },
   "outputs": [],
   "source": [
    "# index of known outliers and outliers identified with the known outliers removed\n",
    "outliers = [\n",
    "    53,\n",
    "    185,\n",
    "    197,\n",
    "    437,\n",
    "    492,\n",
    "    762,\n",
    "    796,\n",
    "    821,\n",
    "    847,\n",
    "    1161,\n",
    "    1221,\n",
    "    1318,\n",
    "    1376,\n",
    "    249,\n",
    "    313,\n",
    "    335,\n",
    "    451,\n",
    "    523,\n",
    "    691,\n",
    "    706,\n",
    "    934,\n",
    "    1182,\n",
    "    1298,\n",
    "]\n",
    "print(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:49:39.160844Z",
     "start_time": "2019-07-13T18:49:39.148134Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove outlers from predictors and response\n",
    "train.data = train.data.drop(train.data.index[outliers])\n",
    "train.target = train.target.drop(index=outliers)\n",
    "print(train.data.shape)\n",
    "print(train.target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Validation5'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing data\n",
    "\n",
    "-__MCAR__ - Completely unsystematic missingness, completely unralted to any of the other variables. simple imputation of mean, median or mode is most acceptable for this type of missingness.\n",
    "\n",
    "-__MAR__ - The nature of the missing data is related to observed data in other variables, not the missing data. The missing data is conditional on some other variable.  For example, men are more likely to tell you their weight than woemn. The missingness of weight has to do with gender.\n",
    "\n",
    "-__MNAR__ - There is a relationship between the propensity of a value to be missing and its values. For example, the wealthiest people choosing not to state their income.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Missing-data'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Evaluate1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:49:39.613511Z",
     "start_time": "2019-07-13T18:49:39.164509Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate missing data\n",
    "train.edaMissingSummary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:49:40.225778Z",
     "start_time": "2019-07-13T18:49:39.620996Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate missing data\n",
    "valid.edaMissingSummary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:49:40.252188Z",
     "start_time": "2019-07-13T18:49:40.228726Z"
    }
   },
   "outputs": [],
   "source": [
    "# compare feature with missing data\n",
    "train.missingColCompare(train.data, valid.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:49:40.278803Z",
     "start_time": "2019-07-13T18:49:40.254739Z"
    }
   },
   "outputs": [],
   "source": [
    "# missingdata_df = merged_df.columns[merged_df.isnull().any()].tolist()\n",
    "# msno.matrix(merged_df[missingdata_df])\n",
    "\n",
    "# msno.bar(merged_df[missingdata_df], color=\"blue\", log=True, figsize=(30,18))\n",
    "\n",
    "# #\n",
    "# msno.heatmap(merged_df[missingdata_df], figsize=(20,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Training1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:49:40.695597Z",
     "start_time": "2019-07-13T18:49:40.281094Z"
    }
   },
   "outputs": [],
   "source": [
    "# apply imputations to missing data in training dataset\n",
    "trainPipe = pipeline.Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"imputeConstantCat\",\n",
    "            train.ConstantImputer(\n",
    "                cols=[\n",
    "                    \"PoolQC\",\n",
    "                    \"Alley\",\n",
    "                    \"Fence\",\n",
    "                    \"FireplaceQu\",\n",
    "                    \"GarageType\",\n",
    "                    \"GarageFinish\",\n",
    "                    \"GarageQual\",\n",
    "                    \"MiscFeature\",\n",
    "                    \"GarageCond\",\n",
    "                    \"BsmtQual\",\n",
    "                    \"BsmtCond\",\n",
    "                    \"BsmtExposure\",\n",
    "                    \"BsmtFinType1\",\n",
    "                    \"BsmtFinType2\",\n",
    "                    \"MasVnrType\",\n",
    "                ],\n",
    "                fill=\"Nonexistent\",\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"imputeConstantNum\",\n",
    "            train.ConstantImputer(cols=[\"GarageYrBlt\", \"MasVnrArea\"], fill=0),\n",
    "        ),\n",
    "        (\"imputeMode\", train.ModeImputer(cols=[\"Electrical\"])),\n",
    "        (\n",
    "            \"imputeContext\",\n",
    "            train.ContextImputer(\n",
    "                nullCol=\"LotFrontage\", contextCol=\"Neighborhood\", strategy=\"mean\"\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "train.data = trainPipe.transform(train.data)\n",
    "train.edaMissingSummary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Validation1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:49:40.855601Z",
     "start_time": "2019-07-13T18:49:40.698022Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# apply imputations to missing data in validation dataset\n",
    "validPipe = pipeline.Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"imputeConstantCat\",\n",
    "            valid.ConstantImputer(\n",
    "                cols=[\n",
    "                    \"PoolQC\",\n",
    "                    \"Alley\",\n",
    "                    \"Fence\",\n",
    "                    \"FireplaceQu\",\n",
    "                    \"GarageType\",\n",
    "                    \"GarageFinish\",\n",
    "                    \"GarageQual\",\n",
    "                    \"MiscFeature\",\n",
    "                    \"GarageCond\",\n",
    "                    \"BsmtQual\",\n",
    "                    \"BsmtCond\",\n",
    "                    \"BsmtExposure\",\n",
    "                    \"BsmtFinType1\",\n",
    "                    \"BsmtFinType2\",\n",
    "                    \"MasVnrType\",\n",
    "                ],\n",
    "                fill=\"Nonexistent\",\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"imputeConstantNum\",\n",
    "            valid.ConstantImputer(\n",
    "                cols=[\n",
    "                    \"GarageYrBlt\",\n",
    "                    \"MasVnrArea\",\n",
    "                    \"BsmtUnfSF\",\n",
    "                    \"GarageArea\",\n",
    "                    \"BsmtFinSF1\",\n",
    "                    \"TotalBsmtSF\",\n",
    "                    \"BsmtFinSF2\",\n",
    "                ],\n",
    "                fill=0,\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"imputeModeCat\",\n",
    "            valid.ModeImputer(\n",
    "                cols=[\n",
    "                    \"Functional\",\n",
    "                    \"SaleType\",\n",
    "                    \"Exterior1st\",\n",
    "                    \"MSZoning\",\n",
    "                    \"Exterior2nd\",\n",
    "                    \"KitchenQual\",\n",
    "                    \"Utilities\",\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"imputeModeNum\",\n",
    "            valid.NumericalImputer(\n",
    "                cols=[\"BsmtHalfBath\", \"GarageCars\", \"BsmtFullBath\"],\n",
    "                strategy=\"most_frequent\",\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"imputeContext\",\n",
    "            valid.ContextImputer(\n",
    "                nullCol=\"LotFrontage\",\n",
    "                contextCol=\"Neighborhood\",\n",
    "                strategy=\"mean\",\n",
    "                train=False,\n",
    "                trainDf=trainPipe.named_steps[\"imputeContext\"].fillDf,\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "valid.data = validPipe.transform(valid.data)\n",
    "valid.edaMissingSummary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Engineering'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Evaluate3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Training3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:49:40.867687Z",
     "start_time": "2019-07-13T18:49:40.859006Z"
    }
   },
   "outputs": [],
   "source": [
    "# additional features\n",
    "train.data[\"BsmtFinSF\"] = train.data[\"BsmtFinSF1\"] + train.data[\"BsmtFinSF2\"]\n",
    "train.data[\"TotalSF\"] = (\n",
    "    train.data[\"TotalBsmtSF\"] + train.data[\"1stFlrSF\"] + train.data[\"2ndFlrSF\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Validation3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:49:40.879596Z",
     "start_time": "2019-07-13T18:49:40.870478Z"
    }
   },
   "outputs": [],
   "source": [
    "# additional features\n",
    "valid.data[\"BsmtFinSF\"] = valid.data[\"BsmtFinSF1\"] + valid.data[\"BsmtFinSF2\"]\n",
    "valid.data[\"TotalSF\"] = (\n",
    "    valid.data[\"TotalBsmtSF\"] + valid.data[\"1stFlrSF\"] + valid.data[\"2ndFlrSF\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Encoding'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Evaluate2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:49:40.936099Z",
     "start_time": "2019-07-13T18:49:40.882385Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# counts of unique values in training data categorical columns\n",
    "train.data[train.featureByDtype_[\"categorical\"]].apply(pd.Series.nunique, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:49:41.025761Z",
     "start_time": "2019-07-13T18:49:40.942010Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print unique values in each categorical columns\n",
    "for col in train.data[train.featureByDtype_[\"categorical\"]]:\n",
    "    print(col, np.unique(train.data[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:49:41.085149Z",
     "start_time": "2019-07-13T18:49:41.031314Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# counts of unique values in validation data string columns\n",
    "valid.data[valid.featureByDtype_[\"categorical\"]].apply(pd.Series.nunique, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:49:41.159781Z",
     "start_time": "2019-07-13T18:49:41.090257Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print unique values in each categorical columns\n",
    "for col in valid.data[valid.featureByDtype_[\"categorical\"]]:\n",
    "    if col not in [\"Name\", \"Cabin\"]:\n",
    "        print(col, np.unique(valid.data[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:49:41.215253Z",
     "start_time": "2019-07-13T18:49:41.162412Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# identify values that are present in the training data but not the validation data, and vice versa\n",
    "for col in train.featureByDtype_[\"categorical\"]:\n",
    "    trainValues = train.data[col].unique()\n",
    "    validValues = valid.data[col].unique()\n",
    "\n",
    "    trainDiff = set(trainValues) - set(validValues)\n",
    "    validDiff = set(validValues) - set(trainValues)\n",
    "\n",
    "    if len(trainDiff) > 0 or len(validDiff) > 0:\n",
    "        print(\"\\n\\n*** \" + col)\n",
    "        print(\"Value present in training data, not in validation data\")\n",
    "        print(trainDiff)\n",
    "        print(\"Value present in validation data, not in training data\")\n",
    "        print(validDiff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Training2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:49:41.355981Z",
     "start_time": "2019-07-13T18:49:41.217669Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ordinal column encoding instructions\n",
    "ordinalEncodings = {\n",
    "    \"Street\": {\"Grvl\": 0, \"Pave\": 1},\n",
    "    \"Alley\": {\"Nonexistent\": 0, \"Grvl\": 1, \"Pave\": 2},\n",
    "    \"LotShape\": {\"IR3\": 0, \"IR2\": 1, \"IR1\": 2, \"Reg\": 3},\n",
    "    \"Utilities\": {\"ELO\": 0, \"NoSeWa\": 1, \"NoSewr\": 2, \"AllPub\": 3},\n",
    "    \"LotConfig\": {\"FR3\": 0, \"FR2\": 1, \"Corner\": 2, \"Inside\": 3, \"CulDSac\": 4},\n",
    "    \"LandSlope\": {\"Sev\": 0, \"Mod\": 1, \"Gtl\": 2},\n",
    "    \"ExterQual\": {\"Po\": 0, \"Fa\": 1, \"TA\": 2, \"Gd\": 3, \"Ex\": 4},\n",
    "    \"ExterCond\": {\"Po\": 0, \"Fa\": 1, \"TA\": 2, \"Gd\": 3, \"Ex\": 4},\n",
    "    \"BsmtQual\": {\"Nonexistent\": 0, \"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5},\n",
    "    \"BsmtCond\": {\"Nonexistent\": 0, \"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5},\n",
    "    \"BsmtExposure\": {\"Nonexistent\": 0, \"No\": 1, \"Mn\": 2, \"Av\": 3, \"Gd\": 4},\n",
    "    \"BsmtFinType1\": {\n",
    "        \"Nonexistent\": 0,\n",
    "        \"Unf\": 1,\n",
    "        \"LwQ\": 2,\n",
    "        \"BLQ\": 3,\n",
    "        \"Rec\": 4,\n",
    "        \"ALQ\": 5,\n",
    "        \"GLQ\": 6,\n",
    "    },  # split?\n",
    "    \"BsmtFinType2\": {\n",
    "        \"Nonexistent\": 0,\n",
    "        \"Unf\": 1,\n",
    "        \"LwQ\": 2,\n",
    "        \"BLQ\": 3,\n",
    "        \"Rec\": 4,\n",
    "        \"ALQ\": 5,\n",
    "        \"GLQ\": 6,\n",
    "    },  # split?\n",
    "    \"HeatingQC\": {\"Po\": 0, \"Fa\": 1, \"TA\": 2, \"Gd\": 3, \"Ex\": 4},\n",
    "    \"CentralAir\": {\"N\": 0, \"Y\": 1},\n",
    "    \"Electrical\": {\"FuseP\": 0, \"FuseF\": 1, \"FuseA\": 2, \"Mix\": 3, \"SBrkr\": 4},\n",
    "    \"KitchenQual\": {\"Po\": 0, \"Fa\": 1, \"TA\": 2, \"Gd\": 3, \"Ex\": 4},\n",
    "    \"Functional\": {\n",
    "        \"Sal\": 0,\n",
    "        \"Sev\": 1,\n",
    "        \"Maj2\": 2,\n",
    "        \"Maj1\": 3,\n",
    "        \"Mod\": 4,\n",
    "        \"Min2\": 5,\n",
    "        \"Min1\": 6,\n",
    "        \"Typ\": 7,\n",
    "    },\n",
    "    \"FireplaceQu\": {\"Nonexistent\": 0, \"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5},\n",
    "    \"GarageFinish\": {\"Nonexistent\": 0, \"Unf\": 1, \"RFn\": 2, \"Fin\": 3},\n",
    "    \"GarageQual\": {\"Nonexistent\": 0, \"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5},\n",
    "    \"GarageCond\": {\"Nonexistent\": 0, \"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5},\n",
    "    \"PavedDrive\": {\"N\": 0, \"P\": 1, \"Y\": 2},\n",
    "    \"PoolQC\": {\"Nonexistent\": 0, \"Fa\": 1, \"TA\": 2, \"Gd\": 3, \"Ex\": 4},\n",
    "}\n",
    "\n",
    "# nominal columns\n",
    "nomCatCols = [\n",
    "    \"MSSubClass\",\n",
    "    \"MSZoning\",\n",
    "    \"LandContour\",\n",
    "    \"Neighborhood\",\n",
    "    \"Condition1\",\n",
    "    \"Condition2\",\n",
    "    \"BldgType\",\n",
    "    \"HouseStyle\",\n",
    "    \"RoofStyle\",\n",
    "    \"RoofMatl\",\n",
    "    \"Exterior1st\",\n",
    "    \"Exterior2nd\",\n",
    "    \"MasVnrType\",\n",
    "    \"Foundation\",\n",
    "    \"Heating\",\n",
    "    \"GarageType\",\n",
    "    \"Fence\",\n",
    "    \"SaleType\",\n",
    "    \"SaleCondition\",\n",
    "    \"MiscFeature\",\n",
    "]\n",
    "\n",
    "# apply encodings to training data\n",
    "trainPipe = pipeline.Pipeline(\n",
    "    [\n",
    "        (\"encodeOrdinal\", train.CustomOrdinalEncoder(encodings=ordinalEncodings)),\n",
    "        (\"dummyNominal\", train.Dummies(cols=nomCatCols, dropFirst=False)),\n",
    "    ]\n",
    ")\n",
    "train.data = trainPipe.transform(train.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Validation2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:49:41.492121Z",
     "start_time": "2019-07-13T18:49:41.358203Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# apply encodings to validation data\n",
    "validPipe = pipeline.Pipeline(\n",
    "    [\n",
    "        (\"encodeOrdinal\", valid.CustomOrdinalEncoder(encodings=ordinalEncodings)),\n",
    "        (\"dummyNominal\", valid.Dummies(cols=nomCatCols, dropFirst=False)),\n",
    "        (\"levels\", valid.MissingDummies(trainCols=train.data.columns)),\n",
    "    ]\n",
    ")\n",
    "valid.data = validPipe.transform(valid.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Transformation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Evaluate4'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:49:41.740406Z",
     "start_time": "2019-07-13T18:49:41.494087Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate skew of continuous features - validation data\n",
    "train.skewSummary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:49:42.067837Z",
     "start_time": "2019-07-13T18:49:41.742350Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate skew of continuous features - training data\n",
    "valid.skewSummary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Training4'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:49:47.962187Z",
     "start_time": "2019-07-13T18:49:42.070424Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# skew correct in training dataset, which also learns te best lambda value for each columns\n",
    "trainPipe = pipeline.Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"skew\",\n",
    "            train.SkewTransform(\n",
    "                cols=train.featureByDtype_[\"continuous\"], skewMin=0.75, pctZeroMax=1.0\n",
    "            ),\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "train.data = trainPipe.transform(train.data)\n",
    "train.skewSummary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Validation4'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:49:48.250489Z",
     "start_time": "2019-07-13T18:49:47.964724Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# skew correction in validation dataset using lambdas learned on training data\n",
    "validPipe = pipeline.Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"skew\",\n",
    "            valid.SkewTransform(\n",
    "                train=False, trainDict=trainPipe.named_steps[\"skew\"].colValueDict_\n",
    "            ),\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "valid.data = validPipe.transform(valid.data)\n",
    "valid.skewSummary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers (final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Outliers-final'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Training6'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:49:48.342846Z",
     "start_time": "2019-07-13T18:49:48.253439Z"
    }
   },
   "outputs": [],
   "source": [
    "# identify outliers using IQR\n",
    "trainPipe = pipeline.Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"outlier\",\n",
    "            train.OutlierIQR(\n",
    "                outlierCount=8, iqrStep=1.5, features=nonNullNumCol, dropOutliers=False\n",
    "            ),\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "train.data = trainPipe.transform(train.data)\n",
    "\n",
    "# capture outliers\n",
    "iqrOutliers = np.array(sorted(trainPipe.named_steps[\"outlier\"].outliers_))\n",
    "print(iqrOutliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:49:48.703057Z",
     "start_time": "2019-07-13T18:49:48.346667Z"
    }
   },
   "outputs": [],
   "source": [
    "# identify outliers using Isolation Forest\n",
    "clf = ensemble.IsolationForest(\n",
    "    behaviour=\"new\", max_samples=train.data.shape[0], random_state=0, contamination=0.02\n",
    ")\n",
    "clf.fit(train.data[nonNullNumCol])\n",
    "preds = clf.predict(train.data[nonNullNumCol])\n",
    "# np.unique(preds, return_counts = True)\n",
    "\n",
    "# evaluate index values\n",
    "mask = np.isin(preds, -1)  # np.in1d if np.isin is not available\n",
    "ifOutliers = np.where(mask)\n",
    "print(ifOutliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:49:56.330004Z",
     "start_time": "2019-07-13T18:49:48.705166Z"
    }
   },
   "outputs": [],
   "source": [
    "# identify outliers using Extended Isolation Forest\n",
    "import eif as iso\n",
    "\n",
    "if_eif = iso.iForest(\n",
    "    train.data[nonNullNumCol].values, ntrees=100, sample_size=256, ExtensionLevel=1\n",
    ")\n",
    "\n",
    "# calculate anomaly scores\n",
    "anomalies_ratio = 0.009\n",
    "anomaly_scores = if_eif.compute_paths(X_in=train.data[nonNullNumCol].values)\n",
    "anomaly_scores_sorted = np.argsort(anomaly_scores)\n",
    "eifOutliers = anomaly_scores_sorted[\n",
    "    -int(np.ceil(anomalies_ratio * train.data.shape[0])) :\n",
    "]\n",
    "print(sorted(eifOutliers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:49:56.347870Z",
     "start_time": "2019-07-13T18:49:56.340340Z"
    }
   },
   "outputs": [],
   "source": [
    "# identify outliers that are identified in multiple algorithms\n",
    "# reduce(np.intersect1d, (iqrOutliers, ifOutliers, eifOutliers))\n",
    "reduce(np.intersect1d, (ifOutliers, eifOutliers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Data evaluation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Feature-importance'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:49:56.473554Z",
     "start_time": "2019-07-13T18:49:56.350194Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# feature importance summary table\n",
    "featureImp = train.featureImportanceSummary()\n",
    "featureImp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rationality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Rationality'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:50:01.619435Z",
     "start_time": "2019-07-13T18:49:56.476184Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# percent difference summary\n",
    "dfDiff = abs(\n",
    "    (\n",
    "        ((valid.data.describe() + 1) - (train.data.describe() + 1))\n",
    "        / (train.data.describe() + 1)\n",
    "    )\n",
    "    * 100\n",
    ")\n",
    "dfDiff = dfDiff[dfDiff.columns].replace({0: np.nan})\n",
    "dfDiff[dfDiff < 0] = np.nan\n",
    "dfDiff = dfDiff.fillna(\"\")\n",
    "display(dfDiff)\n",
    "display(train.data.describe())\n",
    "display(valid.data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value override"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Value override'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:50:01.628944Z",
     "start_time": "2019-07-13T18:50:01.622872Z"
    }
   },
   "outputs": [],
   "source": [
    "# change clearly erroneous value to what it probably was\n",
    "valid.data[\"GarageYrBlt\"].replace({2207: 2007}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous feature EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Continuous-feature-EDA3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate & feature vs. target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Univariate-&-feature-vs.-target3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:50:34.573678Z",
     "start_time": "2019-07-13T18:50:01.632033Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# continuous features\n",
    "train.edaNumTargetNumFeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Correlation3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation (top vs. target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Correlation-top-vs-target3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:50:35.335842Z",
     "start_time": "2019-07-13T18:50:34.576403Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# correlation heat map with most highly correlated features relative to the target\n",
    "p = PrettierPlot()\n",
    "ax = p.makeCanvas()\n",
    "p.prettyCorrHeatmapTarget(df=train.data, target=train.target, thresh=0.6, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Remarks - There are three pairs of highly correlated features:\n",
    "    - 'GarageArea' and 'GarageCars'\n",
    "    - 'TotRmsAbvGrd' and 'GrLivArea'\n",
    "    - '1stFlrSF' and 'TotalBsmtSF\n",
    "This makes sense, given what each feature represents and how each pair items relate to each other. We likely only need one feature from each pair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Modeling'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Prepare-training-data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:50:40.762466Z",
     "start_time": "2019-07-13T18:50:35.338099Z"
    }
   },
   "outputs": [],
   "source": [
    "# import training data\n",
    "dfTrain = pd.read_csv(\"/home/data-science-portfolio/data/kaggleHousingPrices/train.csv\")\n",
    "train = mlm.Machine(\n",
    "    data=dfTrain,\n",
    "    target=[\"SalePrice\"],\n",
    "    removeFeatures=[\"Id\", \"MiscVal\"],\n",
    "    overrideCat=[\n",
    "        \"MSSubClass\",\n",
    "        \"OverallQual\",\n",
    "        \"OverallCond\",\n",
    "        \"YearBuilt\",\n",
    "        \"YearRemodAdd\",\n",
    "        \"MoSold\",\n",
    "        \"YrSold\",\n",
    "    ],\n",
    "    targetType=\"continuous\",\n",
    ")\n",
    "\n",
    "### training data transformation pipeline\n",
    "### ordinal columns\n",
    "ordinalEncodings = {\n",
    "    \"Street\": {\"Grvl\": 0, \"Pave\": 1},\n",
    "    \"Alley\": {\"Nonexistent\": 0, \"Grvl\": 1, \"Pave\": 2},\n",
    "    \"LotShape\": {\"IR3\": 0, \"IR2\": 1, \"IR1\": 2, \"Reg\": 3},\n",
    "    \"Utilities\": {\"ELO\": 0, \"NoSeWa\": 1, \"NoSewr\": 2, \"AllPub\": 3},\n",
    "    \"LotConfig\": {\"FR3\": 0, \"FR2\": 1, \"Corner\": 2, \"Inside\": 3, \"CulDSac\": 4},\n",
    "    \"LandSlope\": {\"Sev\": 0, \"Mod\": 1, \"Gtl\": 2},\n",
    "    \"ExterQual\": {\"Po\": 0, \"Fa\": 1, \"TA\": 2, \"Gd\": 3, \"Ex\": 4},\n",
    "    \"ExterCond\": {\"Po\": 0, \"Fa\": 1, \"TA\": 2, \"Gd\": 3, \"Ex\": 4},\n",
    "    \"BsmtQual\": {\"Nonexistent\": 0, \"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5},\n",
    "    \"BsmtCond\": {\"Nonexistent\": 0, \"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5},\n",
    "    \"BsmtExposure\": {\"Nonexistent\": 0, \"No\": 1, \"Mn\": 2, \"Av\": 3, \"Gd\": 4},\n",
    "    \"BsmtFinType1\": {\n",
    "        \"Nonexistent\": 0,\n",
    "        \"Unf\": 1,\n",
    "        \"LwQ\": 2,\n",
    "        \"BLQ\": 3,\n",
    "        \"Rec\": 4,\n",
    "        \"ALQ\": 5,\n",
    "        \"GLQ\": 6,\n",
    "    },  # split?\n",
    "    \"BsmtFinType2\": {\n",
    "        \"Nonexistent\": 0,\n",
    "        \"Unf\": 1,\n",
    "        \"LwQ\": 2,\n",
    "        \"BLQ\": 3,\n",
    "        \"Rec\": 4,\n",
    "        \"ALQ\": 5,\n",
    "        \"GLQ\": 6,\n",
    "    },  # split?\n",
    "    \"HeatingQC\": {\"Po\": 0, \"Fa\": 1, \"TA\": 2, \"Gd\": 3, \"Ex\": 4},\n",
    "    \"CentralAir\": {\"N\": 0, \"Y\": 1},\n",
    "    \"Electrical\": {\"FuseP\": 0, \"FuseF\": 1, \"FuseA\": 2, \"Mix\": 3, \"SBrkr\": 4},\n",
    "    \"KitchenQual\": {\"Po\": 0, \"Fa\": 1, \"TA\": 2, \"Gd\": 3, \"Ex\": 4},\n",
    "    \"Functional\": {\n",
    "        \"Sal\": 0,\n",
    "        \"Sev\": 1,\n",
    "        \"Maj2\": 2,\n",
    "        \"Maj1\": 3,\n",
    "        \"Mod\": 4,\n",
    "        \"Min2\": 5,\n",
    "        \"Min1\": 6,\n",
    "        \"Typ\": 7,\n",
    "    },\n",
    "    \"FireplaceQu\": {\"Nonexistent\": 0, \"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5},\n",
    "    \"GarageFinish\": {\"Nonexistent\": 0, \"Unf\": 1, \"RFn\": 2, \"Fin\": 3},\n",
    "    \"GarageQual\": {\"Nonexistent\": 0, \"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5},\n",
    "    \"GarageCond\": {\"Nonexistent\": 0, \"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5},\n",
    "    \"PavedDrive\": {\"N\": 0, \"P\": 1, \"Y\": 2},\n",
    "    \"PoolQC\": {\"Nonexistent\": 0, \"Fa\": 1, \"TA\": 2, \"Gd\": 3, \"Ex\": 4},\n",
    "}\n",
    "\n",
    "### nominal columns\n",
    "nomCatCols = [\n",
    "    \"MSSubClass\",\n",
    "    \"MSZoning\",\n",
    "    \"LandContour\",\n",
    "    \"Neighborhood\",\n",
    "    \"Condition1\",\n",
    "    \"Condition2\",\n",
    "    \"BldgType\",\n",
    "    \"HouseStyle\",\n",
    "    \"RoofStyle\",\n",
    "    \"RoofMatl\",\n",
    "    \"Exterior1st\",\n",
    "    \"Exterior2nd\",\n",
    "    \"MasVnrType\",\n",
    "    \"Foundation\",\n",
    "    \"Heating\",\n",
    "    \"GarageType\",\n",
    "    \"Fence\",\n",
    "    \"SaleType\",\n",
    "    \"SaleCondition\",\n",
    "    \"MiscFeature\",\n",
    "]\n",
    "\n",
    "### additional features\n",
    "train.data[\"BsmtFinSF\"] = train.data[\"BsmtFinSF1\"] + train.data[\"BsmtFinSF2\"]\n",
    "train.data[\"TotalSF\"] = (\n",
    "    train.data[\"TotalBsmtSF\"] + train.data[\"1stFlrSF\"] + train.data[\"2ndFlrSF\"]\n",
    ")\n",
    "\n",
    "### observation removal\n",
    "outliers = [\n",
    "    53,\n",
    "    185,\n",
    "    197,\n",
    "    437,\n",
    "    492,\n",
    "    762,\n",
    "    796,\n",
    "    821,\n",
    "    847,\n",
    "    1161,\n",
    "    1221,\n",
    "    1318,\n",
    "    1376,\n",
    "    249,\n",
    "    313,\n",
    "    335,\n",
    "    451,\n",
    "    523,\n",
    "    691,\n",
    "    706,\n",
    "    934,\n",
    "    1182,\n",
    "    1298,\n",
    "]\n",
    "train.data = train.data.drop(train.data.index[outliers])\n",
    "train.target = train.target.drop(index=outliers)\n",
    "\n",
    "### pre-processing pipeline\n",
    "trainPipe = pipeline.Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"imputeConstantCat\",\n",
    "            train.ConstantImputer(\n",
    "                cols=[\n",
    "                    \"PoolQC\",\n",
    "                    \"Alley\",\n",
    "                    \"Fence\",\n",
    "                    \"FireplaceQu\",\n",
    "                    \"GarageType\",\n",
    "                    \"GarageFinish\",\n",
    "                    \"GarageQual\",\n",
    "                    \"MiscFeature\",\n",
    "                    \"GarageCond\",\n",
    "                    \"BsmtQual\",\n",
    "                    \"BsmtCond\",\n",
    "                    \"BsmtExposure\",\n",
    "                    \"BsmtFinType1\",\n",
    "                    \"BsmtFinType2\",\n",
    "                    \"MasVnrType\",\n",
    "                ],\n",
    "                fill=\"Nonexistent\",\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"imputeConstantNum\",\n",
    "            train.ConstantImputer(cols=[\"GarageYrBlt\", \"MasVnrArea\"], fill=0),\n",
    "        ),\n",
    "        (\"imputeMode\", train.ModeImputer(cols=[\"Electrical\"])),\n",
    "        (\n",
    "            \"imputeContext\",\n",
    "            train.ContextImputer(\n",
    "                nullCol=\"LotFrontage\", contextCol=\"Neighborhood\", strategy=\"mean\"\n",
    "            ),\n",
    "        ),\n",
    "        (\"encodeOrdinal\", train.CustomOrdinalEncoder(encodings=ordinalEncodings)),\n",
    "        (\"dummyNominal\", train.Dummies(cols=nomCatCols, dropFirst=False)),\n",
    "        (\n",
    "            \"skew\",\n",
    "            train.SkewTransform(\n",
    "                cols=train.featureByDtype_[\"continuous\"], skewMin=0.75, pctZeroMax=1.0\n",
    "            ),\n",
    "        ),\n",
    "        (\"scale\", train.Robust(cols=\"non-binary\")),\n",
    "    ]\n",
    ")\n",
    "train.data = trainPipe.transform(train.data)\n",
    "\n",
    "train.target = np.log1p(train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Prepare-validation-data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-13T18:57:30.150830Z",
     "start_time": "2019-07-13T18:57:29.875765Z"
    }
   },
   "outputs": [],
   "source": [
    "# import valid data\n",
    "dfValid = pd.read_csv(\"/home/data-science-portfolio/data/kaggleHousingPrices/test.csv\")\n",
    "valid = mlm.Machine(\n",
    "    data=dfValid,\n",
    "    removeFeatures=[\"Id\", \"MiscVal\"],\n",
    "    overrideCat=[\n",
    "        \"MSSubClass\",\n",
    "        \"OverallQual\",\n",
    "        \"OverallCond\",\n",
    "        \"YearBuilt\",\n",
    "        \"YearRemodAdd\",\n",
    "        \"MoSold\",\n",
    "        \"YrSold\",\n",
    "    ],\n",
    "    targetType=\"continuous\",\n",
    ")\n",
    "\n",
    "### additional features\n",
    "valid.data[\"BsmtFinSF\"] = valid.data[\"BsmtFinSF1\"] + valid.data[\"BsmtFinSF2\"]\n",
    "valid.data[\"TotalSF\"] = (\n",
    "    valid.data[\"TotalBsmtSF\"] + valid.data[\"1stFlrSF\"] + valid.data[\"2ndFlrSF\"]\n",
    ")\n",
    "valid.data.loc[valid.data[\"TotalSF\"].isnull(), \"TotalSF\"] = (\n",
    "    valid.data[\"1stFlrSF\"] + valid.data[\"2ndFlrSF\"]\n",
    ")\n",
    "\n",
    "### pre-processing pipeline\n",
    "validPipe = pipeline.Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"imputeConstantCat\",\n",
    "            valid.ConstantImputer(\n",
    "                cols=[\n",
    "                    \"PoolQC\",\n",
    "                    \"Alley\",\n",
    "                    \"Fence\",\n",
    "                    \"FireplaceQu\",\n",
    "                    \"GarageType\",\n",
    "                    \"GarageFinish\",\n",
    "                    \"GarageQual\",\n",
    "                    \"MiscFeature\",\n",
    "                    \"GarageCond\",\n",
    "                    \"BsmtQual\",\n",
    "                    \"BsmtCond\",\n",
    "                    \"BsmtExposure\",\n",
    "                    \"BsmtFinType1\",\n",
    "                    \"BsmtFinType2\",\n",
    "                    \"MasVnrType\",\n",
    "                ],\n",
    "                fill=\"Nonexistent\",\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"imputeConstantNum\",\n",
    "            valid.ConstantImputer(\n",
    "                cols=[\n",
    "                    \"GarageYrBlt\",\n",
    "                    \"MasVnrArea\",\n",
    "                    \"BsmtUnfSF\",\n",
    "                    \"GarageArea\",\n",
    "                    \"BsmtFinSF1\",\n",
    "                    \"TotalBsmtSF\",\n",
    "                    \"BsmtFinSF2\",\n",
    "                ],\n",
    "                fill=0,\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"imputeModeCat\",\n",
    "            valid.ModeImputer(\n",
    "                cols=[\n",
    "                    \"Functional\",\n",
    "                    \"SaleType\",\n",
    "                    \"Exterior1st\",\n",
    "                    \"MSZoning\",\n",
    "                    \"Exterior2nd\",\n",
    "                    \"KitchenQual\",\n",
    "                    \"Utilities\",\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"imputeModeNum\",\n",
    "            valid.NumericalImputer(\n",
    "                cols=[\"BsmtHalfBath\", \"GarageCars\", \"BsmtFullBath\"],\n",
    "                strategy=\"most_frequent\",\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"imputeContext\",\n",
    "            valid.ContextImputer(\n",
    "                nullCol=\"LotFrontage\",\n",
    "                contextCol=\"Neighborhood\",\n",
    "                strategy=\"mean\",\n",
    "                train=False,\n",
    "                trainDf=trainPipe.named_steps[\"imputeContext\"].fillDf,\n",
    "            ),\n",
    "        ),\n",
    "        (\"encodeOrdinal\", valid.CustomOrdinalEncoder(encodings=ordinalEncodings)),\n",
    "        (\"dummyNominal\", valid.Dummies(cols=nomCatCols, dropFirst=False)),\n",
    "        (\n",
    "            \"skew\",\n",
    "            valid.SkewTransform(\n",
    "                cols=valid.featureByDtype_[\"continuous\"],\n",
    "                train=False,\n",
    "                trainDict=trainPipe.named_steps[\"skew\"].colValueDict_,\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"scale\",\n",
    "            valid.Robust(\n",
    "                cols=\"non-binary\",\n",
    "                train=False,\n",
    "                trainDict=trainPipe.named_steps[\"scale\"].colValueDict_,\n",
    "            ),\n",
    "        ),\n",
    "        (\"levels\", valid.MissingDummies(trainCols=train.data.columns)),\n",
    "    ]\n",
    ")\n",
    "valid.data = validPipe.transform(valid.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'GridSearch'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T03:53:32.591786Z",
     "start_time": "2019-05-04T03:53:32.529644Z"
    },
    "code_folding": [
     31,
     43,
     51,
     60,
     65,
     72,
     79
    ]
   },
   "outputs": [],
   "source": [
    "# parameter space\n",
    "allSpace = {\n",
    "    \"linear_model.Lasso\": {\"alpha\": hp.uniform(\"alpha\", 0.0000001, 10)},\n",
    "    \"linear_model.Ridge\": {\"alpha\": hp.uniform(\"alpha\", 0.0001, 20)},\n",
    "    \"linear_model.ElasticNet\": {\n",
    "        \"alpha\": hp.uniform(\"alpha\", 0.0000001, 10),\n",
    "        \"l1_ratio\": hp.uniform(\"l1_ratio\", 0.0, 0.2),\n",
    "    },\n",
    "    \"kernel_ridge.KernelRidge\": {\n",
    "        \"alpha\": hp.uniform(\"alpha\", 0.0001, 15),\n",
    "        \"kernel\": hp.choice(\"kernel\", [\"linear\", \"polynomial\", \"rbf\"]),\n",
    "        \"degree\": hp.choice(\"degree\", [2, 3]),\n",
    "        \"gamma\": hp.uniform(\"gamma\", 6.0, 8.0),\n",
    "    },\n",
    "    \"lightgbm.LGBMRegressor\": {\n",
    "        \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.4, 0.65),\n",
    "        \"boosting_type\": hp.choice(\"boosting_type\", [\"gbdt\"]),\n",
    "        \"subsample\": hp.uniform(\"subsample\", 0.5, 1),\n",
    "        \"learning_rate\": hp.uniform(\"learning_rate\", 0.000000001, 0.05),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 8, dtype=int)),\n",
    "        \"min_child_samples\": hp.uniform(\"min_child_samples\", 10, 100),\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 4000, 10, dtype=int)),\n",
    "        \"num_leaves\": hp.uniform(\"num_leaves\", 8, 150),\n",
    "        \"reg_alpha\": hp.uniform(\"reg_alpha\", 0.0, 0.2),\n",
    "        \"reg_lambda\": hp.uniform(\"reg_lambda\", 0.05, 0.25),\n",
    "        \"subsample_for_bin\": hp.uniform(\"subsample_for_bin\", 200000, 400000),\n",
    "    },\n",
    "    \"xgboost.XGBRegressor\": {\n",
    "        \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.4, 0.65),\n",
    "        \"gamma\": hp.uniform(\"gamma\", 0.0, 2),\n",
    "        \"reg_alpha\": hp.uniform(\"reg_alpha\", 0.0, 0.3),\n",
    "        \"reg_lambda\": hp.uniform(\"reg_lambda\", 0.4, 1.0),\n",
    "        \"learning_rate\": hp.uniform(\"learning_rate\", 0.00001, 0.08),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 12, dtype=int)),\n",
    "        \"min_child_weight\": hp.uniform(\"min_child_weight\", 1, 8),\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(4000, 10000, 10, dtype=int))\n",
    "        # ,'objective' : hp.choice('objective', ['binary:logistic'])\n",
    "        ,\n",
    "        \"subsample\": hp.uniform(\"subsample\", 0.5, 0.8),\n",
    "    },\n",
    "    \"ensemble.RandomForestRegressor\": {\n",
    "        \"bootstrap\": hp.choice(\"bootstrap\", [True, False]),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(8, 20, dtype=int)),\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 40000, 10, dtype=int)),\n",
    "        \"max_features\": hp.choice(\"max_features\", [\"sqrt\"]),\n",
    "        \"min_samples_split\": hp.choice(\n",
    "            \"min_samples_split\", np.arange(2, 20, dtype=int)\n",
    "        ),\n",
    "        \"min_samples_leaf\": hp.choice(\"min_samples_leaf\", np.arange(2, 15, dtype=int)),\n",
    "    },\n",
    "    \"ensemble.GradientBoostingRegressor\": {\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 10000, 10, dtype=int)),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 20, dtype=int)),\n",
    "        \"max_features\": hp.choice(\"max_features\", [\"auto\", \"sqrt\"]),\n",
    "        \"learning_rate\": hp.uniform(\"learning_rate\", 0.01, 0.2),\n",
    "        \"loss\": hp.choice(\"loss\", [\"ls\", \"lad\", \"huber\", \"quantile\"]),\n",
    "        \"min_samples_split\": hp.choice(\n",
    "            \"min_samples_split\", np.arange(2, 40, dtype=int)\n",
    "        ),\n",
    "        \"min_samples_leaf\": hp.choice(\"min_samples_leaf\", np.arange(2, 40, dtype=int)),\n",
    "    },\n",
    "    \"ensemble.AdaBoostRegressor\": {\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 10000, 10, dtype=int)),\n",
    "        \"learning_rate\": hp.uniform(\"learning_rate\", 0.01, 0.2),\n",
    "        \"loss\": hp.choice(\"loss\", [\"linear\", \"square\", \"exponential\"]),\n",
    "    },\n",
    "    \"ensemble.ExtraTreesRegressor\": {\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 10000, 10, dtype=int)),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 20, dtype=int)),\n",
    "        \"min_samples_split\": hp.choice(\n",
    "            \"min_samples_split\", np.arange(2, 40, dtype=int)\n",
    "        ),\n",
    "        \"min_samples_leaf\": hp.choice(\"min_samples_leaf\", np.arange(2, 40, dtype=int)),\n",
    "        \"max_features\": hp.choice(\"max_features\", [\"auto\", \"sqrt\"]),\n",
    "    },\n",
    "    \"svm.SVR\": {\n",
    "        \"C\": hp.uniform(\"C\", 0.00001, 10),\n",
    "        \"kernel\": hp.choice(\"kernel\", [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]),\n",
    "        \"degree\": hp.choice(\"degree\", [2, 3]),\n",
    "        \"gamma\": hp.uniform(\"gamma\", 0.0001, 10),\n",
    "        \"epsilon\": hp.uniform(\"epsilon\", 0.001, 5),\n",
    "    },\n",
    "    \"neighbors.KNeighborsRegressor\": {\n",
    "        \"algorithm\": hp.choice(\"algorithm\", [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"]),\n",
    "        \"n_neighbors\": hp.choice(\"n_neighbors\", np.arange(1, 20, dtype=int)),\n",
    "        \"weights\": hp.choice(\"weights\", [\"distance\", \"uniform\"]),\n",
    "        \"p\": hp.choice(\"p\", [1, 2]),\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T03:56:32.541742Z",
     "start_time": "2019-05-04T03:54:18.872942Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# execute bayesian optimization grid search\n",
    "analysis = \"housing\"\n",
    "train.execBayesOptimSearch(\n",
    "    allSpace=allSpace,\n",
    "    resultsDir=\"data/{}_hyperopt_{}.csv\".format(rundate, analysis),\n",
    "    X=train.data,\n",
    "    y=train.target,\n",
    "    scoring=\"rmsle\",\n",
    "    n_folds=8,\n",
    "    n_jobs=16,\n",
    "    iters=1500,\n",
    "    verbose=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Evaluation'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model with full set of predictor variables\n",
    "linReg = linear_model.LinearRegression()\n",
    "linReg.fit(XTrain, yTrain)\n",
    "yPredsTrain = linReg.predict(XTrain)\n",
    "yPredsTest = linReg.predict(XTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat value 1 X times (len of train then test array)\n",
    "yActual = np.vstack((yTrain, yTest))\n",
    "yPreds = np.vstack((yPredsTrain, yPredsTest))\n",
    "yType = np.hstack((np.repeat(0, yTrain.shape[0]), np.repeat(1, yTest.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize predictions using residual plot\n",
    "p = PrettierPlot()\n",
    "ax = p.makeCanvas(title=\"\", xLabel=\"Predicted values\", yLabel=\"Residuals\", yShift=0.8)\n",
    "p.pretty2dScatterHue(\n",
    "    x=yPreds,\n",
    "    y=yPreds - yActual,\n",
    "    target=yType,\n",
    "    label=[\"Training\", \"Test\"],\n",
    "    xUnits=\"f\",\n",
    "    yUnits=\"f\",\n",
    "    bbox=(1.2, 0.9),\n",
    "    ax=ax,\n",
    ")\n",
    "plt.hlines(y=0, xmin=-10, xmax=50, color=\"black\", lw=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T03:56:57.637995Z",
     "start_time": "2019-05-04T03:56:49.863869Z"
    }
   },
   "outputs": [],
   "source": [
    "# read scores summary table\n",
    "resultsDf = pd.read_csv(\"data/20190504_hyperopt_housing.csv\", na_values=\"nan\")\n",
    "results = train.unpackParams(resultsDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T03:21:59.237653Z",
     "start_time": "2019-05-04T03:21:56.081884Z"
    }
   },
   "outputs": [],
   "source": [
    "# loss plot\n",
    "train.lossPlot(resultsDf=results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T03:25:16.349899Z",
     "start_time": "2019-05-04T03:24:33.920766Z"
    }
   },
   "outputs": [],
   "source": [
    "# estimator parameter plots\n",
    "train.paramPlot(results=results, allSpace=allSpace, nIter=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Model explanability\n",
    "\n",
    "https://www.kaggle.com/learn/machine-learning-explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Feature-importance'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Permutation importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Permutation-importance'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T08:30:55.873743Z",
     "start_time": "2019-03-24T08:30:55.870060Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# permutation importance\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "perm = PermutationImportance(my_model, random_state=1).fit(val_X, val_y)\n",
    "eli5.show_weights(perm, feature_names=val_X.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Partial plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'Partial-plots'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "from matplotlib import pyplot as plt\n",
    "from pdpbox import pdp, get_dataset, info_plots\n",
    "\n",
    "# Create the data that we will plot\n",
    "pdp_goals = pdp.pdp_isolate(\n",
    "    model=tree_model, dataset=val_X, model_features=feature_names, feature=\"Goal Scored\"\n",
    ")\n",
    "\n",
    "# plot it\n",
    "pdp.pdp_plot(pdp_goals, \"Goal Scored\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "feature_to_plot = \"Distance Covered (Kms)\"\n",
    "pdp_dist = pdp.pdp_isolate(\n",
    "    model=tree_model,\n",
    "    dataset=val_X,\n",
    "    model_features=feature_names,\n",
    "    feature=feature_to_plot,\n",
    ")\n",
    "\n",
    "pdp.pdp_plot(pdp_dist, feature_to_plot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Build Random Forest model\n",
    "rf_model = RandomForestClassifier(random_state=0).fit(train_X, train_y)\n",
    "\n",
    "pdp_dist = pdp.pdp_isolate(\n",
    "    model=rf_model, dataset=val_X, model_features=feature_names, feature=feature_to_plot\n",
    ")\n",
    "\n",
    "pdp.pdp_plot(pdp_dist, feature_to_plot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 2D plots\n",
    "# Similar to previous PDP plot except we use pdp_interact instead of pdp_isolate and pdp_interact_plot instead of pdp_isolate_plot\n",
    "features_to_plot = [\"Goal Scored\", \"Distance Covered (Kms)\"]\n",
    "inter1 = pdp.pdp_interact(\n",
    "    model=tree_model,\n",
    "    dataset=val_X,\n",
    "    model_features=feature_names,\n",
    "    features=features_to_plot,\n",
    ")\n",
    "\n",
    "pdp.pdp_interact_plot(\n",
    "    pdp_interact_out=inter1, feature_names=features_to_plot, plot_type=\"contour\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### SHAP values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a id = 'SHAP-values'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "row_to_show = 5\n",
    "data_for_prediction = val_X.iloc[\n",
    "    row_to_show\n",
    "]  # use 1 row of data here. Could use multiple rows if desired\n",
    "data_for_prediction_array = data_for_prediction.values.reshape(1, -1)\n",
    "\n",
    "\n",
    "my_model.predict_proba(data_for_prediction_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import shap  # package used to calculate Shap values\n",
    "\n",
    "# Create object that can calculate shap values\n",
    "explainer = shap.TreeExplainer(my_model)\n",
    "\n",
    "# Calculate Shap values\n",
    "shap_values = explainer.shap_values(data_for_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# use Kernel SHAP to explain test set predictions\n",
    "k_explainer = shap.KernelExplainer(my_model.predict_proba, train_X)\n",
    "k_shap_values = k_explainer.shap_values(data_for_prediction)\n",
    "shap.force_plot(k_explainer.expected_value[1], k_shap_values[1], data_for_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "shap.DeepExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import shap  # package used to calculate Shap values\n",
    "\n",
    "# Create object that can calculate shap values\n",
    "explainer = shap.TreeExplainer(my_model)\n",
    "\n",
    "# calculate shap values. This is what we will plot.\n",
    "# Calculate shap_values for all of val_X rather than a single row, to have more data for plot.\n",
    "shap_values = explainer.shap_values(val_X)\n",
    "\n",
    "# Make plot. Index of [1] is explained in text below.\n",
    "shap.summary_plot(shap_values[1], val_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import shap  # package used to calculate Shap values\n",
    "\n",
    "# Create object that can calculate shap values\n",
    "explainer = shap.TreeExplainer(my_model)\n",
    "\n",
    "# calculate shap values. This is what we will plot.\n",
    "shap_values = explainer.shap_values(X)\n",
    "\n",
    "# make plot.\n",
    "shap.dependence_plot(\n",
    "    \"Ball Possession %\", shap_values[1], X, interaction_index=\"Goal Scored\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Stacking'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primary models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Primary-models'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T14:05:17.965746Z",
     "start_time": "2019-03-24T14:05:17.669308Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T14:11:12.912222Z",
     "start_time": "2019-03-24T14:09:44.583800Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T14:11:26.441649Z",
     "start_time": "2019-03-24T14:11:26.032435Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Meta-model'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T14:16:24.776352Z",
     "start_time": "2019-03-24T14:12:03.005790Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T14:16:24.983826Z",
     "start_time": "2019-03-24T14:16:24.779451Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Submission'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Standard'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T13:46:12.681655Z",
     "start_time": "2019-03-24T13:46:06.563199Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T13:46:14.138741Z",
     "start_time": "2019-03-24T13:46:14.106125Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate prediction submission file\n",
    "my_submission = pd.DataFrame({\"Id\": dfTest.Id, \"SalePrice\": np.expm1(yPred)})\n",
    "my_submission.to_csv(\"data/submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Stack'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T14:17:12.332889Z",
     "start_time": "2019-03-24T14:17:11.732365Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T14:17:12.422440Z",
     "start_time": "2019-03-24T14:17:12.335453Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate prediction submission file\n",
    "my_submission = pd.DataFrame({\"Id\": dfTest.Id, \"SalePrice\": np.expm1(yPred)})\n",
    "my_submission.to_csv(\"data/submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# misc code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling missing value of Age\n",
    "\n",
    "## Fill Age with the median age of similar rows according to Pclass, Parch and SibSp\n",
    "# Index of NaN age rows\n",
    "index_NaN_age = list(dataset[\"Age\"][dataset[\"Age\"].isnull()].index)\n",
    "\n",
    "for i in index_NaN_age:\n",
    "    age_med = dataset[\"Age\"].median()\n",
    "    age_pred = dataset[\"Age\"][\n",
    "        (\n",
    "            (dataset[\"SibSp\"] == dataset.iloc[i][\"SibSp\"])\n",
    "            & (dataset[\"Parch\"] == dataset.iloc[i][\"Parch\"])\n",
    "            & (dataset[\"Pclass\"] == dataset.iloc[i][\"Pclass\"])\n",
    "        )\n",
    "    ].median()\n",
    "    if not np.isnan(age_pred):\n",
    "        dataset[\"Age\"].iloc[i] = age_pred\n",
    "    else:\n",
    "        dataset[\"Age\"].iloc[i] = age_med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libs\n",
    "\n",
    "from sklearn.linear_model import ElasticNet, Lasso, BayesianRidge, LassoLarsIC\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-13T03:47:25.794949Z",
     "start_time": "2019-03-13T03:47:25.789160Z"
    }
   },
   "outputs": [],
   "source": [
    "n_folds = 10\n",
    "\n",
    "\n",
    "def rmsle_cv(model):\n",
    "    kf = model_selection.KFold(n_folds, shuffle=True, random_state=42).get_n_splits(\n",
    "        train.data.values\n",
    "    )\n",
    "    rmse = np.sqrt(\n",
    "        -model_selection.cross_val_score(\n",
    "            model,\n",
    "            train.data.values,\n",
    "            train.target,\n",
    "            scoring=\"neg_mean_squared_error\",\n",
    "            cv=kf,\n",
    "        )\n",
    "    )\n",
    "    return rmse\n",
    "\n",
    "\n",
    "averaged_models = AveragingModels(models=(topXGBoost, topGBR, topLGBM))\n",
    "avg_scores = rmsle_cv(averaged_models)\n",
    "\n",
    "\n",
    "cv = train.rmsleCV(\n",
    "    estimator=topLGBM,\n",
    "    X=train.data,\n",
    "    y=train.target,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    cv=10,\n",
    "    modelDesc=\"lightgbm\",\n",
    ")\n",
    "\n",
    "\n",
    "#\n",
    "\n",
    "z = list(zip(rfrFinal.feature_importances_, np.append(numCols, catCols)))\n",
    "z = sorted(z, key=lambda tup: tup[0], reverse=True)[:20]\n",
    "\n",
    "# plot horizontal bar by feature importance\n",
    "z.sort(reverse=False)\n",
    "values, labels = zip(*z)\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.subplot(121)\n",
    "plt.barh(labels, values)\n",
    "plt.xlabel(\"Percent Contribution to Random Forest Model\")\n",
    "plt.ylabel(\"Feature Names\")\n",
    "plt.title(\"Comparison of Features by Importance\")\n",
    "\n",
    "# reverse sorting (for a more intuitive aesthetic) and plot the cumulative value of features\n",
    "plt.subplot(122)\n",
    "z.sort(reverse=True)\n",
    "values, labels = zip(*z)\n",
    "plt.plot(np.cumsum(values))\n",
    "plt.ylabel(\"Contribution to the Random Forest Model\")\n",
    "plt.xlabel(\"Number of Features\")\n",
    "plt.title(\"Cumulative Value of Features by Importance\")\n",
    "\n",
    "plt.subplots_adjust(wspace=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-13T03:44:30.184949Z",
     "start_time": "2019-03-13T03:44:30.177683Z"
    }
   },
   "outputs": [],
   "source": [
    "class AveragingModels(base.BaseEstimator, base.RegressorMixin, base.TransformerMixin):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "\n",
    "    # we define clones of the original models to fit the data in\n",
    "    def fit(self, X, y):\n",
    "        self.models_ = [base.clone(x) for x in self.models]\n",
    "\n",
    "        # Train cloned base models\n",
    "        for model in self.models_:\n",
    "            model.fit(X, y)\n",
    "\n",
    "        return self\n",
    "\n",
    "    # Now we do the predictions for cloned models and average them\n",
    "    def predict(self, X):\n",
    "        predictions = np.column_stack([model.predict(X) for model in self.models_])\n",
    "        return np.mean(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "\n",
    "class ClaimAggregater(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        agg_op_dt_claim = {\n",
    "            \"PayDelay\": {\n",
    "                \"max_PayDelay\": \"max\",\n",
    "                \"min_PayDelay\": \"min\",\n",
    "                \"avg_PayDelay\": \"mean\",\n",
    "            },\n",
    "            \"LengthOfStay\": {\"max_LOS\": \"max\", \"min_LOS\": \"min\", \"avg_LOS\": \"mean\"},\n",
    "            \"DSFS\": {\"max_dsfs\": \"max\", \"min_dsfs\": \"min\", \"avg_dsfs\": \"mean\"},\n",
    "            \"CharlsonIndex\": {\n",
    "                \"max_CharlsonIndex\": \"max\",\n",
    "                \"min_CharlsonIndex\": \"min\",\n",
    "                \"avg_CharlsonIndex\": \"mean\",\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # add binary categorical columns to agg_op_dt_claim for groupby\n",
    "        for i in X.columns[np.array(X.dtypes == \"uint8\")]:\n",
    "            agg_op_dt_claim[\"{0}\".format(i)] = {\"Sum_{0}\".format(i): \"sum\"}\n",
    "\n",
    "        result = X.groupby([\"Year\", \"MemberID\"]).agg(agg_op_dt_claim)\n",
    "        result.columns = result.columns.droplevel()\n",
    "        result = result.reset_index(level=[\"Year\", \"MemberID\"])\n",
    "        result[\"range_dsfs\"] = result[\"max_dsfs\"] - result[\"min_dsfs\"]\n",
    "        result[\"range_CharlsonIndex\"] = (\n",
    "            result[\"max_CharlsonIndex\"] - result[\"min_CharlsonIndex\"]\n",
    "        )\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess via pipeline\n",
    "\n",
    "\n",
    "class DrugAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        dsfs_dt = {\n",
    "            \"0- 1 month\": 15,\n",
    "            \"1- 2 months\": 45,\n",
    "            \"2- 3 months\": 75,\n",
    "            \"3- 4 months\": 105,\n",
    "            \"4- 5 months\": 135,\n",
    "            \"5- 6 months\": 165,\n",
    "            \"6- 7 months\": 195,\n",
    "            \"7- 8 months\": 225,\n",
    "            \"8- 9 months\": 255,\n",
    "            \"9-10 months\": 285,\n",
    "            \"10-11 months\": 315,\n",
    "            \"11-12 months\": 345,\n",
    "        }\n",
    "        X[\"DSFS\"] = X[\"DSFS\"].apply(lambda x: dsfs_dt[x])\n",
    "        X[\"DrugCount\"] = X[\"DrugCount\"].apply(lambda x: 7 if x == \"7+\" else int(x))\n",
    "        return X\n",
    "\n",
    "\n",
    "class DrugAggregater(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        agg_op_dt_drug = {\n",
    "            \"DrugCount\": {\n",
    "                \"max_DrugCount\": \"max\",\n",
    "                \"min_DrugCount\": \"min\",\n",
    "                \"avg_DrugCount\": \"mean\",\n",
    "                \"months_DrugCount\": \"count\",\n",
    "            }\n",
    "        }\n",
    "        result = X.groupby([\"Year\", \"MemberID\"]).agg(agg_op_dt_drug)\n",
    "        result.columns = result.columns.droplevel()\n",
    "        result = result.reset_index(level=[\"Year\", \"MemberID\"])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert 'AgeAtFirstClaim' to numerical approximation\n",
    "\n",
    "\n",
    "class MemberAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        age_dt = {\n",
    "            \"40-49\": 45,\n",
    "            \"70-79\": 75,\n",
    "            \"50-59\": 55,\n",
    "            \"60-69\": 65,\n",
    "            \"30-39\": 35,\n",
    "            \"10-19\": 15,\n",
    "            \"0-9\": 5,\n",
    "            \"20-29\": 25,\n",
    "            \"80+\": 85,\n",
    "        }\n",
    "        X[\"AgeAtFirstClaim\"] = X[\"AgeAtFirstClaim\"].apply(\n",
    "            lambda x: None if pd.isnull(x) else age_dt[x]\n",
    "        )\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitPrep(dataset):\n",
    "    dfTrain, dfTest = train_test_split(dataset, test_size=0.3, random_state=42)\n",
    "\n",
    "    yTrain = dfTrain[\"label\"]\n",
    "    yTest = dfTest[\"label\"]\n",
    "\n",
    "    xTrain = dfTrain.drop([\"label\"], axis=1)\n",
    "    xTest = dfTest.drop([\"label\"], axis=1)\n",
    "\n",
    "    allCols = xTrain.columns.values\n",
    "    catCols = [\"ClaimsTruncated\", \"F\", \"M\"]\n",
    "    index = [np.argwhere(allCols == i)[0][0] for i in catCols]\n",
    "    numCols = np.delete(allCols, index)\n",
    "\n",
    "    numPipeline = Pipeline(\n",
    "        [\n",
    "            (\"selector\", DataFrameSelector(numCols)),\n",
    "            (\"imputer\", Imputer(strategy=\"median\")),\n",
    "            (\"std_scaler\", StandardScaler()),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    catPipeline = Pipeline([(\"selector\", DataFrameSelector(catCols))])\n",
    "\n",
    "    fullPipeline = FeatureUnion(\n",
    "        transformer_list=[(\"numPipeline\", numPipeline), (\"catPipeline\", catPipeline)]\n",
    "    )\n",
    "\n",
    "    xTrain = fullPipeline.fit_transform(xTrain)\n",
    "    xTest = fullPipeline.transform(xTest)\n",
    "\n",
    "    return xTrain, xTest, yTrain, yTest, numCols, catCols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LandContour: Flatness of the property\n",
    "\n",
    "       Lvl\tNear Flat/Level\t\n",
    "       Bnk\tBanked - Quick and significant rise from street grade to building\n",
    "       HLS\tHillside - Significant slope from side to side\n",
    "       Low\tDepression\n",
    "\n",
    "MiscFeature: Miscellaneous feature not covered in other categories\n",
    "\t\t\n",
    "       Elev\tElevator\n",
    "       Gar2\t2nd Garage (if not described in garage section)\n",
    "       Othr\tOther\n",
    "       Shed\tShed (over 100 SF)\n",
    "       TenC\tTennis Court\n",
    "       NA\tNone\n",
    "       \n",
    "     MiscVal: $Value of miscellaneous feature\n",
    "\n",
    "\n",
    "Condition1: Proximity to various conditions\n",
    "\t\n",
    "       Artery\tAdjacent to arterial street\n",
    "       Feedr\tAdjacent to feeder street\t\n",
    "       Norm\tNormal\t\n",
    "       RRNn\tWithin 200' of North-South Railroad\n",
    "       RRAn\tAdjacent to North-South Railroad\n",
    "       PosN\tNear positive off-site feature--park, greenbelt, etc.\n",
    "       PosA\tAdjacent to postive off-site feature\n",
    "       RRNe\tWithin 200' of East-West Railroad\n",
    "       RRAe\tAdjacent to East-West Railroad\n",
    "\t\n",
    "Condition2: Proximity to various conditions (if more than one is present)\n",
    "\t\t\n",
    "       Artery\tAdjacent to arterial street\n",
    "       Feedr\tAdjacent to feeder street\t\n",
    "       Norm\tNormal\t\n",
    "       RRNn\tWithin 200' of North-South Railroad\n",
    "       RRAn\tAdjacent to North-South Railroad\n",
    "       PosN\tNear positive off-site feature--park, greenbelt, etc.\n",
    "       PosA\tAdjacent to postive off-site feature\n",
    "       RRNe\tWithin 200' of East-West Railroad\n",
    "       RRAe\tAdjacent to East-West Railroad\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
